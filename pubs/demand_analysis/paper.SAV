
%% Kluwer Small Journal Article Sample File
%% June 20, 1994

%%   Use one of these two commands: 
\documentstyle{smjrnl} % For Computer Modern Fonts, or
%\documentstyle[smjfonts]{smjrnl} % for PostScript fonts


\input{commands}
\sloppy

\begin{document}
\begin{article}

%%%%% To be entered at Kluwers: =====>>

\journame{Small Journal Name}
\volnumber{9}
\issuenumber{4}
\issuemonth{October}
\volyear{1992}

%% Do not delete either of the following two commands.
%% Please supply facing curly brackets for the part you
%% are not using for this article.
%\received{May 1, 1991}\revised{}

\authorrunninghead{J.R. Anderson and M. Matessa}
\titlerunninghead{An Incremental, Bayesian Algorithm for Categorization}

\setcounter{page}{1} %% This command is optional. 
                       %% May set page number only for first page in
                       %% issue, if desired.

%% <<== End of commands to be entered at Kluwers 


%%  Authors, start here ==>>

\title{PERs from Projections for Binding-time Analysis}

\authors{Kei Davis}
\email{kei@@crl.nmsu.edu}

\affil{Computing Research Laboratory,
New Mexico State University,
Las Cruces, NM  88003, U.S.A.}

%\editor{Dennis Kibler}

\abstract{{\it First-order\/} projection-based binding-time analysis
has proven genuinely useful in partial evaluation
\cite{Lau91a}.  There have been three notable generalisations
of projection-based analysis to higher order.  The first lacked a
formal basis \cite{Mog89}; the second used structures strictly more
general that projections, namely {\it partial equivalence relations\/}
(PERs) \cite{HS91}; the third involved a complex construction that
gave rise to impractically large abstract domains \cite{Dav93b}.  This
paper presents a technique free of these shortcomings: it is simple,
entirely projection-based, satisfies a formal correctness condition,
and gives rise to reasonably small abstract domains.  Though the
technique is cast in terms of projections, there is also an
interpretation in terms of PERs.  The principal limitation of the
technique is the restriction to {\it monomorphic\/} typing.}

\keywords{Binding-time analysis, abstract interpretation, projections,
PERs.}


\section{Introduction and Background}

We take as given that binding-time analysis is essential for good
partial evaluation, and we do not address the issue of annotating
programs according to the results of analysis.  Numerous binding-time
analysis techniques have been proposed and implemented; we greatly
narrow the field of discussion by restricting attention to those for
which there is a formally stated notion of correctness that the
technique has been proven to satisfy.

Analysis techniques can usually be identified as being based on either
a non-standard denotational semantics or a non-standard typing.
Examples in the latter category include those of Gomard \cite{Go92},
Jensen \cite{Jen92}, the Nielsons \cite{NN88}, Schmidt \cite{Sch88},
and Henglein and Mossin \cite{HM94}.  Our focus is on those techniques
based on non-standard interpretation, in particular, those using
projections or partial equivalence relations (PERs) as the basic
abstract values.

A domain {\it projection\/} is a continuous idempotent function that
approximates the identity.  Launchbury \cite{Lau88} hit upon the idea
of using projections to encode degrees of staticness of data.  The
basic idea is that a projection maps to $\bot$ that part of a data
structure that is dynamic (possibly not determined), and acts as the
identity on that part which is static (definitely determined).
Examples are the identity $\ID$, the greatest projection, which
specifies that values are entirely static; the constant $\bot$
function $\BOT$, the least projection, which specifies that values are
entirely dynamic; and projections $\FST$ and $\SND$ on product
domains, defined by
\[
\FST\ (x,y) = (x,\bot)\ ,\hspace{1.0in} \SND\ (x,y) = (\bot,y)\ ,
\]
specifying staticness in the first and second components of pairs,
respectively.  The nominal goal of analysis is, given function $f$
denoted by some programming-language expression, and projection $\D$
encoding the staticness of the argument of $f$, to determine $\G$
satisfying the {\it safety condition\/} $\G\o\f\we\f\o\D$.  For
example, taking $\G$ to be $\SND$ satisfies
$\G\o\itt{swap}\we\itt{swap}\o\FST$ for $\itt{swap}$ defined by
$\itt{swap}~(x,y)=(y,x)$.  Taking $\G$ to be $\BOT$ always satisfies
the safety condition but tells nothing; greater $\G$ is more
informative.  Launchbury \cite{Lau91a} showed that this safety
condition satisfies, and in a sense which he formalises, is equivalent
to the correctness condition for binding-time analysis in the general
framework of Jones \cite{Jon88}.  Using projection-based analysis,
Launchbury implemented both monomorphic and polymorphic versions of a
partial evaluator for a first-order language.

There have been three notable attempts to generalise Launchbury's
techniques to higher order.  The first was Mogensen's generalisation
of the polymorphic technique \cite{Mog89}.  Though successfully
implemented, there is no formal statement of what it means for the
analysis to be correct; even if such a statement were made, proving
correctness would likely be difficult because of the highly
intensional nature of the analysis: the non-standard values associated
with expressions are strongly dependent on their syntactic structure,
and projections are encoded symbolically as {\it abstract closures\/},
with approximation performed algebraically `on-the-fly' according to
time and space considerations.  Nonetheless, the experiment provided
evidence for the {\it practicality\/} of the projection-based approach
at higher order.

The second generalisation was Hunt and Sands', of the monomorphic
technique to higher order \cite{HS91}.  Their observation was that a
projection, regarded as a set of domain-range pairs, is an {\it
equivalence relation\/}: given $\G$, values $u$ and $v$ are in the
same equivalence class if $\G~u=\G~v$, and the canonical elements of
the equivalence classes are the set of fixed points (range) of $\G$.
Hunt showed that the safety condition $\G\o\f\we\f\o\D$ holds iff $f$
is related to itself by $\D\fto\G$ where $\fto$ is the standard
operation on binary relations, so 
"" 
	(\G\o\f\we\f\o\D)\
	\Leftrightarrow\ 
	(\forall u,v\ .\ \D\ u=\D\ v\ \Rightarrow\ \G\ (f\ u)=\G\ (f\ v))\ .  
""
Then, for example, $(\BOT\fto\ID)~(f,f)$ asserts
that $f$ maps dynamic arguments to static results.  In general
$\D\fto\G$ is not an equivalence relation, but it is always a {\it
partial\/} equivalence relation: it is symmetric and transitive but
not necessarily reflexive.  (Partialness is crucial: if $\D\fto\G$
were in general reflexive then $(\D\fto\G)(f,f)$ would hold for all
$f$, hence such assertions would tell nothing.)  Unlike projections
regarded as relations, PERs are closed under $\fto$; the result, as
Hunt and Sands show, is that `scaling up' to higher-order analysis is
reasonably straightforward.  One disadvantage of their method is that
PER spaces are considerably larger than the projection spaces on the
same domains, and it is not clear which PERs to choose for (finite)
abstract domains.  Here they borrowed heavily from the projection
world, using standard abstract projection domains at ground types.
Further, its practicality has not been demonstrated by implementation,
and, because of the unfamiliar territory, a promising route to a
polymorphic generalisation is obscure.

The third generalisation was ours, to an entirely projection-based,
monomorphic, higher-order technique \cite{Dav93b}.  One observation
motivating the approach is that there is no meaningful abstraction of
values to projections, only of functions $f$ to projection
transformers $\T$ (functions from projections to projections)
satisfying $(\T~\D)\o\f\we\f\o\D$ for all $\D$.  To make this
abstraction possible a semantics intermediate between the standard and
analysis semantics was introduced.  Moving from the standard to
intermediate semantics involved a translation of each ground type @T@
to a function type @E@\ @->@\ @T@ (for a fixed type @E@), the values
of which, being functions, could then be abstracted.  The result,
while proven correct with respect to a formal safety condition, is
probably not practicable because of the growth in the sizes of
(usefully rich) abstract domains induced by the type translation.

This paper presents a technique far simpler technique than our
previous one.  No intermediate semantics is required, and the
correctness condition and proof are much simpler.  Because the
translation of ground types to function types is avoided the abstract
domains are much smaller. Though entirely projection-based, we show
that there is a reading of the results in terms of PERs, intimating a
close relationship with the PER-based technique.

\section{Language and Standard Semantics}

The source language is a simple, strongly typed, monomorphic,
non-strict functional language.  The grammar for the language of types
and type definitions is given in Figure~\ref{fig:types}.
\fig{tb}{Types and type definitions.}{fig:types}{
""
@T@	&::=&	\ta			&\reason{Type Name}
	&|&	\tint			&\reason{Integer}
	&|&	\tprod			&\reason{Product, $n\geq0$}
	&|&	\tsum			&\reason{Sum, $n\geq1$} 
	&|&	\tfun			&\reason{Function} \espace 
@D@	&::=&	\tdefn			&\reason{Type Definitions}
""
}
Nullary product corresponds to the so-called {\it unit\/} type.  A
unary product @(T)@ will always have the same semantics as @T@.  The
types used in the examples are defined as follows.
@@@
     FunList =   nil () 
               + cons (Int -> Int, FunList) , 

     FunTree =   leaf (Int -> Int) 
	       + branch (FunTree, FunTree) .
@@@

The grammar for expressions is given in Figure~\ref{fig:exprs}.
Addition for integers is provided as typical of operations on flat data
types in this setting.  A unary tuple @(e)@ will always have the same
semantics as @e@.  The (monomorphic) typing of expressions is entirely
standard and is omitted.
\fig{tb}{Expressions.}{fig:exprs}{
""
@e@	&::=&	\evar				&\reason{Variable} 
	&|&	\enum				&\reason{Numeral}
	&|&	\esum				&\reason{Integer addition}
	&|&	\eapp				&\reason{Function application}
	&|&	\etup				&\reason{Tuple construction}
	&|&	@let@\ @(@\xone@,@\ldots@,@\xn@)@\ @=@\ \enot\ @in@\ \eone			&\reason{Tuple decomposition}
	&|&	\econ				&\reason{Sum construction}
	&|&	@case@\ \enot\ @of @\{\ci\ \xxi\ @->@\ \ei\}	&\reason{Sum decomposition}
	&|&	\elamt				&\reason{Lambda abstraction}
	&|&	\eapp				&\reason{Function application}
	&|&	\efix				&\reason{Fixed point}
""
}

\subsection{Expression semantics}
Since two different expression semantics will be given, following
Abramsky \cite{Abr90} we define a semantics $\cE{}$ parameterised by a
set of {\it defining constants}.  The semantics $\cE{}$ is defined in
Figure~\ref{fig:gensem}; the defining constants are {\it plus},
$\itt{sel}_i$, {\it tuple}, $\itt{inc}_i$, $\itt{outc}_i$, {\it
choose}, {\it mkfun}, {\it apply}, and {\it fix}.  The two instances
of $\cE{}$ are distinguished by a superscript: $\cE{\Ss}$ for the
standard semantics and $\cE{\Ps}$ for the non-standard semantics.  The
corresponding type semantics have the same superscripts, as do the
defining constants.

It is useful to regard the free-variable environment of each
expression as having some tuple type $\tprod$, and environment lookup
as indexing (as in a categorical semantics, or De~Bruijn indexing);
variables are indexed implicitly or explicitly by their index in the
free variable environment.  Then for both versions of the semantics
and all expressions @e@ of type @T@ with environment of type $\tprod$,
""
	\Ee{}{@e@}\ \in\ \Te{}{\tprod}~\fto~\Te{}{@T@}\ .
""
Noting that $\R\br{\xxi}$ is short for $\itt{sel}_i~\R$, environment
update $\R[\xxi\mapsto{}v]$ is defined by
"""
	tuple (sel_1 \R, \ldots, sel_{i-1} \R, v, sel_{i+1} \R, \ldots, sel_n \R) .
"""

\fig{tb}{Parameterised semantics.}{fig:gensem}{
""
	\Ee{}{\evar_i}\ \R = \R\br{@x@_i} = \itt{sel}_i\ \R\espace\espace
%	\Ee{}{\eunit}\ \R = \itt{mkunit}\ \R\espace\espace
%	\Ee{}{\enum}\ \R = \itt{mkint}_i\espace\espace
	\Ee{}{\esum}\ \R = \itt{plus}~(\Ee{}{\eone}\ \R,\ \Ee{}{\etwo}\ \R)\espace\espace
	\Ee{}{\etup}\ \R = \itt{tuple}~(\Ee{}{\eone}\ \R,~\ldots,~\Ee{}{\en}\ \R)\espace\espace
	\Ee{}{\esel}\ \R
	\ \ \ \ \ = \Ee{}{\eone}\ \R[\xxi\mapsto\itt{sel}_i\ (\Ee{}{\enot}\ \R)~|~\oin]\espace\espace
	\Ee{}{\econ}\ \R = \itt{inc}_i\ (\Ee{}{@e@}\ \R)\espace\espace
	\Ee{}{\ecase}\ \R
	\ \ \ \ \  = \itt{choose}~(\ba[t]{l}\Ee{}{\enot}\ \R,
	\Ee{}{\eone}\ \R[\xone\mapsto\itt{outc}_1\ (\Ee{}{\enot}\ \R)],
	\ \ \ ~\vdots
	\Ee{}{\en}\ \R[\xn\mapsto\itt{outc}_n\ (\Ee{}{\enot}\ \R)])\ea\espace\espace
	\Ee{}{\elam}\ \R = \itt{mkfun}\ (\L{}x.\Ee{}{@e@}\ \R[\evar\mapsto{}x])\espace\espace
	\Ee{}{\eapp}\ \R = \itt{apply}\ (\Ee{}{\eone}\ \R)\ (\Ee{}{\etwo}\ \R)\espace\espace
	\Ee{}{\efix}\ \R = (\itt{fix}\o\itt{apply})\ (\Ee{}{@e@}\ \R)
""
}

\subsection{Standard semantics}

The standard $\Ss$ type and expression semantics are defined in
Figure~\ref{fig:ssem}.  Function types give rise to lifted function
spaces as in Abramsky's lazy lambda calculus \cite{Abr89}, and the
semantics distinguishes those expressions of function type that have
WHNF (have value $\itt{lift}~f$ for some $f$) and those that do not
(have value $\bot$).  Products are unlifted; a unary sum-of-products
gives a lifted product.  Domain $\plus$ is separated sum.  Recursive
type definitions give rise to recursive domain specifications which
have the usual least-fixed-point solutions.

\fig{tb}{Standard type and expression semantics.}{fig:ssem}{
""
\Te{\Ss}{\tint} = \Int = \lift{\bf{Z}} \espace\espace
\Te{\Ss}{\tprod} = \Te{\Ss}{\tone}\ \times\ \ldots\ \times\ \Te{\Ss}{\tn}\espace\espace
\Te{\Ss}{\tsum} = \Te{\Ss}{\tone}\ \plus\ \ldots\ \plus\ \Te{\Ss}{\tn}\espace\espace
\Te{\Ss}{\tfun} = \liftp{\Te{\Ss}{\tone} \fto \Te{\Ss}{\ttwo}}

%	\itt{mkunit}^{\Ss}\ \R			 = 	()\espace\espace
%	\itt{mkint}^{\Ss}_n\ \R			 = 	\itt{lift}\ n\espace\espace
	\itt{plus}^{\Ss}\ (x,y) 		 = 	x+y\espace\espace
	\itt{tuple}^{\Ss}\ (x_1,\ldots,x_n)	 = 	(x_1,\ldots,x_n)\espace\espace
	\itt{sel}_i^{\,\Ss}\ (x_1,\ldots,x_n)	 = 	x_i\espace\espace
	\itt{inc}_i^{\Ss} 			 = 	\itt{in}_i\o\itt{lift}\espace\espace
	\itt{outc}_i^{\Ss}\ x			 = 	\itt{drop}\o\itt{out}_i\espace\espace
	\ba{lll}\itt{choose}^{\Ss}\ (\bot,&\ x_1,\ldots,x_n)		 = \bot
	\itt{choose}^{\Ss}\ (\itt{in}_i~v,&\ x_1,\ldots,x_n)	 = x_i\ea\espace\espace
	\itt{mkfun}^{\Ss}		 = 	\itt{lift}\espace\espace
	\itt{apply}^{\Ss}		 = 	\itt{drop}\espace\espace
	\itt{fix}^{\Ss}  		 = 	\itt{lfp} \reason{Least fixed point}
""
}

\section{Domain Factorisation}

A key observation of \cite{Dav93b} was that since there is no concept
of staticness of the body of a lambda expression, there is no point in
having projections on function spaces.  Hence domains are factored into
their evaluable or {\it data\/} parts, and their unevaluable but
applicable {\it forward\/} parts.  For example, for
$\liftp{T\fto{}U}=\Te{\Ss}{@T@\ @->@\ @U@}$ we need only distinguish two
degrees of definedness---between $\bot$ and values of the form
$\itt{lift}~f$.  This may be encoded by $\lone$---the data part of
$\liftp{T\fto{}U}$---on which there are precisely two projections,
namely $\ID$ and $\BOT$.  Here the forward part is $T\fto{}U$.

The {\it data domain\/} corresponding to type @T@ is $\De{}{@T@}$,
where $\cD{}$ is defined exactly like $\cT{\Ss}$ except that function
spaces are replaced by the one-point domain $\one=\{\unit\}$, that is,
$\De{}{\tfun}=\lift{\one}$.  The function $\data$ from values in
$\Te{\Ss}{@T@}$ to their data parts in $\De{}{@T@}$ is a projection:
it is like the identity except that values from function spaces are
mapped into $\one$.  The projection $\data$ is defined in terms of the
structure of @T@ as follows.
""
	{\it{}data}_{\stint} = {\it{}id}_{\stint}\ ,\espace
	{\it{}data}_{\stprod} = {\it{}data}_{\stone} \times \ldots \times {\it{}data}_{\stn} ,\espace
	{\it{}data}_{\stsum} = {\it{}data}_{\stone} \plus \ldots \plus {\it{}data}_{\stn} ,\espace
	{\it{}data}_{\stfun} = {\liftp{\Lx.\unit}} .
""
The last definition uses function lifting, defined by $\lf~\bot=\bot$
and $\lf~(\itt{lift}~x)=\itt{lift}~(f~x)$.  Recursive type definitions
give rise to recursive function specifications with only one solution.

In the same style as  $\De{}{@T@}$ and $\data$ we define
$\Tde{\Ss}{@T@}$ to give the forward domain for @T@, and $\fun$ to be
the function mapping values in $\Te{\Ss}{@T@}$ to their forward parts
in $\Tde{\Ss}{@T@}$, as follows.  Roughly, $\cTd{\Ss}$ is like
$\cT{\Ss}$ with all lifting removed and sum replaced by product.
""
	\Tde{\Ss}{\tint} = \one ,\espace
	\Tde{\Ss}{\tprod} = \Tde{\Ss}{\tone} \times \ldots \times \Tde{\Ss}{\tn} ,\espace
	\Tde{\Ss}{\tsum} = \Tde{\Ss}{\tone} \times \ldots \times \Tde{\Ss}{\tn} ,\espace
	\Tde{\Ss}{\tfun} = \Te{\Ss}{\tone} \fto \Te{\Ss}{\ttwo} .
""
The mapping from standard values to their forward parts is defined by
""
	{\it{}fun}_{\stint} = \Lx.\unit\ ,\espace
	{\it{}fun}_{\stprod} = {\it{}fun}_{\stone}\ \times\ \ldots\ \times\ {\it{}fun}_{\stn}\ ,\espace
	\ba{lll}{\it{}fun}_{\stsum}\ \bot	&= \bot\espace
	{\it{}fun}_{\stsum}\ (\itt{in}_i~v) = (\bot,\ldots,\bot,v,\bot_,\ldots,\bot)	&\reason{$v$ in $\ith$ position}\ ,\ea\espace
	{\it{}fun}_{\stfun} = \itt{drop}\ .
""
We write $\fac$ for $\Lx.(\data~x,\ \fun~x)$.  Then
$\De{}{@T@}\times\Tde{\Ss}{@T@}$ is a factorisation of
$\Te{\Ss}{@T@}$, and
""
	\fac \in \Te{\Ss}{@T@} \fto (\De{}{@T@} \times \Tde{\Ss}{@T@})
""
is an embedding which {\it determines\/} the corresponding projection
$\unfac$ (since they are related by $\unfac\o\fac=\itt{id}$ and
$\fac\o\unfac\we\itt{id}$).  Rather than give an explicit definition
of $\unfac$ we give some examples.  The factorisation of $\Int$ is
$\Int\times\one$; more generally, the factorisation of any domain $D$
corresponding to a type not containing @->@ is just $D\times\one$.  If
$T=\Te{\Ss}{@T@}$ and $U=\Te{\Ss}{@U@}$ then the factorisation of
$\lift{(T\fto{}U)}=\Te{\Ss}{@T@\ @->@\ @U@}$ is

is an embedding which {\it determines\/} the corresponding projection
$\unfac$ (since they are related by $\unfac\o\fac=\itt{id}$ and
$\fac\o\unfac\we\itt{id}$).  Rather than give an explicit definition
of $\unfac$ we give some examples.  The factorisation of $\Int$ is
$\Int\times\one$; more generally, the factorisation of any domain $D$
corresponding to a type not containing @->@ is just $D\times\one$.  If
$T=\Te{\Ss}{@T@}$ and $U=\Te{\Ss}{@U@}$ then the factorisation of
$\lift{(T\fto{}U)}=\Te{\Ss}{@T@\ @->@\ @U@}$ is
$\lift{\one}\times(T\fto{}U)$---note that factorisation `stops' at
function-space constructors.  Now
${\it{}fac}_{@@@T->U@@@}~\bot=(\bot,\bot)$, and
${\it{}fac}_{@@@T->U@@@}~(\itt{lift}~f)=(\lunit,~f)$ for all $f$.  In
the other direction ${\it{}unfac}_{@@@T->U@@@}~(\bot,f)=\bot$ and
${\it{}unfac}_{@@@T->U@@@}~(\lunit,~f)=(\itt{lift}~f)$; this must be
since in general $\unfac\o\fac$ is the identity.

\section{Projection Semantics}

Standard values $v$ and $v'$ are related by nonstandard value
$(\A,\K)$ when their data parts are related by $\A$, and their forward
parts are logically related by $\K$.  At each type @T@ the relation is
$\Re{}{@T@}~((\A,\K),\cdot,\cdot)$, defined by
""
	{\Re{}{@T@}~((\A,\K),v,v')} =&\mit(\A~d=\A~d')\ \wedge\ \Rde{}{@T@}~(\K,f,f')
		&\mbox{\rm{}where}
		&\ \ \ \ba[t]{lll}(d,f)  &\:= \fac~v
				\mit(d',f')	&\:= \fac~v' ,\ea
""
where $\Rde{}{@T@}$ is defined by
""
	\Rde{}{\tint}~((),(),()) = \itt{True}\ ,\espace
	\Rde{}{\tprod} = \Rde{}{\tone}\ \x\ \ldots\ \x\ \Rde{}{\tn}\ ,\espace
	\Rde{}{\tsum} = \Rde{}{\tone}\ \x\ \ldots\ \x\ \Rde{}{\tn}\ ,\espace
	\Rde{}{\tfun} = \Re{}{\tone}\ \fto\ \Re{}{\ttwo}\ .
""
Here $\x$ and $\fto$ are the standard operations on (ternary)
relations:  for relations $R$ and $S$ we have
$(R\times{}S)((x,y),(x',y'),(x'',y''))$ iff $R(x,x',x'')$ and
$S(y,y',y'')$, and $(R\fto{}S)(f,g,h)$ iff for all $x$, $y$, and $z$
such that $R(x,y,z)$ we have $S(f~x,\ g\ y,\ h~z)$.  Here, recursive
type definitions give recursive relation specifications, which have
inclusive least-fixed-point solutions. (A relation is inclusive if,
when it holds for each element of an ascending chain, it also holds at
the limit.  Such relations are sometimes called {\it admissible\/} or
{\it chain complete}.  Some work is required to show inclusivity of
these recursively-defined relations.) Thus the $\Ps$ type semantics
$\cT{\Ps}$ must be
""
	\Te{\Ps}{@T@} = \proj{\De{}{@T@}}\ \times\ \Tde{\Ps}{@T@}\ ,
""
where $\proj{\De{}{@T@}}$ is the lattice of projections on domain
$\De{}{@T@}$, and $\cTd{\Ps}$ is defined exactly like $\cTd{\Ss}$ with
superscript $\Ps$ everywhere replacing superscript $\Ss$.

\fig{tb}{Projection semantics.}{fig:fsem}{
""
%	\itt{mkunit}^{\Ps}\ \R			 = 	()
%\espace\espace
%	\itt{mkint}^{\Ps}_n\ \R			 = 	\itt{lift}\ n
%\espace\espace
	\itt{plus}^{\Ps}\ ((\A,()),(\B,())) = \left\{\ba{ll}(\ID,()),	&\ \ \ \ \mbox{\rm if $\A=\ID$ and $\B=\ID$}
			(\BOT,()),	&\ \ \ \ \mbox{\rm otherwise}\ea\right.\espace
	\itt{tuple}^{\Ps}\ ((\A_1,\K_1),\ldots,(\A_n,\K_n)) = ((\A_1\x\ldots\x\A_n),(\K_1,\ldots,\K_n))\espace
	\itt{sel}_i^{\,\Ps}\ ((\A_1\x\ldots\x\A_n),(\K_1,\ldots,\K_n))  = (\A_i,\K_i)\espace
	\itt{inc}_i^{\Ps}\ (\A,\K)	 = 	(C_i\ \A,\ (\top,\ldots,\top,\K,\top,\ldots,\top))\espace
	\itt{outc}_i^{\Ps}\ (\A,(\K_1,\ldots,\K_n))	 = (\OUTC_i\ \A,\ \K_i)\espace
	\itt{choose}^{\Ps}\ ((\A,\K),x_1,\ldots,x_n) = \left\{\ba{ll}\bot,		&\ \ \ \ \mbox{\rm if}\ \A\not\se\glb_{\oin} C_i~\BOT
			x_1\glb\ldots\glb{}x_n,	&\ \ \ \ \mbox{\rm otherwise}\ea\right.\espace
	\itt{mkfun}^{\Ps}\ f			 = 	(\ID,f)\espace
	\itt{apply}^{\Ps}\ (\A,f) = \left\{\ba{lll}\bot,	&\ \ \ \ \mbox{\rm if}\ \A=\BOT
						f,	&\ \ \ \ \mbox{\rm if}\ \A=\ID \ea\right.\espace
	\itt{fix}^{\Ps} = 	\itt{gfp} \reason{Greatest fixed point}
""
}

The defining constants for the projection semantics $\cE{\Ps}$ are
given in Figure~\ref{fig:fsem}. The following notation is used for
specifying projections on the data domains.  For sum type $\tsum$ with
data domain $D_1\plus\ldots\plus D_n$ define $C_i~\A$ to be
$\ID+\cdots+\ID+\A+\ID+\cdots+\ID$ where $\A$ is the $i^{th}$ summand,
$\OUTC_i~(\G_1+\cdots+\G_n)$ to be $\G_i$, and $\OUTC_i~\BOT$ to be
$\BOT$.

\ \\ \noindent {\bf The Central Result.} For expression @e@ of type
@T@ with free-variable environment of type @E@, the functions
$\Ee{\Ps}{@e@}$, $\Ee{\Ss}{@e@}$, and $\Ee{\Ss}{@e@}$ are logically
related by $\cR{}$, that is
""
	(\Re{}{@E@}\fto\Re{}{@T@})\ (\Ee{\Ps}{@e@},\ \Ee{\Ss}{@e@},\ \Ee{\Ss}{@e@})\ .
""
Further, $\Re{}{@T@}~((\A,\K),\cdot,\cdot)$ is a PER for all $\A$ and
$\K$.\ \ $\Box$

\ \\ \noindent The bulk of proof is omitted; it consists of showing
that the defining constants are similarly related, and a simple
induction on the structure of expressions showing that if the defining
constants are logically related, then so are the semantic functions.
By way of example we consider the expression form $\esum$, and the
relevant defining constant $\it plus$.  To show that $\it plus^{\Ss}$
and $\it plus^{\Ps}$ are correctly related we need to show that
"""
	((\Re{}{\tint}\x\Re{}{\tint})\fto\Re{}{\tint}) (plus^{\Ps},plus^{\Ss},plus^{\Ss}) ,
"""
that is, for all $\A_1$, $\A_2$, $d_1$, $d_2$, $d_1'$, and $d_2'$ that
""
	\mit(\ID d_1=\ID d_1') \land (\ID d_2=\ID d_2') \limp \mit\ID (d_1+d_2)=\ID (d_1'+d_2')
""
and for $\A_1\neq\ID$ or $\A_2\neq\ID$ that
""
	\mit(\A_1 d_1=\A_1 d_1') \land (\A_2 d_2=\A_2 d_2') \limp \mit\BOT (d_1+d_2)=\BOT (d_1'+d_2') .
""
For the inductive case $\esum$ with environment type @E@ we need to 
show that if
""
	(\Re{}{@E@}\fto\Re{}{\tint}) (\Ee{\Ps}{\eone}, \Ee{\Ss}{\eone}, \Ee{\Ss}{\eone}) 
""
and
""
	(\Re{}{@E@}\fto\Re{}{\tint}) (\Ee{\Ps}{\etwo}, \Ee{\Ss}{\etwo}, \Ee{\Ss}{\etwo}) 
""
then
"""
	(\Re{}{@E@}\fto\Re{}{\tint}) (\Ee{\Ps}{\esum},\Ee{\Ss}{\esum},\Ee{\Ss}{\esum}) .
"""
Suppose $\Re{}{@E@}~(\R^{\Ps},\R^{\Ss},\R'^{\Ss})$.  By the induction
hypothesis we have
$\Re{}{\tint}~(\A_i,v_i,v'_i)$, where 
$(\A_i,())=\Ee{\Ps}{\ei}~\R^{\Ps}$,
$(v_i,())=\Ee{\Ps}{\ei}~\R^{\Ss}$, and
$(v'_i,())=\Ee{\Ps}{\ei}~\R'^{\Ss}$, for $i=1,2$.
Now
"""
	\Re{}{\tint} (\Ee{\Ps}{\esum} \R^{\Ps}, \Ee{\Ss}{\esum}~\R^{\Ss}, \Ee{\Ss}{\esum}~\R'^{\Ss})
"""
iff
"""
	\Re{}{\tint} (plus^{\Ps} ((\A_1,()),(\A_2,())), plus^{\Ss} \mit((v_1,()),(v_2,())), plus^{\Ss} \mit((v'_1,()),(v'_2,()))) ,
"""
which holds since $\it plus^{\Ps}$ and $\it plus^{\Ps}$, and their
various arguments, are correctly related.

\section{Abstract Domains}


At each type @T@ we require a finite abstraction of $\Te{\Ps}{@T@}$.
This abstract domain is ${\it{}FProj}_{\stt}\times{\it{}FFor}_{\stt}$,
where ${\it{}FProj}_{\stt}$ is a finite abstraction of the lattice
$\proj{\De{}{@T@}}$, and ${\it{}FFor}_{\stt}$ is a finite abstraction
of $\Tde{\Ps}{@T@}$.  The definition of ${\it{}FProj}_{\stt}$ is based
on that in \cite{Lau91a}.  A projection $\G$ is in
${\it{}FProj}_{\stt}$ if $\G~\mbox{\bf proj}~@T@$ can be inferred from
the rules given in Figure~\ref{fig:infrules}.
%
\fig{tb}{Inference rules for finite projection domains.}{fig:infrules}{
""
\begin{array}{lll}\
\axm{\BOT\ \ \mbox{\bf{}proj}\ \ \tint}\ \ \ \ \ \ \ \ \ \ & \axm{\ID\ \ \mbox{\bf{}proj}\ \ \tint}\
\espace\espace
\axm{\BOT\ \ \mbox{\bf{}proj}\ \ \tfun} & \axm{\ID\ \ \mbox{\bf{}proj}\ \ \tfun}\end{array}\
\espace\espace
\axm{\BOT\ \ \mbox{\bf{}proj}\ \ \tsum}\
\espace\espace
\infero{\G_1\ \ \mbox{\bf{}proj}\ \ \tone\ \ \cdots\ \ \G_n\ \ \mbox{\bf{}proj}\ \ \tn}{(C_1\ \G_1)\glb\cdots\glb(C_n\ \G_n)\ \ \mbox{\bf{}proj}\ \ \tsum}\
\espace\espace
\infero{\G_1\ \ \mbox{\bf{}proj}\ \ \tone\ \ \cdots\ \ \G_n\ \ \mbox{\bf{}proj}\ \ \tn}{\G_1\x\ldots\x\G_n\ \ \mbox{\bf{}proj}\ \ \tprod}\
\espace\espace
\infer{\ba{ll}&\G_1\ \ \mbox{\bf{}proj}\ \ \aone\ \ \cdots\ \ \G_n\ \ \mbox{\bf{}proj}\ \ \an
\vdash & P(\G_1,...,\G_n)\ \ \mbox{\bf{}proj}\ \ \ti@(@\aone@,...,@\an@)@\ea}{\mu(\G_1,\ldots,\G_n).P(\G_1,\ldots,\G_n)\ \ \mbox{\bf{}proj}\ \ \ai}\
\ \ \ \ \ [\mbox{\rm{}where}\ \ai@=@\ti@(@\aone@,...,@\an@)@]
""
}
For recursively-defined types the rules yield only those projections
that act on each recursive instance of a data structure in the same
way.  Thus ${\it{}FProj}_{@@@FunList@@@}$ comprises $\BOT$ and
$\SPINE~\A$ for $\A$ ranging over $\BOT$ and $\ID$, where
""
	\SPINE\ \A = \mu\G.(\itt{NIL}\ \ID)\ \glb\ (\itt{CONS}\ (\A\ \x\ \G))\ ,
""
so $\SPINE\ \BOT$ specifies static spines and dynamic elements, and
$\SPINE\ \ID$ is the identity.  More generally, the abstract list
constructor is isomorphic to lifting.  Similarly,
${\it{}FProj}_{@@@FunTree@@@}$
comprises $\BOT$ and $\LBR\ \A$ for $\A$ ranging over $\BOT$ and $\ID$, where
""
	\LBR\ \A = \mu\G.(\itt{LEAF}\ \A)\ \glb\ (\itt{BRANCH}\ (\G\ \x\ \G))\ ,
""
so $\LBR\ \A$ specifies static branches and leaves, and $\A$ of the
leaf nodes.  Again, the abstract tree constructor is isomorphic to
lifting.  This compares favourably with
BHA strictness analysis, for which the corresponding abstract
constructors are typically {\it double\/} lifting \cite{Wad87,Sew93}.


Value $\K$ is in ${\it{}FFor}_{\stt}$ if $\K~\bfabsf~@T@$ can be
inferred from the following.

There is only one forward value at type $\tint$.
""
	\axm{() {} \bfabsf {} \tint} .
""
For products and sums,
""
\infer{\K_1 {} \bfabsf {} \tone { } \cdots { } \K_n {} \bfabsf {} \tn}\
{(\K_1,\ldots,\K_n) {} \bfabsf {} \tprod} ,
""
%\espace\espace
""
\infero{\K_1 {} \bfabsf {} \tone { } \cdots { } \K_n {} \bfabsf {} \tn}\
{(\K_1,\ldots,\K_n) { } \bfabsf {} \tsum} .
""
Function spaces consist of a set of step functions closed under lub.
""
\infero{\ba{ll}\T_1\in{\it{}FTran}_{\stone} { { } } \K_1 \bfabsf \tone
\T_2\in{\it{}FTran}_{\sttwo} { { } } \K_2 \bfabsf \ttwo\ea}\
{\it{}step ((\T_1,\K_1),(\T_2,\K_2)) {} \bfabsf {} \tfunp} ,
""
where
""
	step (v_1,v_2) x = v_2,		&{\rm if} v_1\we{}x 
	step (v_1,v_2) x = \bot,	&{\rm otherwise} ,
""
and
""
\infer{\K_1 {} \bfabsf {} \tfunp { { } } \K_2 {} \bfabsf {} \tfunp}\
{(\K_1\lub\K_2) {} \bfabsf {} \tfunp} .
""
This gives the full space of monotonic functions on the abstract
domains.

For recursively-defined types, roughly speaking, we choose those
forward values that represent each component of the same type by the
same value.  Given type definitions $\tdefn$, which we will write
$@A@_i@=T@_i@(A@_1@,...,A@_n@)@,\ \oin$, if by assuming
$\K_i~\bfabsf~\ai$ for $\oin$ we may deduce
$P_i(\K_1,\ldots,\K_n)~\bfabsf~\ti(\aone\ldots\an)$ for $\oin$, then
""
	\mu(\K_1,...,\K_n).(P_1(\K_1,...,\K_n),...,P_n(\K_1,...,\K_n))
""
is a tuple $(\K_1,...,\K_n)$ of values such that $\K_i\ \bfabsf\ @A@_i$
for $\oin$.

For all @T@ the set ${\it{}FProj}_{\stt}\times{\it{}FFor}_{\stt}$ is a 
finite lattice containing the top and bottom elements of $\Te{\Ps}{@T@}$.

For @T@ not containing @->@ the domain $\Tde{\Ps}{@T@}$ is isomorphic
to $\one$, so ${\it{}FFor}_{\stint}$ is $\one$.  For @Int@\ @->@\ @Int@
we have
$\Tde{\Ps}{@Int@\ @->@\ @Int@}=(\proj{\De{}{\tint}}\x\one)\fto(\proj{\De{}{\tint}}\x\one)$,
so ${\it{}FFor}_{@@@Int->Int@@@}$ is
$({\it{}FProj}_{\stint}\x\one)\fto({\it{}FProj}_{\stint}\x\one)$.  A
data structure of recursive type @A=T(A)@ may be thought of as some
(possibly infinite) number of elements of @T(())@.  For example, the
value $\itt{cons}\ (f,\ \itt{cons}\ (g,\ \itt{nil}\ ()))$ in the
standard domain for @FunList@ decomposes into $\itt{cons}\ (f,())$,
$\itt{cons}\ (g,())$, and $\itt{nil}\ ()$.  The (implicit) abstraction
function maps such a data structure to the greatest lower bound of
these elements, giving a safe abstraction of the nonstandard values of
each element.  Thus
${\it{}FFor}_{@@@A=T(A)@@@}={\it{}FFor}_{@@@T(())@@@}$, so
${\it{}FFor}_{@@@FunList@@@}\iso{\it{}FFor}_{@@@FunTree@@@}\iso{\it{}FFor}_{@@@Int->Int@@@}$.



\section{Examples of Analysis}

For all closed expressions @e@ the abstract value
$\Ee{\Ps}{@e@}~[\,]$ of @e@ is of the form $(\ID,\K)$, showing
that closed expressions are always entirely static.  For expressions of
function type the abstract forward value $\K$ is a function from
abstract arguments of @e@ to abstract results.

First we consider functions on lists.
Let expression @length@ denote usual length function:
@@@
     fix (\length . \xs . case xs of
                            nil u  -> 0
                            cons p -> let (z,zs) = p in
                                        1 + length zs) .
@@@
The abstract forward value of @length@ is of the form
$\L(\A,\K).(\T~\A,())$, where $\T$ maps $\SPINE~\BOT$ and $\SPINE~\ID$
to $\ID$, and $\BOT$ to $\BOT$.  This reveals that the result of
@length@ is independent of the values of list elements, and gives
a static result when the argument has a static spine.


Let @append@ stand for the expression denoting the usual function for
appending two lists:
@@@
     fix (\append . \xs . \ys . case xs of
                                  nil u  -> ys
                                  cons p -> let (z,zs) = p in
		                              cons (z, append zs ys)) .
@@@
The abstract value of @append@ is
""
(\ID, \L(\A_{@@@xs@@@},\K_{@@@xs@@@}).
(\ID, \L(\A_{@@@ys@@@},\K_{@@@ys@@@}).
(\A_{@@@xs@@@}\glb\A_{@@@ys@@@}, \K_{@@@xs@@@}\glb\K_{@@@ys@@@}))) .
""
This reveals that partial applications of @append@ are static up to WHNF, and the
abstract value of the result is the greatest lower bound of the two
arguments.  In general, the abstract value of a closed expression
of the form $@\x@_1@.\x@_2\ldots@.e@$ will reveal that all partial
applications of the expression are static up to WHNF.

Let @reverse1@ stand for the expression denoting the naive reverse
function:
@@@
     fix (\reverse1 . \xs . case xs of
                              nil u  -> nil ()
                              cons p -> let (z,zs) = p in
                                          append (reverse zs) 
			                         (cons (z, nil ()))) .
@@@
The abstract forward value of @reverse1@ is the identity, so the
abstraction of the elements of a list doesn't change by reversing the
list.

Let @compose@ stand for @\f.\g.\x.f (g x)@.  The abstract value of @compose@ is
""
(\ID, \L(\A_{@@@f@@@},\K_{@@@f@@@}).
(\ID, \L(\A_{@@@g@@@},\K_{@@@g@@@}).
(\ID, \left\{\ba{lll}\bot, &\mbox{\rm if}\ \A_{@@@f@@@}=\BOT\mbox{\rm or} \A_{@@@g@@@}=\BOT
	\K_{@@@f@@@}\o\K_{@@@g@@@},&\mbox{\rm otherwise}\ea\right\}))) .
""
Thus, the
result of the application of the composition of two functions is
dynamic, and maps all values to dynamic values, if either function is
dynamic; otherwise, the result is given by the application of the
composition of the abstract forward values to the abstract argument.

Let @listcomp@ stand for the expression denoting the function
that composes lists of functions:
@@@
     fix (\listcomp . \fs . case fs of
                              nil u  -> \x.x
                              cons p -> let (g,gs) = p in 
                                          compose g (listcomp gs)) .
@@@
The abstract value of @listcomp@ is
""
	(\ID,\ \L(\A_{@@@fs@@@},\K_{@@@fs@@@}).\
	(\ID,\ \left\{\ba{lll}\
\bot,		&\mbox{\rm if}\ \A_{@@@fs@@@}\neq\SPINE\ \ID
\glb_{i\geq0}(\K_{@@@fs@@@})^i,	&\mbox{\rm otherwise}\ea\right\})) .
""
Since the abstract values of lists contain no information about the
length of the lists of which they are abstractions, the abstract value
of the composition of list elements is the glb of the composition over
all lengths.

Let @flatten@ stand for the expression denoting the function that
flattens trees into lists:
@@@
     fix (\flatten . \t . case t of
	                    leaf l   -> cons (l, nil ())
	                    branch p -> let (t1,t2) = p in
			                  append (flatten t1) 
					         (flatten t2)) .
@@@
The abstract value of @flatten@ is
""
	(\ID,\ \L(\A_{@@@fs@@@},\K_{@@@fs@@@}).\
	(\T\ \A,\ \left\{\ba{lll}\
\bot,		&\mbox{\rm if}\ \T\ \A_{@@@fs@@@}\neq\LBR\ \ID
\K_{@@@fs@@@},	&\mbox{\rm otherwise}\ea\right\})) ,
""
where $\T$ maps $\BOT$ to $\BOT$, and $\LBR~\A$ to $\SPINE~\A$ for $\A$
ranging over $\ID$ and $\BOT$.

The function denoted by @compose listcomp flatten@ that composes
trees of functions has abstract value
""
	(\ID,\ \L(\A_{@@@fs@@@},\K_{@@@fs@@@}).\
	(\ID,\ \left\{\ba{lll}\
\bot,		&\mbox{\rm if}\ \A_{@@@fs@@@}\neq\LBR\ \ID
\glb_{i\geq0}(\K_{@@@fs@@@})^i,	&\mbox{\rm otherwise}\ea\right\}))\ .
""
Similarly to the case for lists, the abstract values of trees contain
no information about the structure of the trees of which they are abstractions,
so the abstract value of the composition of the values of the leaves
is the glb of the composition over all tree structures.

\section{More on Abstract Domains}

The sizes of the abstract domains and the representations of the
abstract values can be considerably optimised.  The nonstandard
semantics of application---embodied by $\it apply^{\Ps}$---guarantees
that the abstract values $(\BOT,f)$ and $(\BOT,f')$ from
$\Te{\Ps}{\tfun}$, for all $f$ and $f'$, are effectively the same:
$\it apply^{\Ps}~(\BOT,f)=\bot$ for all $f$.  A practical analyser
would take advantage of this fact, identifying $(\BOT,f)$ over
all $f$.  More generally, for function types embedded
within data structures, e.g.\ $@(@\tint@,@\tfun@)@$, abstract values
$((\A,()),(\BOT,f))$ would be identified over all $f$.

In the following we restrict attention to {\it denotable\/} values:  at
each type @T@ those values that can be expressed as $\Ee{}{@e@}~[]$ for
some @e@.  For every value $v\in\Te{\Ss}{@T@}$ there is a best
abstraction---a greatest value $(\A,\K)\in\Te{\Ps}{@T@}$ such that
$\Re{}{@T@}((\A,\K),v,v)$.  For types $\tone$ and $\ttwo$ not
containing @->@ we have
$\Tde{\Ps}{\tfun}\iso(\proj{\De{}{\tone}}\x\one)\fto(\proj{\De{}{\ttwo}}\x\one)$,
and if $\K\in\Tde{\Ps}{@T@}$ is greatest such that
$\Rde{}{\tfun}~(\K,f,f)$ for denotable $f$, then $\K$ maps $(\ID,())$
to $(\ID,())$ and distributes over $\glb$.  The subset of functions
from $\Tde{\Ps}{\tfun}$ that map $(\ID,())$ to $(\ID,())$ and
distribute over $\glb$ forms a complete lattice, hence attention may be
restricted to this subset.  Not only does this reduce the number of
abstract functions, it also reduces their representation:  such
functions are determined by their behaviour on the $\glb$-{\it basis\/}
of the lattice $\proj{\De{}{\tone}}$---the set $B$ of values such that
every element of $\proj{\De{}{\tone}}$ is the glb of some (possibly
empty) subset of $B$, and no element of $B$ can be expressed as the glb
of some subset not containing that element.  This optimisation can be
generalised to higher order:  $\Tde{\Ps}{\tfun}$ may be restricted to
functions that map $\top$ to $\top$ and distribute over $\glb$, for all
function types $\tfun$.  (These results follow from those shown in
\cite{Dav94}.)

\section{Related Work}

Consel \cite{Con90} describes a binding-time analysis for higher-order
untyped languages.  As in our analysis abstract values have two parts,
the first describing the static/dynamic properties of values, and the
second describing how (for function types) abstract arguments are
mapped to abstract results; there appears to be an implicit domain
factorisation similar to ours, based on implicit type information
collected by analysis semantics.  In this respect there are many
superficial similarities between the two techniques.  No formal
relation to the standard semantics is given, making a formal comparison
with our technique problematic.

\section{Conclusion}

We have successfully generalised Launchbury's monomorphic
projection-based binding-time analysis to higher-order, using
abstract domains smaller than those typically used in BHA
strictness analysis.

The next step would be to generalise to handle Hindley-Milner
polymorphism.  This has been done with good results at first order for
binding-time analysis \cite{Lau91a}, and for BHA strictness analysis
at higher order \cite{Bar91,Bar93}.  We anticipate that the combined
use of these theories will give a reasonably straightforward generalisation
to polymorphism.  In addition to making the analysis more widely applicable,
this should also greatly reduce the run-time cost of analysis.

Though not developed here, using the same approach it is
possible to give a strictness analysis technique, again closely related to
Hunt's PER-based strictness analysis technique \cite{Hun91}.


\begin{thebibliography}{123456}

\bibitem[Abr89]{Abr89}
S. Abramsky. 
``The lazy lambda calculus.'' 
{\it Research Topics in Functional Programming.}
David Turner, ed., Addison-Wesley 1989.

\bibitem[Abr90]{Abr90}
S. Abramsky. 
``Abstract interpretation, logical relations and Kan extensions.''
{\it Journal of Logic and Computation}, 1, 1990.

\bibitem[Bar91]{Bar91}
G. Baraki.
``A note on abstract interpretation of polymorphic functions.''
In \cite{Hug91}.

\bibitem[Bar93]{Bar93}
G. Baraki.
{\it Abstract Interpretation of Polymorphic Higher-Order Functions.}
Ph.D. thesis, Research report FP-1993-7, 
Department of Computing Science, University of Glasgow.

\bibitem[BEJ88]{BEJ88}
D. Bjorner, A.P. Ershov, and N.D. Jones, eds.
{\it Partial Evaluation and Mixed Computation,
Proceedings IFIP TC2 Workshop, Gammel Avern{\ae}s}, Denmark, October 1987.
North-Holland, 1988.

\bibitem[Con90]{Con90}
C. Consel. 
``Binding Time Analysis for Higher Order Untyped Functional Languages.''
Proceedings of the 1990 ACM Conference on LISP and 
Functional Programming, pp264-272.

\bibitem[Dav93]{Dav93b}
K. Davis.
``Higher-order Binding-time Analysis.'' 
{\it Proceedings of the 1993 ACM on Partial Evaluation and
Semantics-Based Program Manipulation (PEPM '93)},
ACM Press, 1993.

\bibitem[Dav94]{Dav94}
K. Davis.\ \ 
{\it Projection-based Program Analysis.}
Thesis submitted for degree of Ph.D., Computing Science Department,
University of Glasgow, 1994.

\bibitem[Go92]{Go92}
C.K. Gomard.
``A self-applicable partial evaluator for the lambda calculus:  Correctness
and pragmatics.''
ACM TOPLAS, Vol 14, No.\ 2, April 1992.

\bibitem[HM94]{HM94}
F. Henglein and C. Mossin.
``Polymorphic binding-time analysis.''
European Symposium on Programming (ESOP '94).

\bibitem[Hug91]{Hug91}
J. Hughes, ed.
{\it Proceedings of the 1991 Conference on Functional Programming Languages
and Computer Architecture (FPCA '91)}, Cambridge, Sept 1991. LNCS 523,
Springer Verlag, 1991.

\bibitem[Hun91]{Hun91}
S. Hunt.
``PERs generalise projections for strictness analysis (extended abstract).''
{\it Proceedings of the 1990 Glasgow Workshop
on Functional Programming}.
Simon L. Peyton Jones {\it et al.}, eds.
Springer Workshops in Computing.
Springer-Verlag, 1991.

\bibitem[HS91]{HS91}
S. Hunt and D. Sands.
``Binding time analysis:  a new PERspective.''
{\it ACM Symposium on Partial Evaluation
and Semantics-Based Program Manipulation},
SIGPLAN Notices Vol.\ 26, No.\ 9, 1991.

\bibitem[Jen92]{Jen92}
T. Jensen.
{\it Abstract Interpretation in Logical Form}.
Ph.D. thesis, Report 93/11, Department of Computer Science,
University of Copenhagen, 1992.

\bibitem[Jon88]{Jon88}
N.D. Jones.
``Automatic program specialization: A re-examination from
basic principles.'' In \cite{BEJ88}.

\bibitem[Lau88]{Lau88}
J. Launchbury.
``Projections for specialisation.''
In \cite{BEJ88}.

\bibitem[Lau91a]{Lau91a}
J. Launchbury. 
{\it Projection Factorisations in Partial Evaluation.} PhD
Thesis, Glasgow University, Nov 89. Distinguished Dissertation in
Computer Science, Vol 1, CUP, 1991.

\bibitem[Mog89]{Mog89}
T. Mogensen.
``Binding-time analysis for polymorphically typed higher order languages.''
{\it International Joint Conference on Theory and Practice of Software
Development}. LNCS 352. Springer-Verlag 1989.

\bibitem[NN88]{NN88}
H.R. Nielson and F. Nielson.
``Automatic binding-time analysis for a typed $\L$-calculus.''
{\it Science of Computer Programming 10},
North Holland, 1988.  Also in POPL '88.

\bibitem[Sch88]{Sch88}
D.A. Schmidt.
``Static properties of partial reduction.''
In \cite{BEJ88}.

\bibitem[Sew93]{Sew93}
J. Seward.
``Polymorphic strictness analysis using frontiers.''
{\it Proceedings of the 1993 ACM on Partial Evaluation and
Semantics-Based Program Manipulation (PEPM '93)},
ACM Press, 1993.

\bibitem[Wad87]{Wad87}
P. Wadler.
``Strictness analysis on non-flat domains by abstract interpretation over finite domains. '' 
S. Abramsky, C. Hankin, eds.
{\it Abstract Interpretation of Declarative Languages.}
Ellis-Horwood, 1987.


\end{thebibliography}











\commentout{
\section{Garbage}


We have been engaged in a project to understand human categorization which
has let us to develop a machine learning algorithm. Our research began as an
exploration of the issue of whether human categorization can be considered
optimal.

To pursue the issue of whether human cognition is optimal requires
specifying two things. First we need a definition of optimality.
second, we need a specification of the structure of the environment
so we can determine what behavior is optimal in that environment.


\subsection{Preliminary definition of optimization}
Our assumption has been that the goal of categorization is to predict
unknown features of various objects that we encounter.

\subsubsection{The structure of the environment}
Our theory of the structure of the environment has been focused the
structure of living things (arguably, the largest portion of the
objects in the world) because of the aid biology gives in objectively
specifying the organization of these objects.

Formally, this amounts to calculating:
\begin{equation}
g_i(y|f)=\sum_x P(x|F_n)f_i(y|x)
\end{equation}
where $g_i(y|F_n)$ is the function specifying the probability an object will
display a value $y$ on a dimension $i$ given $F_n$ the observed feature
structure of all the objects.
\newpage
Here is an example of a wide equation:
\begin{wideequation}
\begin{equation}
\sum_k P(k) \sum_i \sum_y f_i(y|k)^2
\sum_k P(k) \sum_i \sum_y f_i(y|k)^2
\end{equation}
\end{wideequation}

This is an example of a split equation,
moving the top half to the left
and the bottom to the right.
\begin{splitmath}
\sum_k P(k) \sum_i \sum_y f_i(y|k)^2\\
\sum_k P(k) \sum_i \sum_y f_i(y|k)^2
\sum_k P(k) \sum_i \sum_y f_i(y|k)^2
\end{splitmath}

\subsection{Footnote example}
Here is some text with footnotes in it.
Here is some text with footnotes in it.
Here is some text with footnotes in it.\footnote{This is a footnote.}

More text.\footnote{This is a second footnote.
This is a second footnote.
This is a second footnote.
This is a second footnote.
This is a second footnote.
This is a second footnote.} 
More text.\footnote{This is yet another footnote.
This is yet another footnote.
This is yet another footnote.
This is yet another footnote.
This is yet another footnote.
This is yet another footnote.}

\subsection{Indented text}
In an example satisfies the seed of a clause, then it satisfies the clause
as well. In addition, seeds have the following property:

\begin{itemize}
\item[] 
If a seed of clause $c_T$, and example {\bf x} satisfies $c_T$ but
not $c$, then {\bf x} has at least one attibute in $c_T$ that
is not in $c$.\hfill({\tt*})
\end{itemize}
The procedure below...

\subsection{Bulleted List}
Here is an example of a bulleted list:
\begin{itemize}
\item
Some text here. Some text here. Some text here. Some text here. 
Some text here.
Some text here. Some text here. Some text here. Some text here.

\item
Some text here. Some text here. Some text here.
\end{itemize}


\subsection{Numbered List}
Here is an example of a numbered list:
\begin{enumerate}
\item
Some text here. Some text here. Some text here. Some text here.

Some text here. Some text here. Some text here. Some text here. Some
text here.

\item
Some text here. Some text here. Some text here.

\begin{enumerate}
\item
Some text here. Some text here. Some text here. Some text here. Some
text here.  Some text here. Some text here. Some text here. Some text
here.

\item
Some text here. Some text here. Some text here.
\begin{enumerate}
\item
Some text here. Some text here. Some text here. Some text here. Some
text here.  Some text here. Some text here. Some text here. Some text
here.

\item
Some text here. Some text here. Some text here.
\end{enumerate}
\end{enumerate}
\end{enumerate}


\subsection{To Illustrate an Algorithm}
This is the command to use when you want to illustrate an algorithm
with some pseudo code. A backslash followed with a space will
indent the line. Every line will be printed
as it is seen on the screen. Blank lines will be preserved.
Math and font changes may be used. 

The command
\verb+\bit+ will produce bold italics if you are using PostScript fonts, 
boldface in Computer Modern. \verb+\note{}+ will position the
note on the right margin. A backslash followed by a space
will provide a space a bit wider than the width of 2 `M's.

\begin{algorithm}
{\bit Evaluate-Single-FOE} ({\bf x$_f$, I$_0$, I$_1$}):
\ {\bf I}+ := {\bf I}$_1$;
\ ($\phi,\theta$) := (0,0);
\ {\it repeat}\note{/*usually only 1 interation required*/}
\ \ (s$_{opt}${\bf E}$_\eta$) := {\bit Optimal-Shift} ({\bf I$_0$,I$^+$,I$_0$,x$_f$});
\ \ ($\phi^+$, $\theta^+$) := {\bit Equivalent-Rotation} ({\bf s}$_{opt}$);
\ \ ($\phi$, $\theta$) := ($\phi$, $\theta$) + ($\phi^+$, $\theta^+$);
\ \ {\bf I}$^+$:= {\bit Derotate-Image} ({\bf I}$_1$, $\phi$, $\theta$);
\ \ {\it until} ($|\phi^+|\leq\phi_{max}$ \& $|\theta^+|\leq\theta_{max}$);
\ {\it return} ({\bf I}$^+$, $\phi$, $\theta$, E$_\eta$).
End pseudo-code.
\end{algorithm}

\newpage
\subsection{Figures}
Here is an example of a figure with .5 inch space left for
the illustration:
\begin{figure}[h]
\vspace*{.5in}
\caption{This is a figure caption.
This is a figure caption.
This is a figure caption.}
\end{figure}

\section{Making Tables}
Use caption on top of the table. 
Use \verb+\hline+ at the top of the table, underneath the column headers
and at the end of the table.

You are discouraged from using vertical lines in tables, but
it you must include vertical lines, you must also use 
\verb+\savehline+ instead of \verb+\hline+ or there will be a
gap between the vertical and horizontal lines.
(\verb+\hline+ has been redefined to add some vertical space above and
below it.)

The following form will spread out to the width of the page:
                                                                                                               
\begin{table}[h]
\caption{This is an example table caption. As you can
see, it will be as wide as the table that it captions.}
\begin{tabular*}{\textwidth}{@@{\extracolsep{\fill}}lcr}
\hline
$\alpha\beta\Gamma\Delta$ One&Two&Three\cr
\hline
one&two&three\cr
one&two&three\cr
\hline
\end{tabular*}
\end{table}

\begin{table}[h]
\caption{This is a table caption and will fit
the width of the table that it is captioning.}
\begin{tabular}{lcr}
\hline
$\alpha\beta\Gamma\Delta$ One&Two&Three\cr
\hline
one&two&three\cr
one&two&three\cr
\hline
\end{tabular}
\end{table}


\subsection{Unusual Tables}
There are three cases in which authors have felt the need to
make a table other than the plain `tabular' table in this style.
These more unusual tables are: tables with vertical lines;
tables that do not use `tabular'; and tables that have
a number of `tabular's nested. In this section we will show
how to make these tables.

\subsubsection{Vertical Lines}
Notice in the previous examples that no vertical lines were used.
If at all possible to make your meaning clear without vertical
lines, please leave them out. However, if you really must use
vertical lines, you must use \verb+\savehline+ instead of
\verb+\hline+ and
you must add another letter to the preamble.

For instance, this is the effect we usually want; 
horizontal lines extending to
the edge of the text in the column:


\begin{table}[h]
\begin{tabular}{c}\hline
 A         \\ \hline
 B         \\ \hline
\end{tabular}
\end{table}


Since we usually do not want vertical
lines in tables and we do want the horizontal lines to extend
exactly to the left and right of text, 
we must go to some extra efforts to get the vertical lines
to extend to the top and bottom of the column and to not
have extra horizontal space to the right of the vertical line
in the last column.
Remember to
use \verb+\savehline+ and to
to add another column entry to the table preamble
and then not to use that column, as seen below:

\begin{table}[h]
\begin{tabular}{|c|c}\savehline
 A         \\ \savehline
 B         \\ \savehline
\end{tabular}
\end{table}

\newpage

\subsubsection{Table Made Without `tabular'}
Here is an example of a table made without `tabular.' Notice
the caption must be written above the table.

\begin{specialtable}[h]
\caption{Training a Thermal Linear Machine.}
\label{thermal-alg}
\hrule
\vskip3pt
\begin{enumerate}
  \item Initialize $\beta$ to $2$.

  \item If linear machine is correct for all instances or 
$emag/lmag < \alpha$ for the last $2*n$ instances, then return
($n =$ the number of features).

  \item Otherwise, pass through the training instances once, and for each 
instance ${\bf Y}$ that	would be misclassified by the linear machine and 
for which $k < \beta$, immediately
	\begin{enumerate}
	  \item Compute correction $c = \frac{\beta^{2}}{\beta+k}$, and update
${\bf W}_{i}$ and  ${\bf W}_{j}$.

	  \item If the magnitude of the linear machine decreased on this
		adjustment, but increased on the previous adjustment, then
		anneal $\beta$ to $a \beta - b$. 
	\end{enumerate}

  \item Go to step 2.
\end{enumerate}
\vskip1pt
\hrule
\end{specialtable}


\subsubsection{Nesting `Tabular's}
The table macros are made to automatically format the
caption so that it is the width of the table and the caption
appears at the top of the table. This works fine as long
as you use the normal \verb+\begin{tabular}..+ 
to make your
table, and you don't nest another \verb+\begin{tabular}..+ with it. 
However, if you want to use some other way of formatting
your table, or if you want to nest one or more tabulars within
the table, you must start with \verb+\begin{specialtable}+, as you
see in the example below.
Notice that the caption is given at the top of the table.


\begin{specialtable}[h]
\caption{Quantitative and qualitative descriptions}
\begin{tabular}{ccc}& Quantitative & Qualitative \\ \hline & & \\(a) & 
    \begin{tabular}{rr} {\it Time} &
     {\it Level}\\ \hline 3 sec. & 0.01 m\\ 4 sec. & 0.23 m\\ 5 sec. &
     0.31 m\\ \hline
    \end{tabular} & 
           \begin{tabular}{c}$ Level(between(t_1, t_2)) = $\\ \ \ \ \ $
              (between(zero,top), increasing)$\\
           \end{tabular}\\& & \\(b) & $ Amount = 2.5 * 
                    Level + 0.7 * Level ^2 $ & $ Amount =M^+(Level) $\\
                    & & \\  \hline
\end{tabular}
\label{tabqq}
\end{specialtable}



\section{Theorems, Proofs, Examples, etc.}

\begin{example}
Consider an example in which $B_S$ is the structure...

The term $P(B_X)$ is our probability---prior to observing the data
in database $D$---that the data-generating process is a belief network with
structure $B_{S1}$.
\end{example}


\begin{proclaim}{Theorem 1}LEARN-MONOTONE-K-CNF makes at most
$(n+1)^K$ mistakes on any monotone K-CNF formula. (Recall that
n is the size of the largest example seen.
\end{proclaim}

\begin{proof}
Thus on each mistake, we decrease the cost by at least 1, and since the 
cost is never negative, the algorithm makes less than $(n+1)^K$
mistakes total.

The running time of this algorithm is clearly polynomial in 
{\bf size}$(f_T)$ and the length of the longest example seen.
\end{proof}


\begin{proof}
Thus on each mistake, we decrease the cost by at least 1, and since the 
cost is never negative, the algorithm makes less than $(n+1)^K$
mistakes total.

The running time of this algorithm is clearly polynomial in 
{\bf size}$(f_T)$ and the length of the longest example seen.

\[
\alpha\beta\Gamma\Delta\inmathqed
\]
\end{proof}

\begin{proof}[Proof of Theorem A.1]
This is a proof with a particular term. Call for the
particular term after `proof', i.e., 
\verb+\begin{proof}[Proof of Theorem A.1]+.
\end{proof}


In this section, we present an efficient formula for computing $P(B_X, D)$.
We do so by first introducing four assumptions.

\begin{demo}{Assumption 1}
The database variables, which we denote as $Z$, are discrete.
\end{demo}

As this assumption states...


\acknowledgements
We would like to thank....

Trying `cite', \cite{jacobs}, \cite{francis}.


\appendix{} 
% Please supply facing curly brackets if there is only one appendix.
% Supply a letter if you are using a number of appendices;
%% i.e., \appendix{A} ...\appendix{B}. 

\appendixtitle{Sample Appendix Title} % optional appendix title

This is an appendix.
\begin{equation}
\sum_k P(k) \sum_i \sum_y f_i(y|k)^2
\end{equation}

\begin{references}
\bibitem{jacobs}Jacobs, E., ``Design Method Optimizes Scanning
 Phased Array,'' Microwaves, April 1982, pp.\ 69--70.

\bibitem{francis} Francis, M., ``Out-of-band response of array 
 antennas,'' Antenna Meas.  Tech. Proc., September 28--October 2,
1987, Seattle, p.~14.
\end{references}

For alphabetical references:

Maude Francis, (Francis, 1987) showed important new results
with array antennas.

\begin{alphareferences}
Francis, M., ``Out-of-band response of array 
antennas,'' Antenna Meas.  Tech. Proc., September 28--October 2,
1987, Seattle, p.~14.

Jacobs, E., ``Design Method Optimizes Scanning
Phased Array,'' Microwaves, April 1982, pp.\ 69--70.
\end{alphareferences}

\vskip12pt
You can also use Bibtex. See smjrnl.doc for documentation on 
using Bibtex.



%% Volume Table of Contents:

\begin{volumetoc}
\tocnumberline{Number 1}

\tocnumberline{Numbers 2/3 (Special Issue on Computational Learning
Theory)}

\tocnumberline{Number 4}

\TOCarticle{Explorations of an Incremental, Bayesian Algorithm for 
Categorization}{John R. Anderson and Michael Matessa}{275}

\TOCarticle{A Bayesian Method for the Induction of Probabilistic Networks
from Data}{Gregory F. Cooper and Edward Herskovits}{309}

\TOCarticle{Learning Boolean Functions in an Infinite Attribute Space}
{Avrim Blum}{373}

\TOCarticle{Technical Note: First Nearest Neighbor Classification on Frey 
and Slate's Letter Recognition Problem}{Terence C. Fogarty}{387}
\end{volumetoc}


%%%%  Issue Table of Contents %%%%

% \international % <== This command makes `An International Journal' appear
\begin{issuetoc}
\TOCarticle{Explorations of an Incremental, Bayesian Algorithm for 
Categorization}{John R. Anderson and Michael Matessa}{275}

\TOCarticle{A Bayesian Method for the Induction of Probabilistic Networks
from Data}{Gregory F. Cooper and Edward Herskovits}{309}

\TOCarticle{Learning Boolean Functions in an Infinite Attribute Space}
{Avrim Blum}{373}

\TOCarticle{Technical Note: First Nearest Neighbor Classification on Frey 
and Slate's Letter Recognition Problem}{Terence C. Fogarty}{387}

\end{issuetoc}

%%%% End of Issue Table of Contents %%%%

} % commentout

\end{article}
\end{document}

