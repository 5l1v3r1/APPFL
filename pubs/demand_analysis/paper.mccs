\documentstyle[fleqn,12pt]{article}

\setlength{\textwidth}{6.0in}
\setlength{\textheight}{8.75in}
\setlength{\oddsidemargin}{0.4in}
\setlength{\evensidemargin}{0.4in}
\setlength{\topmargin}{-0.375in}

\input{commands}
\sloppy

\begin{document}

\title{PERs from Projections for Binding-time Analysis\
\thanks{To appear in {\it The Journal of Lisp and Symbolic Computation.}}}

\author{Kei Davis}

\date{}

%\maketitle

\commentout{
\abstract{{\it First-order\/} projection-based binding-time analysis
has proven genuinely useful in partial evaluation
\cite{Lau91a}.  There have been three notable generalisations
of projection-based analysis to higher order.  The first lacked a
formal basis \cite{Mog89}; the second used structures strictly more
general that projections, namely {\it partial equivalence relations\/}
(PERs) \cite{HS91}; the third involved a complex construction that
gave rise to impractically large abstract domains \cite{Dav93b}.  This
paper presents a technique free of these shortcomings: it is simple,
entirely projection-based, satisfies a formal correctness condition,
and gives rise to reasonably small abstract domains.  Though the
technique is cast in terms of projections, there is also an
interpretation in terms of PERs.  The principal limitation of the
technique is the restriction to {\it monomorphic\/} typing. \\
\\
{\bf Keywords:} Binding-time analysis, abstract interpretation, projections,
PERs.}

\clearpage}

\section{Introduction and Background}

We take as given that binding-time analysis is essential for good
partial evaluation, and we do not address the issue of annotating
programs according to the results of analysis.  Numerous binding-time
analysis techniques have been proposed and implemented; we greatly
narrow the field of discussion by restricting attention to those for
which there is a formally stated notion of correctness that the
technique has been proven to satisfy.

Analysis techniques can usually be identified as being based on either
a non-standard denotational semantics or a non-standard typing.
Examples in the latter category include those of Gomard \cite{Go92},
Jensen \cite{Jen92}, the Nielsons \cite{NN88}, Schmidt \cite{Sch88},
and Henglein and Mossin \cite{HM94}.  Our focus is on those techniques
based on non-standard interpretation, in particular, those using
projections or partial equivalence relations (PERs) as the basic
abstract values.

A domain {\it projection\/} is a continuous idempotent function that
approximates the identity.  Launchbury \cite{Lau88} hit upon the idea
of using projections to encode degrees of staticness of data.  The
basic idea is that a projection maps to $\bot$ that part of a data
structure that is dynamic (possibly not determined), and acts as the
identity on that part which is static (definitely determined).
Examples are the identity $\ID$, the greatest projection, which
specifies that values are entirely static; the constant $\bot$
function $\BOT$, the least projection, which specifies that values are
entirely dynamic; and projections $\FST$ and $\SND$ on product
domains, defined by
\[
\FST\ (x,y) = (x,\bot)\ ,\hspace{1.0in} \SND\ (x,y) = (\bot,y)\ ,
\]
specifying staticness in the first and second components of pairs,
respectively.  The nominal goal of analysis is, given function $f$
denoted by some programming-language expression, and projection $\D$
encoding the staticness of the argument of $f$, to determine $\G$
satisfying the {\it safety condition\/} $\G\o\f\we\f\o\D$.  For
example, taking $\G$ to be $\SND$ satisfies
$\G\o\itt{swap}\we\itt{swap}\o\FST$ for $\itt{swap}$ defined by
$\itt{swap}~(x,y)=(y,x)$.  Taking $\G$ to be $\BOT$ always satisfies
the safety condition but tells nothing; greater $\G$ is more
informative.  Launchbury \cite{Lau91a} showed that this safety
condition satisfies, and in a sense which he formalises, is equivalent
to the correctness condition for binding-time analysis in the general
framework of Jones \cite{Jon88}.  Using projection-based analysis,
Launchbury implemented both monomorphic and polymorphic versions of a
partial evaluator for a first-order language.

There have been three notable attempts to generalise Launchbury's
techniques to higher order.  The first was Mogensen's generalisation
of the polymorphic technique \cite{Mog89}.  Though successfully
implemented, there is no formal statement of what it means for the
analysis to be correct; even if such a statement were made, proving
correctness would likely be difficult because of the highly
intensional nature of the analysis: the non-standard values associated
with expressions are strongly dependent on their syntactic structure,
and projections are encoded symbolically as {\it abstract closures\/},
with approximation performed algebraically `on-the-fly' according to
time and space considerations.  Nonetheless, the experiment provided
evidence for the {\it practicality\/} of the projection-based approach
at higher order.

The second generalisation was Hunt and Sands', of the monomorphic
technique to higher order \cite{HS91}.  Their observation was that a
projection, regarded as a set of domain-range pairs, is an {\it
equivalence relation\/}: given $\G$, values $u$ and $v$ are in the
same equivalence class if $\G~u=\G~v$, and the canonical elements of
the equivalence classes are the set of fixed points (range) of $\G$.
Hunt showed that the safety condition $\G\o\f\we\f\o\D$ holds iff $f$
is related to itself by $\D\fto\G$ where $\fto$ is the standard
operation on binary relations, so 
"" 
	(\G\o\f\we\f\o\D)\
	\Leftrightarrow\ 
	(\forall u,v\ .\ \D\ u=\D\ v\ \Rightarrow\ \G\ (f\ u)=\G\ (f\ v))\ .  
""
Then, for example, $(\BOT\fto\ID)~(f,f)$ asserts
that $f$ maps dynamic arguments to static results.  In general
$\D\fto\G$ is not an equivalence relation, but it is always a {\it
partial\/} equivalence relation: it is symmetric and transitive but
not necessarily reflexive.  (Partialness is crucial: if $\D\fto\G$
were in general reflexive then $(\D\fto\G)(f,f)$ would hold for all
$f$, hence such assertions would tell nothing.)  Unlike projections
regarded as relations, PERs are closed under $\fto$; the result, as
Hunt and Sands show, is that `scaling up' to higher-order analysis is
reasonably straightforward.  One disadvantage of their method is that
PER spaces are considerably larger than the projection spaces on the
same domains, and it is not clear which PERs to choose for (finite)
abstract domains.  Here they borrowed heavily from the projection
world, using standard abstract projection domains at ground types.
Further, its practicality has not been demonstrated by implementation,
and, because of the unfamiliar territory, a promising route to a
polymorphic generalisation is obscure.

The third generalisation was ours, to an entirely projection-based,
monomorphic, higher-order technique \cite{Dav93b}.  One observation
motivating the approach is that there is no meaningful abstraction of
values to projections, only of functions $f$ to projection
transformers $\T$ (functions from projections to projections)
satisfying $(\T~\D)\o\f\we\f\o\D$ for all $\D$.  To make this
abstraction possible a semantics intermediate between the standard and
analysis semantics was introduced.  Moving from the standard to
intermediate semantics involved a translation of each ground type @T@
to a function type @E@\ @->@\ @T@ (for a fixed type @E@), the values
of which, being functions, could then be abstracted.  The result,
while proven correct with respect to a formal safety condition, is
probably not practicable because of the growth in the sizes of
(usefully rich) abstract domains induced by the type translation.

This paper presents a technique far simpler technique than our
previous one.  No intermediate semantics is required, and the
correctness condition and proof are much simpler.  Because the
translation of ground types to function types is avoided the abstract
domains are much smaller. Though entirely projection-based, we show
that there is a reading of the results in terms of PERs, intimating a
close relationship with the PER-based technique.

\section{Language and Standard Semantics}

The source language is a simple, strongly typed, monomorphic,
non-strict functional language.  The grammar for the language of types
and type definitions is given in Figure~\ref{fig:types}.
\fig{tbp}{Types and type definitions.}{fig:types}{
""
@T@	&::=&	\ta			&\reason{Type Name}
	&|&	\tint			&\reason{Integer}
	&|&	\tprod			&\reason{Product, $n\geq0$}
	&|&	\tsum			&\reason{Sum, $n\geq1$} 
	&|&	\tfun			&\reason{Function} \espace 
@D@	&::=&	\tdefn			&\reason{Type Definitions}
""
}
Nullary product corresponds to the so-called {\it unit\/} type.  A
unary product @(T)@ will always have the same semantics as @T@.  The
types used in the examples are defined as follows.
@@@
     FunList =   nil () 
               + cons (Int -> Int, FunList) , 

     FunTree =   leaf (Int -> Int) 
	       + branch (FunTree, FunTree) .
@@@

The grammar for expressions is given in Figure~\ref{fig:exprs}.
Addition for integers is provided as typical of operations on flat data
types in this setting.  A unary tuple @(e)@ will always have the same
semantics as @e@.  The (monomorphic) typing of expressions is entirely
standard and is omitted.
\fig{tbp}{Expressions.}{fig:exprs}{
""
@e@	&::=&	\evar				&\reason{Variable} 
	&|&	\enum				&\reason{Numeral}
	&|&	\esum				&\reason{Integer addition}
	&|&	\eapp				&\reason{Function application}
	&|&	\etup				&\reason{Tuple construction}
	&|&	@let@\ @(@\xone@,@\ldots@,@\xn@)@\ @=@\ \enot\ @in@\ \eone			&\reason{Tuple decomposition}
	&|&	\econ				&\reason{Sum construction}
	&|&	@case@\ \enot\ @of @\{\ci\ \xxi\ @->@\ \ei\}	&\reason{Sum decomposition}
	&|&	\elamt				&\reason{Lambda abstraction}
	&|&	\eapp				&\reason{Function application}
	&|&	\efix				&\reason{Fixed point}
""
}

\subsection{Expression semantics}
Since two different expression semantics will be given, following
Abramsky \cite{Abr90} we define a semantics $\cE{}$ parameterised by a
set of {\it defining constants}.  The semantics $\cE{}$ is defined in
Figure~\ref{fig:gensem}; the defining constants are {\it plus},
$\itt{sel}_i$, {\it tuple}, $\itt{inc}_i$, $\itt{outc}_i$, {\it
choose}, {\it mkfun}, {\it apply}, and {\it fix}.  The two instances
of $\cE{}$ are distinguished by a superscript: $\cE{\Ss}$ for the
standard semantics and $\cE{\Ps}$ for the non-standard semantics.  The
corresponding type semantics, and defining constants, have the same 
superscripts.

It is useful to regard the free-variable environment of each
expression as having some tuple type $\tprod$, and environment lookup
as indexing (as in a categorical semantics, or De~Bruijn indexing);
variables are indexed implicitly or explicitly by their index in the
free variable environment.  Then for both versions of the semantics
and all expressions @e@ of type @T@ with environment of type $\tprod$,
""
	\Ee{}{@e@}\ \in\ \Te{}{\tprod}~\fto~\Te{}{@T@}\ .
""
Noting that $\R\br{\xxi}$ is short for $\itt{sel}_i~\R$, environment
update $\R[\xxi\mapsto{}v]$ is defined by
"""
	tuple (sel_1 \R, \ldots, sel_{i-1} \R, v, sel_{i+1} \R, \ldots, sel_n \R) .
"""


\fig{tbp}{Parameterised semantics.}{fig:gensem}{
""
	\Ee{}{\evar_i}\ \R = \R\br{@x@_i} = \itt{sel}_i\ \R\espace\espace
%	\Ee{}{\eunit}\ \R = \itt{mkunit}\ \R\espace\espace
%	\Ee{}{\enum}\ \R = \itt{mkint}_i\espace\espace
	\Ee{}{\esum}\ \R = \itt{plus}~(\Ee{}{\eone}\ \R,\ \Ee{}{\etwo}\ \R)\espace\espace
	\Ee{}{\etup}\ \R = \itt{tuple}~(\Ee{}{\eone}\ \R,~\ldots,~\Ee{}{\en}\ \R)\espace\espace
	\Ee{}{\esel}\ \R
	\ \ \ \ \ = \Ee{}{\eone}\ \R[\xxi\mapsto\itt{sel}_i\ (\Ee{}{\enot}\ \R)~|~\oin]\espace\espace
	\Ee{}{\econ}\ \R = \itt{inc}_i\ (\Ee{}{@e@}\ \R)\espace\espace
	\Ee{}{\ecase}\ \R
	\ \ \ \ \  = \itt{choose}~(\ba[t]{l}\Ee{}{\enot}\ \R,
	\Ee{}{\eone}\ \R[\xone\mapsto\itt{outc}_1\ (\Ee{}{\enot}\ \R)],
	\ \ \ ~\vdots
	\Ee{}{\en}\ \R[\xn\mapsto\itt{outc}_n\ (\Ee{}{\enot}\ \R)])\ea\espace\espace
	\Ee{}{\elam}\ \R = \itt{mkfun}\ (\L{}x.\Ee{}{@e@}\ \R[\evar\mapsto{}x])\espace\espace
	\Ee{}{\eapp}\ \R = \itt{apply}\ (\Ee{}{\eone}\ \R)\ (\Ee{}{\etwo}\ \R)\espace\espace
	\Ee{}{\efix}\ \R = (\itt{fix}\o\itt{apply})\ (\Ee{}{@e@}\ \R)
""
}

\subsection{Standard semantics}

The standard $\Ss$ type semantics is defined in Figure~\ref{fig:ssem};
the standarde expression semantics is defined in
Figure~\ref{fig:ssems}.  Domain $\plus$ is separated (lifted) sum.  For
more generality, products are unlifted; a unary sum-of-products gives
a more usual lifted product.  Function types give rise to lifted
function spaces as in Abramsky's lazy lambda calculus \cite{Abr89}.
The important feature of this approach is the modelling of the fact
that functions are, in effect, embedded in the simplest of lazy data
structures, which either have no WHNF---have value $\bot$---or have a
WHNF and contain an applicable function value---have value
$\itt{lift}~f$ for some $f$.  (This can be made even more explicit by
introducing a unary sum type, whence "\L" becomes a constructor
\cite{Dav94}.)  Recursive type definitions give rise to recursive
domain specifications which have the usual least-fixed-point
solutions.

\fig{tbp}{Standard type and expression semantics.}{fig:ssem}{
"""
\Te{\Ss}{\tint} = \Int = \lift{\bf{Z}} \espace\espace
\Te{\Ss}{\tprod} = \Te{\Ss}{\tone}\ \times\ \ldots\ \times\ \Te{\Ss}{\tn}\espace\espace
\Te{\Ss}{\tsum} = \Te{\Ss}{\tone}\ \plus\ \ldots\ \plus\ \Te{\Ss}{\tn}\espace\espace
\Te{\Ss}{\tfun} = \liftp{\Te{\Ss}{\tone} \fto \Te{\Ss}{\ttwo}}
"""
}

\fig{tbp}{Standard type and expression semantics.}{fig:ssems}{
"""
%	\itt{mkunit}^{\Ss}\ \R			 = 	()\espace\espace
%	\itt{mkint}^{\Ss}_n\ \R			 = 	\itt{lift}\ n\espace\espace
	\itt{plus}^{\Ss}\ (x,y) 		 = 	x+y\espace\espace
	\itt{tuple}^{\Ss}\ (x_1,\ldots,x_n)	 = 	(x_1,\ldots,x_n)\espace\espace
	\itt{sel}_i^{\,\Ss}\ (x_1,\ldots,x_n)	 = 	x_i\espace\espace
	\itt{inc}_i^{\Ss} 			 = 	\itt{in}_i\o\itt{lift}\espace\espace
	\itt{outc}_i^{\Ss}\ x			 = 	\itt{drop}\o\itt{out}_i\espace\espace
	\ba{lll}\itt{choose}^{\Ss}\ (\bot,&\ x_1,\ldots,x_n)		 = \bot
	\itt{choose}^{\Ss}\ (\itt{in}_i~v,&\ x_1,\ldots,x_n)	 = x_i\ea\espace\espace
	\itt{mkfun}^{\Ss}		 = 	\itt{lift}\espace\espace
	\itt{apply}^{\Ss}		 = 	\itt{drop}\espace\espace
	\itt{fix}^{\Ss}  		 = 	\itt{lfp} \reason{Least fixed point}
"""
}

\section{Domain Factorisation}

A key observation of our previous work
\cite{Dav93b} is that since there is no concept
of staticness of the body of a lambda expression, there is no point in
having projections on function spaces.  Hence values are factored into
their evaluable or {\it data\/} parts, and their unevaluable but
applicable {\it forward\/} parts.  For example, for expressions of
function type ${@T@\ @->@\ @U@}$, with
$\liftp{T\fto{}U}=\Te{\Ss}{@T@\ @->@\ @U@}$, we distinguish two
degrees of definedness---between values of the form
$\itt{lift}~f$ and $\bot$, corresponding to expressions that do and
do not have a WHNF, respectively.  These two degrees of definedness 
may be encoded by the two-point
domain $\lone=\{\bot, \itt{lift}~()\}$---the {\it data domain\/} corresponding
to type ${@T@\ @->@\ @U@}$.  There are two distinct projections on $\lone$,
namely $\ID$ and $\BOT$: denoted functions are 
either static or dynamic.  Here 
the forward domain is $T\fto{}U$, and the factorisation of $\liftp{T\fto{}U}$
is $\lone\times(T\fto{}U)$.  The factorisation function
${\it{}fac}_{@@@T->U@@@}\in\liftp{T\fto{}U}\fto(\lone\times(T\fto{}U))$
is an embedding defined by
${\it{}fac}_{@@@T->U@@@}~\bot=(\bot,\bot)$, and
${\it{}fac}_{@@@T->U@@@}~(\itt{lift}~f)=(\lunit,~f)$ for all $f$.
Every embedding {\it determines\/} a corresponding projection; 
this follows from the relations $g\o h\we\itt{id}$ and $h\o\g=\itt{id}$
that hold between an embedding $g$ and its corresponding projection $h$.
The importance of factorisation being an embedding is that no information
is lost---values may be recovered by unfactoring.  Here the
projection is given by
${\it{}unfac}_{@@@T->U@@@}~(\bot,f)=\bot$ and
${\it{}unfac}_{@@@T->U@@@}~(\lunit,~f)=\itt{lift}~f$.

Generally, the data domain corresponding to type @T@ is $\De{}{@T@}$,
where $\cD{}$ is defined exactly like $\cT{\Ss}$ except that function
spaces are replaced by the one-point domain $\one$, that is,
$\De{}{\tfun}=\lift{\one}$ for all $\tone$ and $\ttwo$.  
The function $\data$ from values in
$\Te{\Ss}{@T@}$ to their data parts in $\De{}{@T@}$ is a projection:
it is like the identity except that values from function spaces are
mapped into $\one$.  The projection $\data$ is defined in terms of the
structure of @T@ as given in Figure~\ref{fig:fact}.
\fig{tbp}{Factorisation and unfactorisation functions.}{fig:fact}{
""
	{\it{}data}_{\stint} = {\it{}id}_{\stint}\ ,\espace
	{\it{}data}_{\stprod} = {\it{}data}_{\stone} \times \ldots \times {\it{}data}_{\stn} ,\espace
	{\it{}data}_{\stsum} = {\it{}data}_{\stone} \plus \ldots \plus {\it{}data}_{\stn} ,\espace
	{\it{}data}_{\stfun} = {\liftp{\Lx.\unit}} .
""

"""
	{\it{}for}_{\stint} = \Lx.\unit\ ,\espace
	{\it{}for}_{\stprod} = {\it{}for}_{\stone}\ \times\ \ldots\ \times\ {\it{}for}_{\stn}\ ,\espace
        \ba{lll}{\it{}for}_{\stsum}\ \bot	&\ = \bot
	{\it{}for}_{\stsum}\ (\itt{in}_i~v) &\ = (\bot,\ldots,\bot,v,\bot_,\ldots,\bot)	&\ \ \reason{$v$ in $\ith$ position}\ ,\ea\espace
	{\it{}for}_{\stfun} = \itt{drop}\ .
"""

"""
	\itt{unfac}_{\stint} (n,()) = n ,\espace
	\itt{unfac}_{\stprod} ((d_1,...,d_n), (f_1,...,f_n)) = (v_1,...,v_n)
	~~~{\rm{}where} 
	~~~~~~v_i=\itt{unfac}_{\sti} (d_i,f_i), \oin ,\espace
	\itt{unfac}_{\stsum} ((d_1,...,d_n), (f_1,...,f_n)) = (v_1,...,v_n)
	~~~{\rm{}where} 
	~~~~~~v_i=\itt{unfac}_{\sti} (d_i,f_i), \oin ,\espace
	\itt{unfac}_{\stfun} (\bot,f) = \bot,
	\itt{unfac}_{\stfun} (\lunit, f) = \itt{lift} f . 
"""
}
The definition of ${\it{}data}_{\stfun}$ uses function lifting, 
defined by $\lf~\bot=\bot$
and $\lf~(\itt{lift}~x)=\itt{lift}~(f~x)$.  Recursive type definitions
give rise to recursive function specifications with exactly one solution.

In the same style as  $\De{}{@T@}$ and $\data$ we define
$\Tde{\Ss}{@T@}$ to give the forward domain for @T@, and $\for$ to be
the function mapping values in $\Te{\Ss}{@T@}$ to their forward parts
in $\Tde{\Ss}{@T@}$, as follows.  Roughly, $\cTd{\Ss}$ is like
$\cT{\Ss}$ with all lifting removed and sum replaced by product, so
""
	\Tde{\Ss}{\tint} = \one ,\espace
	\Tde{\Ss}{\tprod} = \Tde{\Ss}{\tone} \times \ldots \times \Tde{\Ss}{\tn} ,\espace
	\Tde{\Ss}{\tsum} = \Tde{\Ss}{\tone} \times \ldots \times \Tde{\Ss}{\tn} ,\espace
	\Tde{\Ss}{\tfun} = \Te{\Ss}{\tone} \fto \Te{\Ss}{\ttwo} .
""
The mapping from standard values to their forward parts is defined in
Figure~\ref{fig:fact}.
We write $\fac$ for $\Lx.(\data~x,\ \for~x)$.  Then
$\De{}{@T@}\times\Tde{\Ss}{@T@}$ is a factorisation of
$\Te{\Ss}{@T@}$, and
""
	\fac \in \Te{\Ss}{@T@} \fto (\De{}{@T@} \times \Tde{\Ss}{@T@})
""
is an embedding which, as mentioned, determines the corresponding projection
$\unfac$ as defined in Figure~\ref{fig:fact}.

We give some further examples.  The factorisation of
$\Int$ is $\Int\times\one$; more generally, the factorisation of any
domain $D$ corresponding to a type not containing @->@ is just
$D\times\one$.  For @FunList@ the standard domain is
"""
	\mu{}X . \lone+(\liftp{\itt{Int}\fto\itt{Int}}\x{}X) ,
"""
that is, lists of functions from integers to integers. In factored form
the domain is
"""
	(\mu{}X . \lone+(\lone\x{}X)) \x (\mu{}Y . (\itt{Int}\fto\itt{Int})\x{}Y) ,
"""
that is, pairs consisting of a list of values from $\{\bot,\ \lbot\}$, and
an infinite tuple of functions from integers to integers.  Then the 
standard value of the list @cons (\x.x-1, cons (\x.x-2, nil ()))@ is
"""
	\itt{incons} (\itt{lift} (\L{}x.x-1), \itt{incons} (\itt{lift} (\L{}x.x-2), \itt{innil} ())) ,
"""
the data value is
"""
	\itt{incons} (\lunit, \itt{incons} (\lunit, \itt{innil} ())) ,
"""
and the forward value is 
$(\L x.x-1,~(\L x.x-2,~(\bot,~(\bot,~(\bot,~\ldots)))))$.

\section{Projection Semantics}

Standard values $v$ and $v'$ are related by nonstandard value
$(\A,\K)$ when their data parts are related by projection $\A$, and
their forward parts are {\it logically related\/} \cite{Abr90} by
$\K$.  At each type @T@ the relation is
$\Re{}{@T@}~((\A,\K),\cdot,\cdot)$, defined by
""
	{\Re{}{@T@}~((\A,\K),v,v')} =&\mit(\A~d=\A~d')\ \wedge\ \Rde{}{@T@}~(\K,f,f')
		&\mbox{\rm{}where}
		&\ \ \ \ba[t]{lll}(d,f)  &\:= \fac~v
				\mit(d',f')	&\:= \fac~v' ,\ea
""
where $\Rde{}{@T@}$ is defined by
""
	\Rde{}{\tint}~((),(),()) = \itt{True}\ ,\espace
	\Rde{}{\tprod} = \Rde{}{\tone}\ \x\ \ldots\ \x\ \Rde{}{\tn}\ ,\espace
	\Rde{}{\tsum} = \Rde{}{\tone}\ \x\ \ldots\ \x\ \Rde{}{\tn}\ ,\espace
	\Rde{}{\tfun} = \Re{}{\tone}\ \fto\ \Re{}{\ttwo}\ .
""
Here $\x$ and $\fto$ are the standard operations on {\it ternary\/}
relations:  for relations $R$ and $S$ we have
$(R\times{}S)((x,x'),(y,y'),(z,z'))$ iff $R(x,y,z)$ and
$S(x',y',z')$, and $(R\fto{}S)(f,g,h)$ iff for all $x$, $y$, and $z$
such that $R(x,y,z)$ we have $S(f~x,\ g\ y,\ h~z)$.  Here, recursive
type definitions give recursive relation specifications, which have
inclusive least-fixed-point solutions. (A relation is inclusive if,
when it holds for each element of a directed set, and in particular
an ascending chain, it also holds at
the limit.  Such relations are sometimes called {\it admissible\/} or
{\it directed complete}.  Some work is required to show inclusivity of
these recursively-defined relations.) Thus the $\Ps$ type semantics
$\cT{\Ps}$ must be
""
	\Te{\Ps}{@T@} = \proj{\De{}{@T@}}\ \times\ \Tde{\Ps}{@T@}\ ,
""
where $\proj{\De{}{@T@}}$ is the lattice of projections on domain
$\De{}{@T@}$, and $\cTd{\Ps}$ is defined exactly like $\cTd{\Ss}$ with
superscript $\Ps$ everywhere replacing superscript $\Ss$.

\fig{tbp}{Projection semantics.}{fig:fsem}{
""
%	\itt{mkunit}^{\Ps}\ \R			 = 	()
%\espace\espace
%	\itt{mkint}^{\Ps}_n\ \R			 = 	\itt{lift}\ n
%\espace\espace
	\itt{plus}^{\Ps}\ ((\A,()),(\B,())) = \left\{\ba{ll}(\ID,()),	&\ \ \ \ \mbox{\rm if $\A=\ID$ and $\B=\ID$}
			(\BOT,()),	&\ \ \ \ \mbox{\rm otherwise}\ea\right.\espace
	\itt{tuple}^{\Ps}\ ((\A_1,\K_1),\ldots,(\A_n,\K_n)) = ((\A_1\x\ldots\x\A_n),(\K_1,\ldots,\K_n))\espace
	\itt{sel}_i^{\,\Ps}\ ((\A_1\x\ldots\x\A_n),(\K_1,\ldots,\K_n))  = (\A_i,\K_i)\espace
	\itt{inc}_i^{\Ps}\ (\A,\K)	 = 	(C_i\ \A,\ (\top,\ldots,\top,\K,\top,\ldots,\top))\espace
	\itt{outc}_i^{\Ps}\ (\A,(\K_1,\ldots,\K_n))	 = (\OUTC_i\ \A,\ \K_i)\espace
	\itt{choose}^{\Ps}\ ((\A,\K),x_1,\ldots,x_n) = \left\{\ba{ll}\bot,		&\ \ \ \ \mbox{\rm if}\ \A\not\se\glb_{\oin} C_i~\BOT
			x_1\glb\ldots\glb{}x_n,	&\ \ \ \ \mbox{\rm otherwise}\ea\right.\espace
	\itt{mkfun}^{\Ps}\ f			 = 	(\ID,f)\espace
	\itt{apply}^{\Ps}\ (\A,f) = \left\{\ba{lll}\bot,	&\ \ \ \ \mbox{\rm if}\ \A=\BOT
						f,	&\ \ \ \ \mbox{\rm if}\ \A=\ID \ea\right.\espace
	\itt{fix}^{\Ps} = 	\itt{gfp} \reason{Greatest fixed point}
""
}

The defining constants for the projection semantics $\cE{\Ps}$ are
given in Figure~\ref{fig:fsem}. The following notation is used for
specifying projections on the data domains.  For sum type $\tsum$ with
data domain $D_1\plus\ldots\plus D_n$, define $C_i~\A$ to be
$\ID+\cdots+\ID+\A+\ID+\cdots+\ID$ where $\A$ is the $i^{th}$ summand,
$\OUTC_i~(\G_1+\cdots+\G_n)$ to be $\G_i$, and $\OUTC_i~\BOT$ to be
$\BOT$.

\ \\ \noindent {\bf The Central Result.} For expression @e@ of type
@T@ and free-variable environment type @E@, the functions
$\Ee{\Ps}{@e@}$, $\Ee{\Ss}{@e@}$, and $\Ee{\Ss}{@e@}$ are logically
related by $\cR{}$, that is
""
	(\Re{}{@E@}\fto\Re{}{@T@})\ (\Ee{\Ps}{@e@},\ \Ee{\Ss}{@e@},\ \Ee{\Ss}{@e@})\ .
""
Further, $\Re{}{@T@}~((\A,\K),\cdot,\cdot)$ is a PER for all $\A$ and
$\K$.\ \ $\Box$

\ \\ \noindent The bulk of proof is omitted; it consists of showing
that the defining constants are similarly related, and a simple
induction on the structure of expressions showing that if the defining
constants are logically related, then so are the semantic functions
$\cE{\Ps}$, $\cE{\Ss}$, and $\cE{\Ss}$.
By way of example we consider the expression form $\esum$ and the
relevant defining constant $\it plus$.  To show that $\it plus^{\Ss}$
and $\it plus^{\Ps}$ are correctly related we need to show that
"""
	((\Re{}{\tint}\x\Re{}{\tint})\fto\Re{}{\tint}) (plus^{\Ps},plus^{\Ss},plus^{\Ss}) .
"""
By the definitions of $\fto$ and $\x$ on ternary relations, this is
equivalent to 
""
	(\Re{}{\tint}\x\Re{}{\tint}) (x,y,z) \limp \Re{}{\tint}\
		\ (plus^{\Ps} x, plus^{\Ss} y, plus^{\Ss} z) ,
""
for all $x$, $y$, and $z$.  In turn, this is equivalent to
"""
	&\ \Re{}{\tint} (x,y,z) \land \Re{}{\tint} (x',y',z')
\limp	&\ \Re{}{\tint} (plus^{\Ps} (x,x'), plus^{\Ss} (y,y'), plus^{\Ss} (z,z'))
"""
for all $x$, $x'$, $y$, $y'$, $z$, and $z'$; equivalently, for
all values $y$, $y'$, $z$, and $z'$ from the integer domain
$\Te{\Ss}{\tint}$ and
all projections $\A_1$ and $\A_2$ on this domain, we have
""
	\mit(\ID y=\ID z) \land (\ID y'=\ID z') \limp \mit\ID (y+y')=\ID (z+z') ,
""
which trivially holds, and for $\A_1\neq\ID$ or $\A_2\neq\ID$ that
""
	\mit(\A_1 y=\A_1 z) \land (\A_2 y'=\A_2 z') \limp \mit\BOT (y+y')=\BOT (z+z') ,
""
which is also trivial.  For the inductive case $\esum$ with 
environment type @E@ we need to show that if
""
	(\Re{}{@E@}\fto\Re{}{\tint}) (\Ee{\Ps}{\eone}, \Ee{\Ss}{\eone}, \Ee{\Ss}{\eone}) 
""
and
""
	(\Re{}{@E@}\fto\Re{}{\tint}) (\Ee{\Ps}{\etwo}, \Ee{\Ss}{\etwo}, \Ee{\Ss}{\etwo}) 
""
then
"""
	(\Re{}{@E@}\fto\Re{}{\tint}) (\Ee{\Ps}{\esum},\Ee{\Ss}{\esum},\Ee{\Ss}{\esum}) .
"""
Suppose $\Re{}{@E@}~(\R^{\Ps},\R^{\Ss},\R'^{\Ss})$.  Then by 
the induction hypothesis we have that
$\Re{}{\tint}~(\A_i,v_i,v'_i)$, where 
$(\A_i,())=\Ee{\Ps}{\ei}~\R^{\Ps}$,
$(v_i,())=\Ee{\Ps}{\ei}~\R^{\Ss}$, and
$(v'_i,())=\Ee{\Ps}{\ei}~\R'^{\Ss}$, for $i=1,2$.
Now
"""
	\Re{}{\tint} (\Ee{\Ps}{\esum} \R^{\Ps}, \Ee{\Ss}{\esum}~\R^{\Ss}, \Ee{\Ss}{\esum}~\R'^{\Ss})
"""
iff
"""
	\Re{}{\tint} (&plus^{\Ps} ((\A_1,()),(\A_2,())), 
		      &plus^{\Ss} \mit((v_1,()),(v_2,())), 
		      &plus^{\Ss} \mit((v'_1,()),(v'_2,()))) ,
"""
which holds since $\it plus^{\Ps}$ and $\it plus^{\Ps}$, and their
arguments, are correctly related.


\section{Abstract Domains}

At each type @T@ we require a finite abstraction of $\Te{\Ps}{@T@}$.
This abstract domain is ${\it{}FProj}_{\stt}\times{\it{}FFor}_{\stt}$,
where ${\it{}FProj}_{\stt}$ is a finite abstraction of the lattice
$\proj{\De{}{@T@}}$, and ${\it{}FFor}_{\stt}$ is a finite abstraction
of $\Tde{\Ps}{@T@}$.  The definition of ${\it{}FProj}_{\stt}$ is based
on Launchbury's \cite{Lau91a}.  A projection $\G$ is in
${\it{}FProj}_{\stt}$ if $\G~\mbox{\bf proj}~@T@$ can be inferred from
the rules given in Figure~\ref{fig:infrules}.
%
\fig{tbp}{Inference rules for finite projection domains.}{fig:infrules}{
""
\begin{array}{lll}\
\axm{\BOT\ \ \mbox{\bf{}proj}\ \ \tint}\ \ \ \ \ \ \ \ \ \ & \axm{\ID\ \ \mbox{\bf{}proj}\ \ \tint}\
\espace\espace
\axm{\BOT\ \ \mbox{\bf{}proj}\ \ \tfun} & \axm{\ID\ \ \mbox{\bf{}proj}\ \ \tfun}\end{array}\
\espace\espace
\axm{\BOT\ \ \mbox{\bf{}proj}\ \ \tsum}\
\espace\espace
\infero{\G_1\ \ \mbox{\bf{}proj}\ \ \tone\ \ \cdots\ \ \G_n\ \ \mbox{\bf{}proj}\ \ \tn}{(C_1\ \G_1)\glb\cdots\glb(C_n\ \G_n)\ \ \mbox{\bf{}proj}\ \ \tsum}\
\espace\espace
\infero{\G_1\ \ \mbox{\bf{}proj}\ \ \tone\ \ \cdots\ \ \G_n\ \ \mbox{\bf{}proj}\ \ \tn}{\G_1\x\ldots\x\G_n\ \ \mbox{\bf{}proj}\ \ \tprod}\
\espace\espace
\infer{\ba{ll}&\G_1\ \ \mbox{\bf{}proj}\ \ \aone\ \ \cdots\ \ \G_n\ \ \mbox{\bf{}proj}\ \ \an
\vdash & P(\G_1,...,\G_n)\ \ \mbox{\bf{}proj}\ \ \ti@(@\aone@,...,@\an@)@\ea}{\mu(\G_1,\ldots,\G_n).P(\G_1,\ldots,\G_n)\ \ \mbox{\bf{}proj}\ \ \ai}\
\ \ \ \ \ [\mbox{\rm{}where}\ \ai@=@\ti@(@\aone@,...,@\an@)@]
""
}
For recursively-defined types the rules yield only those projections
that are {\it uniform}---that act on each recursive instance of a 
data structure in the same
way.  Thus ${\it{}FProj}_{@@@FunList@@@}$ comprises $\BOT$ and
$\SPINE~\A$ for $\A$ ranging over $\BOT$ and $\ID$, where
""
	\SPINE\ \A = \mu\G.(\itt{NIL}\ \ID)\ \glb\ (\itt{CONS}\ (\A\ \x\ \G))\ ,
""
so $\SPINE\ \BOT$ specifies static spines and dynamic elements, and
$\SPINE\ \ID$ is the identity, thus specifying static spines and
static elements.  More generally, the abstract list
constructor is isomorphic to lifting.  Similarly,
${\it{}FProj}_{@@@FunTree@@@}$
comprises $\BOT$ and $\LBR\ \A$ for $\A$ ranging over $\BOT$ and $\ID$, where
""
	\LBR\ \A = \mu\G.(\itt{LEAF}\ \A)\ \glb\ (\itt{BRANCH}\ (\G\ \x\ \G))\ ,
""
so $\LBR\ \A$ specifies static branches and leaves, and $\A$ of the
values in the leaf nodes.  Again, the abstract tree constructor is
isomorphic to lifting.  This compares favourably with BHA strictness
analysis, for which the corresponding abstract constructors are
typically {\it double\/} lifting \cite{Wad87,Sew93}.


Value $\K$ is in ${\it{}FFor}_{\stt}$ if $\K~\bfabsf~@T@$ can be
inferred from the following.
First, there is only one forward value at type $\tint$.
""
	\axm{() {} \bfabsf {} \tint} .
""
For products and sums,
""
\infer{\K_1 {} \bfabsf {} \tone { } \cdots { } \K_n {} \bfabsf {} \tn}\
{(\K_1,\ldots,\K_n) {} \bfabsf {} \tprod} ,
""
%\espace\espace
""
\infero{\K_1 {} \bfabsf {} \tone { } \cdots { } \K_n {} \bfabsf {} \tn}\
{(\K_1,\ldots,\K_n) { } \bfabsf {} \tsum} .
""
Function spaces consist of a set of step functions closed under lub.
""
\infero{\ba{ll}\T_1\in{\it{}FTran}_{\stone} { { } } \K_1 \bfabsf \tone
\T_2\in{\it{}FTran}_{\sttwo} { { } } \K_2 \bfabsf \ttwo\ea}\
{\it{}step ((\T_1,\K_1),(\T_2,\K_2)) {} \bfabsf {} \tfunp} ,
""
where
""
	step (v_1,v_2) x = v_2,		&{\rm if} v_1\we{}x 
	step (v_1,v_2) x = \bot,	&{\rm otherwise} ,
""
and
""
\infer{\K_1 {} \bfabsf {} \tfunp { { } } \K_2 {} \bfabsf {} \tfunp}\
{(\K_1\lub\K_2) {} \bfabsf {} \tfunp} .
""
This gives the full space of monotonic functions on the abstract
domains.

For recursively-defined types, roughly speaking, we choose those
forward values that represent each component of the same type by the
same value.  Given type definitions $\tdefn$, which will be written
$@A@_i@ = T@_i@(A@_1@,...,A@_n@)@,\ \oin$, if from assumptions
$\K_i~\bfabsf~\ai$ for $\oin$ it may be deduced that
$P_i(\K_1,\ldots,\K_n)~~\bfabsf~~\ti(\aone\ldots\an)$ for $\oin$, then
""
	\mu(\K_1,...,\K_n).(P_1(\K_1,...,\K_n),...,P_n(\K_1,...,\K_n))
""
is a tuple $(\K_1,...,\K_n)$ such that $\K_i\ \bfabsf\ @A@_i$
for $\oin$.
For all @T@ the set ${\it{}FProj}_{\stt}\times{\it{}FFor}_{\stt}$ is a 
finite lattice containing the top and bottom elements of $\Te{\Ps}{@T@}$.

For @T@ not containing @->@ the domain $\Tde{\Ps}{@T@}$ is isomorphic
to $\one$, so ${\it{}FFor}_{\stint}$ is $\one$.  For @Int@\ @->@\ @Int@
we have
$\Tde{\Ps}{@Int@\ @->@\ @Int@}=(\proj{\De{}{\tint}}\x\one)\fto(\proj{\De{}{\tint}}\x\one)$,
so ${\it{}FFor}_{@@@Int->Int@@@}$ is
$({\it{}FProj}_{\stint}\x\one)\fto({\it{}FProj}_{\stint}\x\one)$.  A
data structure of recursive type @A=T(A)@ may be thought of as some
(possibly infinite) number of elements of @T(())@.  For example, the
value $\itt{cons}\ (f,\ \itt{cons}\ (g,\ \itt{nil}\ ()))$ in the
standard domain for @FunList@ decomposes into $\itt{cons}\ (f,())$,
$\itt{cons}\ (g,())$, and $\itt{nil}\ ()$.  The (implicit) abstraction
function maps such a data structure to the greatest lower bound of
these elements, giving a safe abstraction of the nonstandard values of
each element.  Thus
${\it{}FFor}_{@@@A=T(A)@@@}={\it{}FFor}_{@@@T(())@@@}$, so
${\it{}FFor}_{@@@FunList@@@}\iso{\it{}FFor}_{@@@FunTree@@@}\iso{\it{}FFor}_{@@@Int->Int@@@}$.



\section{Examples of Analysis}

For all closed expressions @e@ the abstract value
$\Ee{\Ps}{@e@}~[\,]$ of @e@ is of the form $(\ID,\K)$, showing
that closed expressions are always entirely static.  For expressions of
function type the abstract forward value $\K$ is a function from
abstract arguments of @e@ to abstract results.

First we consider functions on lists.
Let expression @length@ denote usual length function:
@@@
     fix (\length . \xs . case xs of
                            nil u  -> 0
                            cons p -> let (z,zs) = p in
                                        1 + length zs)
@@@
The abstract forward value of @length@ is of the form
$\L(\A,\K).(\T~\A,())$, where $\T$ maps $\SPINE~\BOT$ and $\SPINE~\ID$
to $\ID$, and $\BOT$ to $\BOT$.  This reveals that the result of
@length@ is independent of the values of list elements, and gives
a static result when the argument has a static spine.  It is worth
pointing out that the result of @length@ is static---determined---for
infinite-length arguments with static spines; staticness and definedness
are independent properties.


Let @append@ stand for the expression denoting the usual function for
appending two lists:
@@@
     fix (\append . \xs . \ys . case xs of
                                  nil u  -> ys
                                  cons p -> let (z,zs) = p in
		                              cons (z, 
						    append zs ys))
@@@
The abstract value of @append@ is
""
(\ID, \L(\A_{@@@xs@@@},\K_{@@@xs@@@}).
(\ID, \L(\A_{@@@ys@@@},\K_{@@@ys@@@}).
(\A_{@@@xs@@@}\glb\A_{@@@ys@@@}, \K_{@@@xs@@@}\glb\K_{@@@ys@@@}))) .
""
This reveals that partial applications of @append@ are static up to
WHNF, and the abstract value of the result is the greatest lower bound
of the two arguments.  In general, the abstract value of a closed
expression of the form $@\x@_1@.\x@_2\ldots@.e@$ will reveal that all
partial applications of the expression are static up to WHNF.

Let @reverse1@ stand for the expression denoting the naive reverse
function:
@@@
     fix (\reverse1 . \xs . case xs of
                              nil u  -> nil ()
                              cons p -> let (z,zs) = p in
                                          append (reverse zs) 
			                         (cons (z, nil ())))
@@@
The abstract forward value of @reverse1@ is the identity, so the
abstraction of the elements of a list doesn't change by reversing the
list.

Let @compose@ stand for @\f.\g.\x.f (g x)@.  The abstract value of @compose@ is
""
(\ID, \L(\A_{@@@f@@@},\K_{@@@f@@@}).
(\ID, \L(\A_{@@@g@@@},\K_{@@@g@@@}).
(\ID, \left\{\ba{lll}\bot, &\mbox{\rm if}\ \A_{@@@f@@@}=\BOT\mbox{\rm or} \A_{@@@g@@@}=\BOT
	\K_{@@@f@@@}\o\K_{@@@g@@@},&\mbox{\rm otherwise}\ea\right\}))) .
""
Thus, the
result of the application of the composition of two functions is
dynamic, and maps all values to dynamic values, if either function is
dynamic; otherwise, the result is given by the application of the
composition of the abstract forward values to the abstract argument.

Let @listcomp@ stand for the expression denoting the function
that composes lists of functions:
@@@
     fix (\listcomp . \fs . case fs of
                              nil u  -> \x.x
                              cons p -> let (g,gs) = p in 
                                          compose g (listcomp gs))
@@@
The abstract value of @listcomp@ is
""
	(\ID,\ \L(\A_{@@@fs@@@},\K_{@@@fs@@@}).\
	(\ID,\ \left\{\ba{lll}\
\bot,		&\mbox{\rm if}\ \A_{@@@fs@@@}\neq\SPINE\ \ID
\glb_{i\geq0}(\K_{@@@fs@@@})^i,	&\mbox{\rm otherwise}\ea\right\})) .
""
Since the abstract values of lists contain no information about the
length of the lists of which they are abstractions, the abstract value
of the composition of list elements is the glb of the composition over
all lengths.

Let @flatten@ stand for the expression denoting the function that
flattens trees into lists:
@@@
     fix (\flatten . \t . case t of
	                    leaf l   -> cons (l, nil ())
	                    branch p -> let (t1,t2) = p in
			                  append (flatten t1) 
					         (flatten t2))
@@@
The abstract value of @flatten@ is
""
	(\ID,\ \L(\A_{@@@fs@@@},\K_{@@@fs@@@}).\
	(\T\ \A,\ \left\{\ba{lll}\
\bot,		&\mbox{\rm if}\ \T\ \A_{@@@fs@@@}\neq\LBR\ \ID
\K_{@@@fs@@@},	&\mbox{\rm otherwise}\ea\right\})) ,
""
where $\T$ maps $\BOT$ to $\BOT$, and $\LBR~\A$ to $\SPINE~\A$ for $\A$
ranging over $\ID$ and $\BOT$.

The function denoted by @compose listcomp flatten@ that composes
trees of functions has abstract value
""
	(\ID,\ \L(\A_{@@@fs@@@},\K_{@@@fs@@@}).\
	(\ID,\ \left\{\ba{lll}\
\bot,		&\mbox{\rm if}\ \A_{@@@fs@@@}\neq\LBR\ \ID
\glb_{i\geq0}(\K_{@@@fs@@@})^i,	&\mbox{\rm otherwise}\ea\right\}))\ .
""
Similarly to the case for lists, the abstract values of trees contain
no information about the structure of the trees of which they are abstractions,
so the abstract value of the composition of the values of the leaves
is the glb of the composition over all tree structures.

\section{More on Abstract Domains}

The sizes of the abstract domains and the representations of the
abstract values can be considerably optimised.  The nonstandard
semantics of application---embodied by $\it apply^{\Ps}$---guarantees
that the abstract values $(\BOT,f)$ and $(\BOT,f')$ from
$\Te{\Ps}{\tfun}$, for all $f$ and $f'$, are effectively the same:
$\it apply^{\Ps}~(\BOT,f)=\bot$ for all $f$.  A practical analyser
would take advantage of this fact, identifying $(\BOT,f)$ over all
$f$.  More generally, for function types embedded within data
structures, e.g.\ $@(@\tint@,@\tfun@)@$, abstract values
$((\A,()),(\BOT,f))$ would be identified over all $f$.

In the following we restrict attention to {\it denotable\/} values: at
each type @T@ those values that can be expressed as $\Ee{}{@e@}~[\,]$
for some @e@.  For every value $v\in\Te{\Ss}{@T@}$ there is a best
abstraction---a greatest value $(\A,\K)\in\Te{\Ps}{@T@}$ such that
$\Re{}{@T@}((\A,\K),v,v)$.  For types $\tone$ and $\ttwo$ not
containing @->@ we have
$\Tde{\Ps}{\tfun}\iso(\proj{\De{}{\tone}}\x\one)\fto(\proj{\De{}{\ttwo}}\x\one)$,
and if $\K\in\Tde{\Ps}{@T@}$ is greatest such that
$\Rde{}{\tfun}~(\K,f,f)$ for denotable $f$, then $\K$ maps $(\ID,())$
to $(\ID,())$ and distributes over $\glb$.  The subset of functions
from $\Tde{\Ps}{\tfun}$ that map $(\ID,())$ to $(\ID,())$ and
distribute over $\glb$ forms a complete lattice, hence attention may
be restricted to this subset.  Not only does this reduce the number of
abstract functions, it also reduces their representation: such
functions are determined by their behaviour on the unique $\glb$-{\it
basis\/} of the lattice $\proj{\De{}{\tone}}$---the set $B$ of values
such that every element of $\proj{\De{}{\tone}}$ is the glb of some
(possibly empty) subset of $B$, and no element of $B$ can be expressed
as the glb of some subset not containing that element.  This
optimisation can be generalised to higher order: $\Tde{\Ps}{\tfun}$
may be restricted to functions that map $\top$ to $\top$ and
distribute over $\glb$, for all function types $\tfun$.  (These
results follow from those shown in earlier work \cite{Dav94}.)

\section{Related Work}

Consel \cite{Con90} describes a binding-time analysis for higher-order
untyped languages.  As in our analysis abstract values have two parts,
the first describing the static/dynamic properties of values, and the
second describing how (for function types) abstract arguments are
mapped to abstract results; there appears to be an implicit domain
factorisation similar to ours, based on implicit type information
collected by analysis semantics.  In this respect there are many
superficial similarities between the two techniques.  No formal
relation to the standard semantics is given, making a formal
comparison with our technique problematic.

Hughes \cite{Hug88} proposes a general approach to promoting related
analysis techniques to higher order; instantiation to projection-based
analysis yields a technique much more closely related to that of our
previous work \cite{Dav93b} than to the one given here.

\section{Conclusion}

We have successfully generalised Launchbury's monomorphic
projection-based binding-time analysis to higher-order, using abstract
domains smaller than those typically used in BHA strictness analysis.

The next step would be to generalise to handle Hindley-Milner
polymorphism.  This has been done with good results at first order for
binding-time analysis \cite{Lau91a}, and for BHA strictness analysis
at higher order \cite{Bar91,Bar93}.  We anticipate that the combined
use of these theories will give a reasonably straightforward
generalisation to polymorphism.  In addition to making the analysis
more widely applicable, this should also greatly reduce the run-time
cost of analysis, as it does for BHA analysis.

Though not developed here, using the same approach it is possible to
give a strictness analysis technique, again closely related to Hunt's
PER-based strictness analysis technique \cite{Hun91}.


\begin{thebibliography}{123456}

\bibitem[Abr89]{Abr89}
S. Abramsky. 
``The lazy lambda calculus.'' 
{\it Research Topics in Functional Programming.}
David Turner, ed., Addison-Wesley 1989.

\bibitem[Abr90]{Abr90}
S. Abramsky. 
``Abstract interpretation, logical relations and Kan extensions.''
{\it Journal of Logic and Computation}, 1, 1990.

\bibitem[Bar91]{Bar91}
G. Baraki.
``A note on abstract interpretation of polymorphic functions.''
In \cite{Hug91}.

\bibitem[Bar93]{Bar93}
G. Baraki.
{\it Abstract Interpretation of Polymorphic Higher-Order Functions.}
Ph.D. thesis, Research report FP-1993-7, 
Department of Computing Science, University of Glasgow.

\bibitem[BEJ88]{BEJ88}
D. Bjorner, A.P. Ershov, and N.D. Jones, eds.
{\it Partial Evaluation and Mixed Computation,
Proceedings IFIP TC2 Workshop, Gammel Avern{\ae}s}, Denmark, October 1987.
North-Holland, 1988.

\bibitem[Con90]{Con90}
C. Consel. 
``Binding Time Analysis for Higher Order Untyped Functional Languages.''
Proceedings of the 1990 ACM Conference on LISP and 
Functional Programming, pp264-272.

\bibitem[Dav93]{Dav93b}
K. Davis.
``Higher-order Binding-time Analysis.'' 
{\it Proceedings of the 1993 ACM on Partial Evaluation and
Semantics-Based Program Manipulation (PEPM '93)},
ACM Press, 1993.

\bibitem[Dav94]{Dav94}
K. Davis.
{\it Projection-based Program Analysis.}
Ph.D. thesis, Computing Science Department,
The University of Glasgow, 1994.

\bibitem[Go92]{Go92}
C.K. Gomard.
``A self-applicable partial evaluator for the lambda calculus:  Correctness
and pragmatics.''
ACM TOPLAS, Vol 14, No.\ 2, April 1992.

\bibitem[HM94]{HM94}
F. Henglein and C. Mossin.
``Polymorphic binding-time analysis.''
European Symposium on Programming (ESOP '94).

\bibitem[Hug88]{Hug88}
J. Hughes.
Backwards analysis of functional programs.
In \cite{BEJ88}.

\bibitem[Hug91]{Hug91}
J. Hughes, ed.
{\it Proceedings of the 1991 Conference on Functional Programming Languages
and Computer Architecture (FPCA '91)}, Cambridge, Sept 1991. LNCS 523,
Springer Verlag, 1991.

\bibitem[Hun91]{Hun91}
S. Hunt.
``PERs generalise projections for strictness analysis (extended abstract).''
{\it Proceedings of the 1990 Glasgow Workshop
on Functional Programming}.
Simon L. Peyton Jones {\it et al.}, eds.
Springer Workshops in Computing.
Springer-Verlag, 1991.

\bibitem[HS91]{HS91}
S. Hunt and D. Sands.
``Binding time analysis:  a new PERspective.''
{\it ACM Symposium on Partial Evaluation
and Semantics-Based Program Manipulation},
SIGPLAN Notices Vol.\ 26, No.\ 9, 1991.

\bibitem[Jen92]{Jen92}
T. Jensen.
{\it Abstract Interpretation in Logical Form}.
Ph.D. thesis, Report 93/11, Department of Computer Science,
University of Copenhagen, 1992.

\bibitem[Jon88]{Jon88}
N.D. Jones.
``Automatic program specialization: A re-examination from
basic principles.'' In \cite{BEJ88}.

\bibitem[Lau88]{Lau88}
J. Launchbury.
``Projections for specialisation.''
In \cite{BEJ88}.

\bibitem[Lau91a]{Lau91a}
J. Launchbury. 
{\it Projection Factorisations in Partial Evaluation.} PhD
Thesis, Glasgow University, Nov 89. Distinguished Dissertation in
Computer Science, Vol 1, CUP, 1991.

\bibitem[Mog89]{Mog89}
T. Mogensen.
``Binding-time analysis for polymorphically typed higher order languages.''
{\it International Joint Conference on Theory and Practice of Software
Development}. LNCS 352. Springer-Verlag 1989.

\bibitem[NN88]{NN88}
H.R. Nielson and F. Nielson.
``Automatic binding-time analysis for a typed $\L$-calculus.''
{\it Science of Computer Programming 10},
North Holland, 1988.  Also in POPL '88.

\bibitem[Sch88]{Sch88}
D.A. Schmidt.
``Static properties of partial reduction.''
In \cite{BEJ88}.

\bibitem[Sew93]{Sew93}
J. Seward.
``Polymorphic strictness analysis using frontiers.''
{\it Proceedings of the 1993 ACM on Partial Evaluation and
Semantics-Based Program Manipulation (PEPM '93)},
ACM Press, 1993.

\bibitem[Wad87]{Wad87}
P. Wadler.
``Strictness analysis on non-flat domains by abstract interpretation over finite domains. '' 
S. Abramsky, C. Hankin, eds.
{\it Abstract Interpretation of Declarative Languages.}
Ellis-Horwood, 1987.


\end{thebibliography}

\end{document}

