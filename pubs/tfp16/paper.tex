% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
% mkd for preliminary paper notes
\usepackage{enumitem} 
\usepackage{color} % for parenthetical notes while paper is in progress
%
\begin{document}
%
%mkd \frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
%
%mkd \mainmatter              % start of the contributions
%
\title{Strict Pure Functional Programming and Automatic Parallelization and Transparent Fault Tolerance\\
       (New project paper)}
%
\titlerunning{Automatic Parallelization}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Kei Davis\inst{1} \and
        Dean Prichard\inst{1} \and
        David Ringo\inst{1,2} \and
        Loren Anderson\inst{1,3} \and
        Jacob Marks\inst{1,4}\thanks{All project contributors, past and present, are listed.}
}
%
\authorrunning{Kei Davis et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Ivar Ekeland, Roger Temam, Jeffrey Dean, David Grove,
%Craig Chambers, Kim B. Bruce, and Elisa Bertino}
%
\institute{Los Alamos National Laboratory, Los Alamos, NM, USA\\
\email{kei.davis@lanl.gov},\\ WWW home page:
\texttt{http://ccsweb.lanl.gov/\homedir kei/}
\and
University of New Mexico,
Albuquerque, NM, USA
\and
North Dakota State University,
Fargo, North Dakota, USA
\and
New Mexico Institute of Mining and Technology,
Socorro, New Mexico, USA
}

\maketitle              % typeset the title of the contribution

% moronic author listing code increments footnote counter
\setcounter{footnote}{0}

\begin{abstract}
The claimed trend is an increasing appreciation and practice of functional
programming in scientific computing, and in particular strict (by default)
functional programming, in various guises.  Our project is the demonstration
of a light-weight, higher-order, polymorphic, pure functional language
implementation in which we can experiment with automatic parallelization
strategies and varying degrees of default strictness.  Non-strictness
constrains the degree of potential parallelism, that is, we are not exploring
speculative evaluation.  A secondary goal is to experiment with mechanisms
for transparent fault tolerance.
\\
\emph{Draft paper notes:}
\begin{enumerate}[before=\itshape]
  \item We are reporting work in progress.  At the time of writing we have a
    complete serial implementation, a primitive proof-of-concept parallel
    implementation, and are currently developing a more realistic parallel
    runtime.  We expect to have preliminary results from our new parallel
    implementation by the time of the workshop.  The fault-tolerance mechanisms
    remain future work.

  \item While this does not qualify as a student paper, students
    (undergraduate!)  have contributed considerably to this effort, and to the
    writing of this paper, and will continue to do so.  If this paper is
    accepted we plan to have one of these students present it at TiFP.

  \item Hopefully in the spirit of a student-friendly workshop, for this draft
    version we have chosen a more informal, conversational style of
    presentation than necessarily appropriate for a final paper, have
    highlighted the major student contributions and involvement, and included
    perhaps gratuitous detail about process.
  \end{enumerate}


\keywords{strict pure functional programming, automatic parallelization, Haskell, STG}

\end{abstract}

\section{Background and Motivation}
The practice of so-called high-performance scientific computing (HPC) is,
overall, highly conservative with respect to change.  Even as the US
government commits to a push for exascale computing ($10^{18}$ FLOPS in a
single system) within the next decade~\cite{Exascale-roadmap} a highly
respected member of the HPC community recently stated publicly that ``Fortran
is essential for exascale programming.''~\cite{Heroux16}\footnote{Various EU,
  Japanese, and international groups are also anticipating the realization
  exascale computing.~\cite{EUexascale,Exascale-org}} Existing US government
laboratory HPC codes run to hundreds of thousands of lines of Fortran and
complete rewrites are infeasible.\footnote{The codes of our lab's British
  counterpart are reportedly in the same state.}  C++ has become more common
for new code starts, but their nature remains the same: huge codes with fair
to non-existent encapsulation of side effects, with multiple levels of
parallelism also poorly abstracted, especially at the thread (shared-memory)
level.  Thread-level parallel efficiency can be poor and is explained by
Amdahl's law because the parallel model \emph{as typically implemented} is
bulk synchronous.



\subsection{The trend: strict but still pure functional programming}
There is a small but growing understanding that \emph{pure functional
  semantics}, in some form, is essential to reining in the complexity of these
ever-evolving scientific codes.  Such semantics are expressed in a number
of ways as highlighted following.

\subsubsection{Rely on programmer discipline.}
Legacy Fortran scientific codes likely represent worst practices in state
encapsulation, with most data global and accessible to all; indeed, Fortran
was designed to make this easy via \texttt{COMMON} blocks---aggregates of
global variables---which further obfuscate program meaning by allowing
arbitrary naming of a block's variables at a subroutine/function granularity,
and requiring that the entire block be brought into scope.  It is almost
ironic, then, that of the mainstream high-performance scientific programming
languages (Fortran, C, and C++), Fortran~95 was the first to introduce the
\texttt{pure} function qualifier, and with enforcement by the compiler.
Otherwise it is entirely up to the programmer to make functions composable and
thread safe.  Here at LANL this approach is being used to good effect to clean
up some legacy code bases.

\subsubsection{Obey constraints prescribed by a parallel runtime system.}
There are a number of parallel runtime systems that encourage (but cannot
enforce) a pure functional style.  We will highlight Stanford's Legion
runtime as an example~\cite{Bauer12}.

In Legion, non-function-local data, or \emph{regions}, are requested from the
runtime system.  Regions may be shared among \emph{tasks} (essentially C++
functions), and each task is prescribed access privileges (read, write,
read/write, etc.)  and coherency requirements for each region to which it will
have access.  Other than registered access to regions, tasks must not access
non-constant global data.  In serial execution these access requirements are
notionally superfluous, but in parallel execution the runtime can dynamically
calculate the data dependency graph and relax the programmatically defined
serial ordering of task execution.  Very high parallel efficiency has been
demonstrated on extremely large computing systems~\cite{Bauer14}.  Here at
LANL the Legion system is being strongly considered for a new large-scale C++
code effort.\footnote{It seems significant that the PI of the Legion
  project was previously a co-developer of a strict, arguably pure functional
  language implementation~\cite{AikenFL,FLreport89}, in collaboration with
John Backus as a refinement of his FP language~\cite{Backus:1978}.}

\subsubsection{Use a pure functional language.}
Some twenty years after the US Department of Energy laboratory complex was
presented with the most highly parallel (and thereby performant)
implementation of a pure functional language to date~\cite{Davis96}, and
collectively had no conceptual understanding of what it was, much less why it
might be useful or important, there is now a small number of laboratory HPC
practioners who are developing an appreciation for pure functional programming
in Haskell.  Still, they see their needs as different than other Haskell
users: performance is paramount, unboxed arrays are a (perhaps \emph{the})
primary data structure, and non-strictness or laziness is more of a
nuisance--even a serious hindrance--than a convenience, much less ever
algorithmically essential in the sense of Bird's \emph{repmin}~\cite{Bird84}
or Johnsson's general attribute grammars~\cite{Johnsson87}.

A common complaint among these aspiring high-performance functional
programmers is the need to get strictness just right: to remove space leaks,
achieve acceptable performance, get communication (e.g.\ MPI) and IO to work
as they expect, and so on.  More specifically, they expect to be able to
reason about space and time usage, and order of execution, using their
existing mental models. While they are happy to write code that is strongly
reminiscent of the numerical examples in Hughes' classic piece~\cite{Hughes89}
(whether they've read it or not), they would also be happy to write in a
slightly modified style that does not require non-strictness---after all,
that's how they've been thinking their entire careers.

Much anecdotal evidence suggests that many others have similar difficulties
with lazy (or non-strict) by default semantics, with the most telling recent
concrete evidence being the push for, and realization of, the \texttt{Strict}
and \texttt{StrictData} language extensions newly available in GHC 8.0.
Quoting the documentation:
\begin{quote}
  High-performance Haskell code (e.g.\ numeric code) can sometimes be littered
  with bang patterns, making it harder to read. The reason is that laziness
  isn't the right default in this particular code, but the programmer has no
  way to say that except by repeatedly adding bang
  patterns~\cite{strict-strictdata}.
\end{quote}
It is a fact that \emph{HPC practioners are by and large not computer scientists}.
The concept of a space leak, for example, caused by other than a missing explicit
deallocation statement (C \emph{free}, etc.) is difficult to understand and not
something they particularly want to try to understand.  Hence, we eschew explicit
strictness annotations for varying uniform degrees of strictness.

More generally, \emph{task-based} parallelism is becoming increasingly popular
for HPC, often in conjunction with a higher-level (typically inter-node) bulk
synchronous model typically implemented with the Message Passing Interface
(MPI)~\cite{MPI}.  In the imperative world, example systems include the
aforementioned Legion, Intel's Threading Building Blocks~\cite{Reinders:2007}, and the
HPX C++11/14 extension~\cite{HPX}.  While the concept of task is somewhat
vague, the general idea is a unit of computation with no or
minimal/constrained side effects.  Function call in a pure functional language
could be regarded as the essence of the concept of task.

In the quest for computing platforms approaching exascale, it is widely
recognized that new mechanisms for fault tolerance will need to be built into
the software stack.  Current practice in HPC is checkpoint/restart, wherein at
various point in time sufficient program state is dumped to backing store such
that the program, in case of a fault, can restart from the latest stored
state.  Most implementations are explicit, though there has been some success
with transparent (invisible to the programmer) systems.  Wiring
checkpoint/restart into a large-scale application is not only burdensome and
error-prone, it can severely warp the engineering of a large-scale application
because it must be designed such that essential state can be readily captured.
And now, it is widely believed that the scalability of checkpoint/restart may
be reaching its end.

In the imperative world side-effect-free tasks could be safely restarted after
failure, needing only their well-defined inputs and not arbitrary access to
mutable global state.  The Legion system is being augmented with this
capability, and in such a way that it will be largely transparent to the programmer.
Ericsson's quasi-functional language Erlang and its runtime system was
designed for fault tolerance.~\cite{Cesarini:2009}.  It has long been
recognized that the pure parts of a functional program (excluding, e.g., the
I/O monad in Haskell) could be safely restarted.  The recent work of
[Trinder's student] impressively demonstrates this for an augmented ghc/runtime
on a large-scale distributed-memory system.

We claim, then, that there is a trend, in HPC at least, towards
\emph{strict-by-default pure functional programming}, whether as imposed by
programmer discipline, a parallel runtime system, or the language itself
(e.g.\ strict Haskell).  Second, we claim that there is a trend in
high-performance computing, and by transitivity functional programming,
towards transparent fault tolerance.

\subsection{Strictness and Parallelism}

It is a trivial observation that given strict functional semantics, wherein
function arguments may be safely evaluated before function evaluation, it is
safe to evaluate the arguments, and the function, in parallel, and that this
principle applies recursively in the (dynamic) expression tree (or graph).

\section{Project and Goals}

Our project is the light-weight implementation of a pure, higher-order,
polymorphic, functional language and runtime system with which we can
experiment with automatic parallelization strategies with degrees of language
strictness ranging from lazy to fully strict, and secondarily, with mechanisms
for transparent fault tolerance.  Light-weight has several meanings or
implications.  First and foremost it means that \emph{we are not attempting to
  compete with ghc in any respect}---succumbing to the temptation of
recreating ghc language, type system, and runtime features, or significant
code optimization, would simply divert us from our central goals.\footnote{We
  estimate that we will have, to within an order of magnitude, one person-hour
  of total available development time for each person-year that ghc currently
  represents, or put another way, one second for each hour, respectively.}

\begin{itemize}
\item The implementation must be feasible in terms of effort;
\item The language must be strongly typed, implicitly or explicitly.  More
  specifically, the language should explicitly support both Hindley-Milner
  polymorphism and unboxed types~\cite{Jones:1991};
\item Straightforward direct linkage with C, preferably without a sophisticated
  foreign function interface, is essential;
  \item The language should be \emph{currying-friendly} as described by Marlow
and Peyton Jones~\cite{Marlow:2004};
\item Choice of degree of strictness should be easily selected;
\item Sharing, in the sense of what \emph{lazy} means vs.\ merely non-strict,
  should be preserved for all degrees of strictness;
\item Tail calling should be properly implemented, i.e., not as a trampoline;
\end{itemize}

For feasibility, we restrict ourselves to shared-memory implicit parallelism,
but distributed-memory parallelism should be straightforwardly realizable
using e.g.\ MPI or POSIX sockets.

A minor, informal goal is to gain a sense of how often relatively
inexperienced functional programmers make essential use of
non-strictness.\footnote{The starting experience of our student interns to
  date has been normalized by having them complete Hutton's \emph{Programming
    in Haskell} textbook~\cite{Hutton-book} as baseline preparation,
  substituting his article \emph{Higher-order Functions for
    Parsing}~\cite{Hutton-parsing:1992} for the corresponding material in the
  book.}

\subsection{Varying degrees of strictness}

First some words about terminology.  For all degrees of strictness, sharing
(as implied by lazy but not by non-strict) of heap-allocated objects is
preserved so we will not use the term \emph{lazy} and simply refer to
strictness properties.  As is common we will use strict in an operational
sense, meaning that argument(s) are evaluated before being passed in a
function call, rather than the denotational sense, unless stated otherwise.
Similarly, when referring to language semantics we refer by default to the
operational rather than denotational semantics.

Greater strictness implies more opportunities for parallelism, possible losses
or gains in space efficiency, and less expressiveness.  We seek to explore
the interplay of all of these.

ghc 8.0 gives three possibilities: conventional non-strict, constructor
strict, and function- and constructor strict, but there are possible
variations.  The obvious variation is function, but not constructor,
strictness.  In Haskell the distinction between constructors and functions is
somewhat blurred because constructors behave like functions (outside of
pattern matching), but this need not be the case.  

\section{Design and Implementation}

Our wish list sounds much like the ghc Haskell implementation, and indeed ghc
is part of our master plan.  In short, our plan is to use ghc as our front-end
to generate its intermediate form, STG~\cite{PJ-stockhardware}, then escape to
our system from there.  There are various possible ways to accomplish the
former: using ghc's option to dump STG as text, hacking ghc itself and,
optionally, using ghc as a library.

\subsection{STG Language}
\setlength{\tabcolsep}{5pt}

Our incarnation of STG, as is our presentation in Figure~\ref{STGsyntax} which
is meant to evoke both an abstract and concrete syntax, are very similar to
that given in Marlow and Peyton Jones' \emph{fast curry}
paper~\cite{Marlow:2004}, to which we refer so many times that we will
subsequently refer to it as simply \emph{EA}.\footnote{\emph{Eval/Apply} from
  the title because \emph{FC} likely has another connotation to the reader.}

\begin{figure}
%\centering
\begin{centering}
\footnotesize % tiny scriptsize footnotesize small
\begin{tabular}{r r c l l}

Variable     & $f,\ x$        &     &                                              & Initial lower-case letter \\
Constructor  & $C$            &     &                                              & Initial upper-case letter \\
Literal      & $\mathit{lit}$ & ::= & $i\ |\ d$                                    & Integral or floating point literal \\
Atom         & $a$            & ::= & $\mathit{lit}\ |\ x$                         & \\ % Function, constructor args are atoms \\
\\
Expression   & $e$            & ::= & $a$                                          & Atom \\
             &                & $|$ & $f\ a_1\dots a_n$                            & Application, $n\ge 1$ \\
             &                & $|$ & $\oplus\ a_1\dots a_n$                       & Saturated primitive operation, $n\ge 1$ \\

             &                & $|$ & \texttt{let \{}                              & Recursive let, $n\ge 1$ \\
             &                &     & \texttt{ } $x_1$ \texttt{=} $\mathit{obj}_1$ \texttt{;} \\
             &                &     & \hspace{0.2in} $\dots$ \texttt{;} \\
             &                &     & \texttt{ } $x_n$ \texttt{=} $\mathit{obj}_n$ \\
             &                &     & \texttt{\} in} $e$  \\

             &                & $|$ & \texttt{case} $e$ \texttt{of} $x$ \texttt{\{}  & Case expression, $n \ge 1$\\
             &                &     & \texttt{ } $\mathit{alt}_1$ \texttt{;} \\
             &                &     & \hspace{0.2in} $\dots$ \texttt{;} \\
             &                &     & \texttt{ } $\mathit{alt}_n$ \\
             &                &     & \texttt{\}} \\
\\
Alternatives & $\mathit{alt}$ & ::= & $C\ x_1\dots x_n$ \texttt{->} $e$            & Pattern match, $n \ge 0$ \\
             &                & $|$ & \texttt{\_ ->} $e$                           & Default \\
\\
Heap objects & $\mathit{obj}$ & ::= &\texttt{FUN} $x_1\dots x_n$ \texttt{->} $e$   & Function definition, arity $=n\ge 1$ \\
             &                & $|$ &\texttt{CON} $C\ a_1\dots a_n$                & Saturated constructor, $n \ge 0$ \\
             &                & $|$ &\texttt{THUNK} $e$                            & Thunk---explicit deferred evaluation \\
             &                & $|$ & $\mathit{PAP}\ f\ a_1\dots a_n$               & Evaluation-time partial application \\
             &                & $|$ & $\mathit{BLACKHOLE}$                         & Evaluation-time black hole \\
\\
Program      & $\mathit{prog}$& ::= & $f_1\ =\ \mathit{obj}_1$ \texttt{;}          & $n \ge 1$, distinguished \texttt{main}\\
             &                &     & \texttt{ } $\dots$ \texttt{;} \\
             &                &     & $f_n\ =\ \mathit{obj}_n$

\end{tabular}
\end{centering}
\caption{STG syntax}
\label{STGsyntax}
\end{figure}

Partial application (\emph{PAP}) and black hole (\emph{BLACKHOLE}), i.e., a thunk being
evaluated, are only runtime entities and not actually part of our STG language.

The semantics of STG is what one would expect for a higher-order, polymorphic,
pure functional language, with one exception for a Haskell programmer familiar
with unboxed types:  \texttt{case} is strict in the scrutinee, so
\texttt{case} is similar to \texttt{pseq} (as opposed to \texttt{seq}) in Haskell,
that is, the scrutinee is evaluated first.  

In STG---one of ghc's
intermediate representations---the distinction is clear: heap objects are
explicitly constructed by \texttt{let} constructs in which constructor names
are given explicitly (not aliased); constructor names are not functions.



A distinctive feature of STG is that heap allocation is explicit; this is exactly
what \texttt{let} bindings do.  As such unboxed values cannot be let-bound; instead
unboxed values can be bound with \texttt{case}:
\begin{verbatim}
    case e of x# {
      _ -> e
    }
\end{verbatim}
where the \texttt{\#} suffix is merely a convention to signify an unboxed type.
Deferred evaluation is made explicit:  a deferred expression evaluation is
encoded as a thunk.

\subsection{Degrees of strictness, take two}

Other possible variations concern the notional arity of the underlying
function being applied.  This is readily explained in terms of the operation
of the STG.  Our partial description is based on that given in \emph{EA}.

In STG the arity of a function is syntactically defined as the number of
manifest formal parameters, thus \texttt{id x = x} has by definition arity 1
regardless of the type of the argument (e.g., some function type) it might be
applied to.  We distinguish \emph{application}, e.g., \texttt{f x y}, which is
a syntactic notion, from actually calling a function. Thus \texttt{id id x} is
an application of \texttt{id} to two arguments, but the underlying \emph{FUN}
(user- or system- defined function) when actually called is given exactly one
argument (the first one).

In the eval-apply model, given application \texttt{f x y}, in the standard
(non-strict) semantics \texttt{f} is first evaluated and the arity of the
underlying \emph{FUN} (user- or system- defined function) or \emph{PAP}
(partial application of a \emph{FUN}) is determined---in the case of
\emph{PAP} the arity is that of the underlying \emph{FUN} less the number of
arguments previously provided.  If the arity is greater than two then a new
\emph{PAP} is created with arguments \texttt{x} and \texttt{y} inserted and
the new \emph{PAP} returned.  If the arity is equal to two then the underlying
\emph{FUN} is tail-called with any previous argument (in case of \emph{PAP})
and new arguments \texttt{x} and \texttt{y}.  If the arity is less than two
(it must be at least one, so one in this case) the underlying \emph{FUN} is
called-with-return with any previous arguments (in case of \emph{PAP}) and new
argument \texttt{x}, then the object returned applied to remaining argument
\texttt{y}.

Given application \texttt{f x y} when might \texttt{x} and \texttt{y} be
evaluated?  In the non-strict semantics evaluation is on demand, after
\texttt{f} has been evaluated, the underlying \emph{FUN} actually called
(i.e., not just partially applied and wrapped up in a \emph{PAP}), and one of
those arguments forced in subsequent evaluation (assuming that they weren't
forced in the evaluation of \texttt{f} itself).

In a strict semantics one can come up with various evaluation orders, but
excluding the absurd, as far as distinguishing termination properties goes
these can be distilled down to two possibilities, namely whether on
construction of a \emph{PAP} the (new) arguments are forced or not.  (An
absurd strategy might force every second argument.)  These are in fact
distinguishable because (in Haskell) the first argument to \texttt{seq} might
evaluate to a \emph{PAP}.

This then gives three reasonable operational modes that are strictly ordered
in terms of termination properties: non-strict, fully strict---all arguments
in an application are evaluated---and the intermediate in which only those
arguments needed by a function call are evaluated.  The STG machinery makes
the choice of evaluation strategy easy to change, both in terms of function
application and building a constructor in the heap.

\subsection{STG to C}

The back-end is written in Haskell and generates proper tail-calling C code;
the runtime system is written in C/C++.  The system fairly faithfully
implements the STG machine as described in \emph{EA} and dynamically
illustrated by Pope's Ministg~\cite{ministg} with the exception of the state
transition rules which, of course, may be different than those for non-strict
semantics.  Sharing is preserved.

\subsubsection{Proper tail calls.}
While performance is not a high priority, we attain proper tail calling instead of
using the trampoline used in earlier versions of ghc~\cite{PJ-stockhardware}.
Because we maintain our own data and control (continuation) stack all
generated C functions take no explicit arguments and have return type void,
and with a single exception in the non-strict case are invoked in tail call
positions, at non-zero optimization levels both gcc and Clang/LLVM generate
jumps instead of calls for C calls in tail call positions.  When using gcc
we also use a fixed set of machine registers for passing/returning values.

The exceptions to tail calling are for convenience.  Similar to what is
described in \emph{EA} we have a generic variadic \emph{stgApply} function
that is used for applications of non-known functions, and known functions with
a deficit or excess number of arguments.  The \emph{stgApply} function takes
as arguments the first element of the application (\emph{FUN}, \emph{PAP},
\emph{THUNK}, or \emph{BLACKHOLE}) and its arguments, and if a \emph{THUNK},
evaluates it with a call (rather than jump) to yield a \emph{FUN} or
\emph{PAP}.  This allows us to implement a single \emph{stgApply} function as
a C function taking a contiguous block of machine words (unboxed values
or pointers) on the explicit stack.  The implementation of \emph{stgApply}
as described in \emph{EA} makes the same concession regarding
call-with-return, but the scheme described there uses an auto-generated family
of \emph{stgApply} functions parameterized by number and type (unboxed or
pointer) of arguments.  We used this scheme initially as well, but because the
parameter block contains a bitmap describing which arguments are boxed/unboxed
for the benefit of the garbage collector, this elaboration is unnecessary.

For \emph{stgApply} in the non-strict semantics an alternative to
call-with-return would be a pair of functions \emph{stgApply1} and
\emph{stgApply2}.  In one possible variation, \emph{stgApply1} would push a
continuation with \emph{stgApply2} as the return address, then invoke the
first element of the application.  Call-with-return becomes more convenient
when considering the various forms of strictness: when evaluating function or
constructor arguments it is simpler, and arguably more efficient in terms of
time (but not space in general) to ``evaluate in place'' than implement the
continuation style to the logical limit.

\subsection{Mini-Haskell}
As mentioned, a current near-term goal is to use ghc as our front-end to
generate STG as input to our system.  In the meantime, as a stop-gap measure
to make the generation of test cases easier, we have implemented a
\emph{mini-Haskell} front-end that supports a subset of standard Haskell
syntax with \emph{MagicHash} enabled, with a single extension.  Recall that
\emph{MagicHash} allows \texttt{\#} to be a suffix of an identifier, or denote an
unboxed literal value.  Our extension (to STG as well)
is the provision of the optional \texttt{unboxed} keyword in datatype definitions
to allow user-defined unboxed data types, e.g.,
\begin{verbatim}
  data unboxed Bool# = False# | True# .
\end{verbatim}

Writing STG in the concrete syntax conceived for our project is tedious and
prone to errors.  Compared to Haskell, the eventual source language for the
compiler, STG is relatively verbose, requiring explicit block and expression
delimiters, and classification of heap objects. Though its general structure is
similar to primitive Haskell, it lacks many of the conveniences that make
Haskell a pleasure to program in.  The motivation for a more expressive language
for testing our compiler and runtime system in their nascent stages is then clear.  This language,
mini-Haskell, was progressively ``grown'' from the STG syntax towards a subset of
Haskell itself*, removing much of the frustrating syntactic cruft and adding
some of the core features that are often taken for granted in Haskell (and other
languages).

An informal specification of the mini-Haskell grammar is given in
Figure~\ref{miniHaskell}.  There is significant overlap with STG resulting
from the structural similarities, but the redundancies are included for
completeness.


% Haskell Report:
%   https://www.haskell.org/onlinereport/haskell2010/haskell.html
% (PDF):
%   https://www.haskell.org/definition/haskell2010.pdf

% Chapter 10:
%   https://www.haskell.org/onlinereport/haskell2010/haskellch10.html
% (No standalone PDF)


% because # looks strangely large next to math terms
\newcommand{\hash}{{\scriptsize\#}}

\begin{figure}
\centering
\footnotesize % tiny scriptsize footnotesize small
\begin{tabular}{r r c l l}
Variable         & $f,\ x$             &     &
& Initial lower-case letter \\

Constructor        & $C$                 &     & 
& Initial upper-case letter \\

Literal            & $\mathit{lit}$      & ::= & $i\ |\ d\ 
                                                 |\ i$\hash\ $|\ d$\hash
& Integral or floating point literal (boxed) \\

                   &                     &     & 
& optional \# suffix denotes unboxed value \\

Atom               & $a$                 & ::= & $\mathit{lit}\ |\ x\ |\ C$
& Constructors are first class \\ 
% not sure if that's the right terminology 
\\


Expression         & $e$                 & ::= & $a$
& Atomic expression \\

                   &                     & $|$ & $e\ e$
& Expression application \\

                   &                     & $|$ & \texttt{\textbackslash} $\mathit{pat}_i$ 
                                               \texttt{->} $e$
& Lambda expression, $i > 0$ \\

                   &                     & $|$ & \texttt{case} $e$ \texttt{of} $\mathit{alt}_i$
& Case expression, $i > 0$\\

                   &                     & $|$ & \texttt{let} $\mathit{odecl}_i$ \texttt{in} $e$
& Recursive let expression, $i > 0$ \\

\\


Pattern            & $\mathit{pat}$      & ::= & $C\ \mathit{pat}_i$
& Nested constructor matching \\
                   &                     & $|$ & $\mathit{lit}$
& Match numeric literals \\
                   &                     & $|$ & $x$
& Bind to variable \\

\\


Alternative        & $\mathit{alt}$      & ::= & $\mathit{pat}_i$ \texttt{->} $e$
& $i > 0$ \\

\\


Type               & $\mathit{type}$     & $|$ & $C$
& Concrete type (constructor naming) \\
                   &                     & $|$ & $x$
& polymorphic type variable \\
                   &                     & $|$ & $\mathit{type}$ \texttt{->} $\mathit{type}$
& Function type \\

\\


Object decl. & $\mathit{odecl}$    & ::= & $x$ \texttt{::} $\mathit{type}$
& Type signature \\
                   &                     & $|$ & $x = e$
& Simple binding \\
                   &                     & $|$ & $f\ \mathit{pat}_i = e$
& Function declaration \\
                   &                     &     &
& $e$ cannot be an unboxed literal \\

\\


Constructor defn. & $\mathit{con}$  & ::= & $C\ \mathit{type}_i$
& $i \ge 0$ \\

Datatype defn. &  $\mathit{ddecl}$ & ::= & \texttt{data} 
                                                 [\texttt{unboxed}]
                                                 $C\ x_i =$ 
& User-defined data type, $i \ge 1, n > 0$ \\
                     &                   &     & $\mathit{con}_1 | \dots |
                                                 \mathit{con}_n$
& \\

\\


Program                & $\mathit{prog}$ & ::= & $(o|d)\mathit{decl}_i$
& Objects and datatypes

\end{tabular}
\label{miniHaskell}
\caption{mini-Haskell syntax}
\end{figure}


Because mini-Haskell was designed to replace STG, it makes sense to talk about
it in terms of the differences between the two and how mini-Haskell is
translated into STG for the backend.  Here, the differences are given in order
of perceived utility, or importance to someone transition from STG to
mini-Haskell.


\subsubsection{Removal of explicit heap objects.}
In STG, all object definitions require explicit classification with
\texttt{FUN}, \texttt{PAP}, \texttt{CON}, \texttt{THUNK}. This classification can be
inferred from context by the compiler so these keywords are not necessary.
Functions can then be defined in Haskell style with the parameters following
the function's identifier on the left side of the equality. With these simple
changes the syntax becomes a subset of Haskell.

Heap objects are still only created at the top level or in \texttt{let}
expressions, so the translation is straightforward: \texttt{FUN} objects are
identifiable by the inclusion of these parameters.  \texttt{PAP} objects are
created when a known function is partially applied.  \texttt{CON} objects are
created when constructors are fully applied.  Every other object becomes a
\texttt{THUNK}.

\subsubsection{Avoiding building thunks.}
As \emph{EA} points out, there is no need to build a thunk for the scrutinee
of a \texttt{case} expression because if the \texttt{case} is evaluated the
scrutinee is certain to be evaluated.  Similarly, if a function is strict
there is no need to build a thunk for each of its arguments (assuming boxed
argument type).  Because STG allows only \emph{atoms} (variables or literal
values) in argument positions, naive translation of Haskell \texttt{f e} to
\texttt{let \{x = e\} in f x}, followed by naive code generation, will result in
an unnecessary heap allocation.  An alternative is to translate to
\texttt{case e of x \{\_ -> f x\}}.



\subsubsection{The Layout Rule}
Mini-Haskell implements Haskell's Layout Rule in an almost direct translation of
the specification given in section 10.3 of the Haskell 2010 Report.  The notable
exception is the omission of any attempt to define the \texttt{parse-error}
function.  This is apparently a tricky spot for Haskell compilers (even GHC has
not always had it right) so, in the name of tradition, it remains a tricky spot
for our mini-Haskell front end.  The edge cases that it will incorrectly handle
as a result of this are exactly that: edge cases.  As such, they are easy to
avoid in practical usage.

The inclusion of the layout rule may seem small and insignificant, but it
allows its users to produce code that is visually similar to Haskell code.
This makes it easier to read and write for any Haskell programmer who relies
on layout to delineate context, presumably the vast majority.  It also allows
for the use of Haskell programming environments that support ``smart''
indentation, e.g., \emph{haskell-mode} for Emacs.



% Layout Rule Spec:
%   https://www.haskell.org/onlinereport/haskell2010/haskellch10.html\#x17-17800010.3
% GHC's syntax "infelicities":
%   https://downloads.haskell.org/~ghc/latest/docs/html/users\_guide/bugs-and-infelicities.html\#infelicities-syntax
% More on parse-error:
%   http://www.dcs.gla.ac.uk/mail-www/haskell/msg01711.html


\subsubsection{Expression to Expression application}

In STG, function application must be to atomic values, i.e., either literals or
variables.  This often requires introducing bindings with either \texttt{let} or
\texttt{case} expressions.  Consider the \texttt{append} function that
concatenates its two list parameters.  In STG, it is written as

\begin{verbatim}
append = FUN(l1 l2 ->
            case l1 of
              { Nil -> l2;
                Cons hd tl -> 
                  let { rec = THUNK(append tl l2);
                        result = CON(Cons hd rec) }
                  in result });
\end{verbatim}

In mini-Haskell, expressions may be applied to other expressions, so the
recursive call to \texttt{append} can be parenthesized. There is no need for the
\texttt{let} expression:

\begin{verbatim}
append l1 l2 = case l1 of
                Nil -> l2
                Cons hd tl -> Cons hd (append tl l2)
\end{verbatim}

This also serves as a good example of how expression to expression application is
transformed into STG.  Any non-atomic expression in an application expression is
bound to a heap object in a generated \texttt{let} expression.  These
expressions are thus properly ``atomized'' for STG.  The above mini-Haskell code
is transformed in STG that is structurally identical to the STG code that
precedes it.

\subsubsection{Fancy Pattern Matching.}
{\color{red}Implemented, not documented\dots}


<<<<<<< HEAD
\paragraph{Fancy Pattern Matching} -\\

STG supports pattern matching but only in a simple form and only in
\texttt{case} expressions.  mini-Haskell extends this to something approaching
what is found in Haskell.  Functions, including lambda expressions, may match on
all their arguments (with multiple non-anonymous definitions allowed to exhaust
the pattern combinations).  Patterns in all contexts (functions and case
expressions) may be nested arbitrarily deep.  These two features greatly
simplify the programmer's job.
=======
>>>>>>> fb0b84e3162684212b211a7c6fbc2a9ad5bb26fd

\subsubsection{Less significant features}

\begin{itemize}
\item Constructors as functions \\
  (partial application uses generated functions, full creates CONS)

\item Primitive operations as functions \\
  (partial application uses generated functions, full creates EPrimop)

\item Lambda Expressions \\
  (desugar to let-bound FUN objects)

\item Type Annotations \\
  (passed on to type checker)

\item Haskell Block \\
  (ignored by MHS frontend, not other Haskell compilers;
  allows the same file to be tested on our compiler vs GHC)
\end{itemize}


\section{Current Status, and Expected Results by the time of TiFP'16}

No bridge from ghc STG to our STG yet.

At the time of writing we have a complete serial implementation, including
\begin{itemize}
\item STG (inc. ADT w/unboxed) parser
\item type inferencer
\item code generator
\item runtime system including a garbage collector
\end{itemize}

We have implemented a simple copying garbage collector \cite{Cheney:1970} with
small \emph{nurseries} in the multithreaded case \cite{Marlow:2008}. We have
not focused on the development of a more sophisticated garbage collector
because the goal of this project is to explore automatic parallelization.

Mutual exclusion for garbage collector--heap check with every heap allocation.
In fact garbage collector can be invoked at any point in time except when
populating a heap object immediately after its allocation.



\subsection{Type system}

We perform standard Hindley-Milner type inference and enforcement, augmented
with built-in and user-defined unboxed types as described by Peyton-Jones and
Launchbury's seminal work~\cite{Jones91unboxedvalues}, but for unboxed types code
generation is currently limited to built-in single-word (64-bit) or smaller
types (e.g., \texttt{Int\#}, \texttt{UInt\#}, \texttt{Float\#}, \texttt{Double\#})
and user-defined enumerations, e.g., \texttt{data unboxed Bool\# = False\# |
  True\#}.

\subsection{Test suite and testing}

We have developed a extensive test suite which uses the CMake \cite{cmake-book}
CTest tool. The full test suite is automatically run, and any failures reported,
after every comment to the version control system. We use a Jenkins continuous
integration server \cite{jenkins-book} to automate this build and testing process.
The (git) version control repository is hosted externally so our student
collaborators can contribute at their leisure.

There are several hundred small STG and mini-Haskell programs in the test suite.
It is partitioned into a number of directories which control what
testing is done for a specific program. Given our goal of exploring various
levels of strictness, most tests are run with all evaluation strategies (3
degrees of function strictness by two degrees of constructor strictness).
One interesting class of test programs are those which require non-strict
semantics. For these programs we test not only that the test leads to the expected
result with non-strict semantics, but also that the program returns a
"blackhole" in the case of strict semantics. Students who were Haskell/STG
beginners have contributed substantially to the test suite. We have also found
that the test suite has be of use as a learning tool for these students to
discover  what level of non-strictness was required for their programs,
a benefit that we did not initially envision. Over time we have developed
extensive STG and mini-Haskell preludes of common programming patterns, to
aid in the writing of test programs.

Another important class of test programs are those which should fail with a
known error (e.g. a parsing error) for these programs the test suite checks
the output against an expected error regular expression. This class of test
programs has proved valuable in development and debugging of both the STG
and mini-Haskell front-ends.

We also run each test with various levels of garbage collection enabled, this
has helped us to find bugs not only in the garbage collector, but also in the
code generator.

The various enumerations of strictness and garbage collection levels mean that
we run a quite large number of tests. However, this is fully automated via the
testing and continuous integration infrastructure.

debug level (runtime/compile time)

\subsection{Parallel evaluation}

Parallel runtime is being implemented following the ideas
of~\cite{SPJs-many-papers}, thread pool, stack and heap for each thread, blah,
blah.

\section{Closely Related Work}

Clearly various papers describing some of the inner workings of ghc at various point of its
evolution have greatly informed our approach and these have all be previously cited.

In his BSc thesis, Don Stewart described an implementation wherein ghc was used as a Haskell
compiler front-end, dumping STG code as text, parsing that text, then translating to code for
execution on the Java Virtual Machine.~\cite{Stewart-BSc}.

Nvidia research compiler.

\section{Summary, Future Work, Conclusions?}

\section{Acknowledgments}

Funding for this project and its participants has been provided by the DOE
NNSA LANL Laboratory Directed Research and Development program award
20150845ER; the National Science Foundation Science, Technology, Engineering,
and Mathematics Talent Expansion Program (STEP) program for undergraduate
students; and the DOE Science Undergraduate Laboratory Internship (SULI) program.
%
Los Alamos National Laboratory is managed and operated by Los Alamos National
Security, LLC (LANS), under contract number DE-AC52-06NA25396 for the
Department of Energy’s National Nuclear Security Administration (NNSA).

% just get these into the bib
\cite{ghc-as-library}
\cite{Heren02}

%
% ---- Bibliography ----
%

\bibliographystyle{splncs03}

\bibliography{tfp}

\end{document}
