% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
% mkd for preliminary paper notes
\usepackage{enumitem} 
\usepackage{color} % for parenthetical notes while paper is in progress
%
\begin{document}
%
%mkd \frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
%
%mkd \mainmatter              % start of the contributions
%
\title{Strict Pure Functional Programming and Automatic Parallelization\\
       (New-project-in-progress Paper)}
%
\titlerunning{Automatic Parallelization}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Kei Davis\inst{1} \and
        Dean Prichard\inst{1} \and
        David Ringo\inst{1,2} \and
        Loren Anderson\inst{1,3} \and
        Jacob Marks\inst{1,4}\thanks{All project contributors, past and present, are listed.}
}
%
\authorrunning{Kei Davis et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Ivar Ekeland, Roger Temam, Jeffrey Dean, David Grove,
%Craig Chambers, Kim B. Bruce, and Elisa Bertino}
%
\institute{Los Alamos National Laboratory, Los Alamos, NM, USA\\
\email{kei.davis@lanl.gov},\\ WWW home page:
\texttt{http://ccsweb.lanl.gov/\homedir kei/}
\and
University of New Mexico,
Albuquerque, NM, USA
\and
North Dakota State University,
Fargo, North Dakota, USA
\and
New Mexico Institute of Mining and Technology,
Socorro, New Mexico, USA
}

\maketitle              % typeset the title of the contribution

% moronic author listing code increments footnote counter
\setcounter{footnote}{0}

\begin{abstract}
The claimed trend is an increasing appreciation and practice of functional
programming in scientific computing, and in particular strict (by default)
functional programming, in various guises.  Our project is the demonstration
of a light-weight, higher-order, polymorphic, pure functional language
implementation in which we can experiment with automatic parallelization
strategies and varying degrees of default strictness.  Non-strictness
constrains the degree of potential parallelism, that is, we are not exploring
speculative evaluation.
\\\ \\
\emph{Preliminatry paper notes:}
\begin{enumerate}[before=\itshape]
  \item We are reporting work in progress.  At the time of writing we have a
    complete serial implementation, a primitive proof-of-concept parallel
    implementation, and are currently developing a more realistic parallel
    runtime.  We expect to have preliminary results from our new parallel
    implementation by the time of the workshop.

  \item While this does not qualify as a student paper, students
    (undergraduate!)  have contributed considerably to this effort, and if
    accepted we plan to have one of these students present this paper.

  \item Hopefully in the spirit of a student-friendly workshop, for this draft
    version we have chosen a more informal, conversational style of
    presentation than necessarily appropriate for a final paper, have
    highlighted the major student contributions and involvement, and included
    perhaps gratuitous detail about process.
  \end{enumerate}


\keywords{strict pure functional programming, automatic parallelization, Haskell, STG}

\end{abstract}

\section{Background and Motivation}
The practice of so-called high-performance scientific computing (HPC) is,
overall, highly conservative with respect to change.  Even as the US
government commits to a push for exascale computing ($10^{18}$ FLOPS in a
single system) within the next decade, a highly respected member of the HPC
community recently stated publicly that ``Fortran is essential for exascale
programming.''~\cite{Heroux16}\footnote{Various EU, Japanese, and
  international groups are also anticipating exascale
  computing.~\cite{EUexascale,Exascale-org}} Existing US government laboratory
HPC codes run to hundreds of thousands of lines of Fortran and complete
rewrites are infeasible.~\footnote{The codes of our lab's British counterpart
  are reportedly in the same state.}  C++ has become more common for new code
starts, but their nature remains the same: huge codes with fair to
non-existent encapsulation of side effects, with multiple levels of
parallelism also poorly abstracted, especially at the thread (shared-memory)
level.  Thread-level parallel efficiency can be poor and is explained by
Amdahl's law because the parallel model \emph{as typically implemented} is
bulk synchronous.



\subsection{The trend: strict but still pure functional programming}
There is a small but growing understanding that \emph{pure functional
  semantics}, in some form, is essential to reining in the complexity of these
ever-evolving scientific codes.  Such semantics are expressed in a number
of ways as highlighted following.

\subsubsection{Rely on programmer discipline.}
Legacy Fortran scientific codes likely represent worst practices in state
encapsulation, with most data global and accessible to all; indeed, Fortran
was designed to make this easy via \texttt{COMMON} blocks---aggregates of
global variables---which further obfuscate program meaning by allowing
arbitrary naming of a block's variables at a subroutine/function granularity,
and requiring that the entire block be brought into scope.  It is almost
ironic, then, that of the mainstream high-performance scientific programming
languages (Fortran, C, and C++), Fortran~95 was the first to introduce the
\texttt{pure} function qualifier, and with enforcement by the compiler.
Otherwise it is entirely up to the programmer to make functions composable and
thread safe.  Here at LANL this approach is being used to good effect to clean
up some legacy code bases.

\subsubsection{Obey constraints prescribed by a parallel runtime system.}
There are a number of parallel runtime systems that encourage (but cannot
enforce) a pure functional style.  We will highlight Stanford's Legion
runtime as an example~\cite{Bauer12}.

In Legion, non-function-local data, or \emph{regions}, are requested from the
runtime system.  Regions may be shared among \emph{tasks} (essentially C++
functions), and each task is prescribed access privileges (read, write,
read/write, etc.)  and coherency requirements for each region to which it will
have access.  Other than registered access to regions, tasks must not access
non-constant global data.  In serial execution these access requirements are
notionally superfluous, but in parallel execution the runtime can dynamically
calculate the data dependency graph and relax the programmatically defined
serial ordering of task execution.  Very high parallel efficiency has been
demonstrated on extremely large computing systems~\cite{Bauer14}.  Here at
LANL the Legion system is being strongly considered for a new large-scale C++
code effort.\footnote{It seems significant that the PI of the Legion
  project was previously a co-developer of a strict, arguably pure functional
  language implementation~\cite{AikenFL,FLreport89}, in collaboration with
John Backus as a refinement of his FP language~\cite{Backus:1978}.}

\subsubsection{Use a pure functional language.}
Some twenty years after the US Department of Energy laboratory complex was
presented with the most highly parallel (and thereby performant)
implementation of a pure functional language to date~\cite{Davis96}, and
collectively had no conceptual understanding of what it was, much less why it
might be useful or important, there is now a small number of laboratory HPC
practioners who are developing an appreciation for pure functional programming
in Haskell.  Still, they see their needs as different than other Haskell
users: performance is paramount, unboxed arrays are a (perhaps \emph{the})
primary data structure, and non-strictness or laziness is more of a
nuisance--even a serious hindrance--than a convenience, much less ever
algorithmically essential in the sense of Bird's \emph{repmin}~\cite{Bird84}
or Johnsson's general attribute grammars~\cite{Johnsson87}.

A common complaint among these budding high-performance functional programmers
is the need to get strictness just right: to remove space leaks, achieve
acceptable performance, get communication (e.g.\ MPI) and IO to work as they
expect, and so on.  More specifically, they expect to be able to reason about
space and time usage using their existing mental models. While they are happy
to write code that is strongly reminiscent of the numerical examples in
Hughes' classic piece (whether they've read it or not)~\cite{Hughes89}, they
would also be happy to write in a slightly modified style that does not
require non-strictness---after all, that's how they've been thinking their
entire careers.

Much anecdotal evidence suggests that many others have similar difficulties
with lazy (or non-strict) by default semantics, with the most telling recent
concrete evidence being the push for, and realization of, the \texttt{Strict}
and \texttt{StrictData} language extensions newly available in GHC 8.0.
Quoting the documentation:
\begin{quote}
High-performance Haskell code (e.g. numeric code) can sometimes be littered
with bang patterns, making it harder to read. The reason is that laziness
isn't the right default in this particular code, but the programmer has no way
to say that except by repeatedly adding bang patterns~\cite{strict-strictdata}.
\end{quote}

We claim, then, that there is a trend towards \emph{strict-by-default pure
  functional programming}, whether as imposed by programmer discipline, a parallel
runtime system (such as the aforementioned Legion, wherein the pure semantics
at function granularity allows the runtime system to relax the nominal serial
ordering), or the language itself (e.g.\ strict Haskell).

Intel's TBB?  Cilk++?

\subsection{Strictness and Parallelism}

It is a trivial observation that given strict functional semantics, wherein
function arguments may be safely evaluated before function evaluation, it is
safe to evaluate the arguments, and the function, in parallel, and that this
principle applies recursively in the (dynamic) expression tree (or graph).

\section{Project and Goals}

Our project is the light-weight implementation of a pure, higher-order,
polymorphic, functional language and runtime system with which we can
experiment with automatic parallelization strategies with degrees of language
strictness ranging from lazy to fully strict.  Light-weight has several
meanings or implications.  First and foremost it means that \emph{we are not
  attempting to compete with ghc in any respect}.\footnote{We estimate that we
  will have, to within an order of magnitude, one person-hour of available
  development time for each person-year that ghc currently represents, or put
  another way, one second for each hour, respectively.}

\begin{itemize}
\item The implementation must be feasible in terms of effort;
\item The language must be strongly typed, implicitly or explicitly.  More
  specifically, the language should explicitly support both Hindley-Milner
  polymorphism and unboxed types~\cite{Jones:1991};
\item Straightforward direct linkage with C, preferably without a sophisticated
  foreign function interface, is essential;
  \item The language should be \emph{currying-friendly} as described by Marlow
and Peyton Jones~\cite{Marlow2004};
\item Choice of degree of strictness is easily selected;
\item Sharing, in the sense of what \emph{lazy} means vs.\ merely non-strict,
  should be preserved for all degrees of strictness;
\item Tail calling should be properly implemented, i.e., not as a trampoline;
\item etc.
\end{itemize}

For feasibility, we restrict ourselves to shared-memory implicit parallelism,
but distributed-memory parallelism should be straightforwardly realizable
using e.g.\ MPI or POSIX sockets.

A secondary, informal goal is to gain a sense of how often relatively
inexperienced functional programmers make essential use of
non-strictness.\footnote{The starting experience of our student interns to
  date has been normalized by having them complete Hutton's \emph{Programming in Haskell} textbook
as baseline preparation.~\cite{Hutton-book}}

\subsection{Varying degrees of strictness}

First some words about terminology.  For all degrees of strictness, sharing
(as implied by lazy but not by non-strict) of heap-allocated objects is
preserved so we will not use the term \emph{lazy} and simply refer to
strictness properties.  As is common we will use strict in an operational
sense, meaning that argument(s) are evaluated before being passed in a
function call, rather than the denotational sense.  Similarly, when referring
to language semantics we refer to the operational rather than denotational
semantics.

Greater strictness implies more opportunities for parallelism, possible losses
or gains in space efficiency, and less expressiveness.  We seek to explore
the interplay of all of these.

ghc 8.0 gives three possibilities: conventional non-strict, constructor
strict, and function- and constructor strict, but there are possible
variations.  The obvious variation is function, but not constructor,
strictness.  In Haskell the distinction between constructors and functions is
somewhat blurred because constructors behave like functions (outside of
pattern matching), but this need not be the case.  In STG---one of ghc's
intermediate representations---the distinction is clear: heap objects are
explicitly constructed by \texttt{let} constructs in which constructor names
are given explicitly (not aliased); constructor names are not functions.

\section{Design and Implementation}

Our wish list sounds much like the ghc Haskell implementation, and indeed ghc
is part of our master plan.  In short, our plan is to use ghc as our front-end
to generate its intermediate form, STG, then escape to our system from there.
There are various possible ways to accomplish the former: using ghc's option
to dump STG as text (see Stuart's report~\cite{Stuart}), hacking ghc itself
and, optionally, using ghc as a library.  We note that this approach has
a precedent in the aforementioned Nvidia research compiler.

\subsection{STG Language}
\setlength{\tabcolsep}{5pt}

The STG language is ...

Our incarnation of STG is very similar to that given in EA, as is
our presentation which is meant to evoke both an abstract and concrete syntax.

\begin{table}
\footnotesize % tiny scriptsize footnotesize small
\centering
\begin{tabular}{r r c l l}

Variable     & $f,\ x$        &     &                                              & Initial lower-case letter \\
Constructor  & $C$            &     &                                              & Initial upper-case letter \\
Literal      & $\mathit{lit}$ & ::= & $i\ |\ d$                                    & Integral or floating point literal \\
Atom         & $a$            & ::= & $\mathit{lit}\ |\ x$                         & \\ % Function, constructor args are atoms \\
\\
Expression   & $e$            & ::= & $a$                                          & Atom \\
             &                & $|$ & $f\ a_1\dots a_n$                            & Application, $n\ge 1$ \\
             &                & $|$ & $\oplus\ a_1\dots a_n$                       & Saturated primitive operation, $n\ge 1$ \\
             &                & $|$ & \texttt{let} $x_i = \mathit{obj}_i$
                                         \texttt{in} $e$                           & Recursive let $1\le i \le n$\\
             &                & $|$ & \texttt{case} $e$ \texttt{as}
                                        $x$ \texttt{of} $\mathit{alt}_i$           & Case expression, $1\le i \le n \ge 1$\\
\\
Alternatives & $\mathit{alt}$ & ::= & $C\ x_1\dots x_n$ \texttt{->} $e$            & Pattern match, $n \ge 0$ \\
             &                & $|$ & \texttt{\_ ->} $e$                           & Default \\
\\
Heap objects & $\mathit{obj}$ & ::= &\texttt{FUN} $x_1\dots x_n$ \texttt{->} $e$   & Function definition, arity $=n\ge 1$ \\
             &                & $|$ &\texttt{CON} $C\ a_1\dots a_n$                & Saturated constructor, $n \ge 0$ \\
             &                & $|$ &\texttt{THUNK} $e$                            & Thunk---explicit deferred evaluation \\
             &                & $|$ & $\mathit{PAP}\ f\ a_1\dots a_n$               & Evaluation-time partial application \\
             &                & $|$ & $\mathit{BLACKHOLE}$                         & Evaluation-time black hole \\
\\
Program      & $\mathit{prog}$& ::= & $f_i\ =\ \mathit{obj}_i$                      & $1\le i \le n \ge 1$, distinguished $\mathit{main}$ \\

\end{tabular}
\end{table}

While our interpretation of STG is as a simple higher-order, polymorphic, pure
functional language, some syntactic restrictions make direct programming
rather tedious.  This is discussed further in a later section.

Operationally, when an application such as

\subsection{Degrees of strictness, take two}

Other possible variations concern the notional arity of the underlying
function being applied.  This is readily explained in terms of the operation
of the STG machine described later.  Our description is based on that given in
Marlow and Peyton Jones' \emph{fast curry} paper~\cite{fastcurry}, to which we
refer so many times that we will subsequenty refer to it as
\emph{EA}.\footnote{\emph{Eval/Apply} from the title because \emph{FC} likely
  has another connotation to the reader.}

In STG the arity of a function is syntactically defined as the number of
manifest formal parameters, thus \texttt{id x = x} has by definition arity 1
regardless of what argument type it might be dynamically applied to.  We
distinguish \emph{application}, e.g.\ \texttt{f x y}, which is a syntactic
notion, from actually calling a function. Thus \texttt{id id x} is an
application of \texttt{id} to two arguments, but the underlying \emph{FUN}
(user- or system- defined function) when actually called is given exactly one
argument.

In the eval-apply model, given application \texttt{f x y}, in the standard
(non-strict) semantics \texttt{f} is first evaluated and the arity of the
underlying \emph{FUN} (user- or system- defined function) or \emph{PAP}
(partial application of a \emph{FUN}) is determined---in the case of
\emph{PAP} the arity is that of the underlying \emph{FUN} less the number of
arguments already provided.  If the arity is greater than two then a new
\emph{PAP} is created with arguments \texttt{x} and \texttt{y} inserted and
the new \emph{PAP} returned.  If the arity is equal to two then the underlying
\emph{FUN} is tail-called with any previous argument (in case of \emph{PAP})
and new arguments \texttt{x} and \texttt{y}.  If the arity is less than two
(it must be at least one, so one in this case) the underlying \emph{FUN} is
called-with-return with any previous argument (in case of \emph{PAP}) and new
argument \texttt{x}, then the object returned applied to remaining argument
\texttt{y}.

Given application \texttt{f x y} when might \texttt{x} and \texttt{y} be
evaluated?  In the non-strict semantics evaluation is on demand, after
\texttt{f} has been evaluated, the underlying \emph{FUN} actually called
(i.e., not just partially applied and wrapped up in a \emph{PAP}), and one of
those arguments forced in subsequent evaluation (assuming that they weren't
forced in the evaluation of \texttt{f} itself).

In a strict semantics one can come up with various evaluation orders, but
excluding the absurd, as far as distinguishing termination properties goes
these can be distilled down to two possibilities, namely whether on
construction of a \emph{PAP} the (new) arguments are forced or not.  (An
absurd strategy might force every second argument.)  These are in fact
distinguishable because (in Haskell) the first argument to \texttt{seq} might
evaluate to a \emph{PAP}.

This then gives three reasonable operational modes that are strictly ordered
in terms of termination properties: non-strict, fully strict---all arguments
in an application are evaluated---and the intermediate in which only those
arguments needed by a function call are evaluated.  The STG machinery makes
the choice of evaluation strategy easy to change, both in terms of function
and constructor application.

\subsection{STG to C}

The back-end is written in Haskell and generates proper tail-calling C code;
the runtime system is written in C/C++.  The system fairly faithfully
implements the STG machine as described in Peyton-Jones and Marlow's ``fast
curry'' paper~\cite{fastcurry} and dynamically illustrated by Pope's
miniSTG~\cite{ministg} with the exception of the state transition rules which,
of course, are different than those for lazy semantics.  Sharing is preserved.

\subsubsection{Proper tail calls}

Proper tail calling is achieved.  Because we maintain our own data and control
(continuation) stack all generated C functions take no arguments and have
return type void, and with a single exception in the non-strict case are
invoked in tail call positions, at non-zerop optimization levels both gcc and
Clang/LLVM generate jumps instead of calls for C calls in tail call positions.

The exceptions to tail calling are for convenience.  Similar to what is
described in the \emph{fast curry} paper we have a generic variadic stgApply
function that is used for applications of non-known functions, and known
functions with a deficit or excess number of arguments.  The stgApply function
takes as arguments the first element of the application (FUN, PAP, THUNK, or
BLACKHOLE) and its arguments, and if a THUNK, evaluates it with a call (rather
than jump) to yield a FUN or PAP.  This allows us to implement a single
stgApply function as a single C function.  The implementation of stgApply as
described in \emph{fast curry} makes the same concession.

For stgApply in the non-strict semantics the alternative to call-with-return
would be a pair of functions stgApply1 and stgApply2.  In one possible variation,
stgApply1 would push a continuation with stgApply2 as the return address, then
invoke the first element of the application.  Call-with-return becomes more
convenient when considering the various forms of strictness:  when evaluating
function or constructor arguments it is simpler, and arguably more efficient
in terms of time (but not space in general) to ``evaluate in place'' than
implement the continuation style to the logical limit.

\subsection{Mini-Haskell}

As mentioned, a current near-term goal is to use ghc as our front-end
to generate STG as input to our system.  In the meantime, as a stop-gap
measure to make the generation of test cases easier, we have implemented
a \emph{mini-Haskell} front-end, a strict subset of standard Haskell syntax.

First while the scrutinee of a case expression may be an arbitrary expression,
arguments in function applications, constructor allocation, and primitive
operations are constrained to be atoms.  For unboxed types a case
a trivial case expression to alleviates
the difficulty.
\begin{verbatim}
  f ... e ... -> case e as x of _ -> f ... x ...
\end{verbatim}

{\color{red} I didn't want to touch the above text, but there's obviously some 
redundancy in the content here...}

Writing STG in the concrete syntax conceived for our project is tedious and
prone to errors.  Compared to Haskell, the eventual source language for the
compiler, STG is relatively verbose, requiring explicit block and expression
delimiters, and classification of heap objects. Though its general structure is
similar to primitive Haskell, it lacks many of the conveniences that make
Haskell a pleasure to work with.  The motivation for a more expressive language
for testing our compiler in its nascent stages is then clear.  This language,
mini-Haskell, was progressively "grown" from the STG syntax towards a subset of
Haskell itself*, removing much of the frustrating syntactic cruft and adding
some of the core features that are often taken for granted in Haskell (and other
languages).

{\color{red}
* Exceptions include the "unboxed" keyword allowed in datatype definitions and
the ability to specify unboxed ints and doubles with a postfix "\#" character.
The former feature is not in any extension of Haskell, while the latter can be
included using the MagicHash extension.}


An informal specification of the mini-Haskell grammar is given in
{\color{red}numbered table?}.  There is significant overlap with STG, resulting
from the structural similarities, but the redundancies are included for
completeness. {\color{red} should they be?}


% Haskell Report:
%   https://www.haskell.org/onlinereport/haskell2010/haskell.html
% (PDF):
%   https://www.haskell.org/definition/haskell2010.pdf

% Chapter 10:
%   https://www.haskell.org/onlinereport/haskell2010/haskellch10.html
% (No standalone PDF)


% because # looks strangely large next to math terms
\newcommand{\hash}{{\scriptsize\#}}

\begin{table}
\centering
\begin{tabular}{r r c l l}
Variable         & $f,\ x$             &     &
& Initial lower-case letter \\

Constructor        & $C$                 &     & 
& Initial upper-case letter \\

Literal            & $\mathit{lit}$      & ::= & $i\ |\ d\ 
                                                 |\ i$\hash\ $|\ d$\hash
& Integral or floating point literal (boxed) \\

                   &                     &     & 
& optional \# suffix denotes unboxed value \\

Atom               & $a$                 & ::= & $\mathit{lit}\ |\ x\ |\ C$
& Constructors are first class \\ 
% not sure if that's the right terminology 
\\


Expression         & $e$                 & ::= & $a$
& Atomic expression \\

                   &                     & $|$ & $e\ e$
& Expression application \\

                   &                     & $|$ & \texttt{\textbackslash} $\mathit{pat}_i$ 
                                               \texttt{->} $e$
& Lambda expression, $i > 0$ \\

                   &                     & $|$ & \texttt{case} $e$ \texttt{of} $\mathit{alt}_i$
& Case expression, $i > 0$\\

                   &                     & $|$ & \texttt{let} $\mathit{odecl}_i$ \texttt{in} $e$
& Recursive let expression, $i > 0$ \\

\\


Pattern            & $\mathit{pat}$      & ::= & $C\ \mathit{pat}_i$
& Nested constructor matching \\
                   &                     & $|$ & $\mathit{lit}$
& Match numeric literals \\
                   &                     & $|$ & $x$
& Bind to variable \\

\\


Alternative        & $\mathit{alt}$      & ::= & $\mathit{pat}_i$ \texttt{->} $e$
& $i > 0$ \\

\\


Type               & $\mathit{type}$     & $|$ & $C$
& Concrete type (constructor naming) \\
                   &                     & $|$ & $x$
& polymorphic type variable \\
                   &                     & $|$ & $\mathit{type}$ \texttt{->} $\mathit{type}$
& Function type \\

\\


Object Declaration & $\mathit{odecl}$    & ::= & $x$ \texttt{::} $\mathit{type}$
& Type signature \\
                   &                     & $|$ & $x = e$
& Simple binding \\
                   &                     & $|$ & $f\ \mathit{pat}_i = e$
& Function declaration \\
                   &                     &     &
& $e$ cannot be an unboxed literal \\

\\


Constructor Definition & $\mathit{con}$  & ::= & $C\ \mathit{type}_i$
& $i \ge 0$ \\

Datatype Declaration &  $\mathit{ddecl}$ & ::= & \texttt{data} 
                                                 [\texttt{unboxed}]
                                                 $C\ x_i =$ 
& User-defined data type, $i \ge 1, n > 0$ \\
                     &                   &     & $\mathit{con}_1 | \dots |
                                                 \mathit{con}_n$
& \\

\\


Program                & $\mathit{prog}$ & ::= & $(o|d)\mathit{decl}_i$
& Objects and datatypes, no order necessary

\end{tabular}
\end{table}


Because mini-Haskell was designed to replace STG, it makes sense to talk about
it in terms of the differences between the two and how mini-Haskell is
translated into STG for the backend.  Here, the differences are given in order
of perceived utility, or importance to someone transition from STG to
mini-Haskell.


\paragraph{Removal of explicit heap objects} -\\

In STG, all object definitions require explicit classification with
\texttt{CON}, \texttt{PAP}, \texttt{THUNK}, \texttt{FUN}, or
\texttt{BLACKHOLE}. This classification can be inferred from context by the
compiler, so these keywords are no longer necessary.  Functions can then be
defined in Haskell-style, with the parameters following the function's
identifier on the left side of the equality. With these simple changes, the
syntax becomes a subset of Haskell's*.

{\color{red}
* With only these changes, the semantics are a little different; numeric
  literals would still be treated as unboxed values, for example.  I don't know
  if that's worth noting though.}

Heap objects are still only created at the top level or in \texttt{let}
expressions, so the translation is straightforward: \texttt{FUN} objects are
identifiable by the inclusion of these parameters.  \texttt{PAP} objects are
created when a known function is partially applied.  \texttt{CON} objects are
created when constructors are fully applied.  Every other object becomes a
\texttt{THUNK}.



\paragraph{The Layout Rule} -\\

Mini-Haskell implements Haskell's Layout Rule in an almost direct translation of
the specification given in section 10.3 of the Haskell 2010 Report.  The notable
exception is the omission of any attempt to define the \texttt{parse-error}
function.  This is apparently a tricky spot for Haskell compilers (even GHC has
not always had it right) so, in the name of tradition, it remains a tricky spot
for our mini-Haskell front end.  The edge cases that it will incorrectly handle
as a result of this are exactly that: edge cases.  As such, they are easy to
avoid in practical usage.

The inclusion of the layout rule may seem small and insignificant, but it allows
its users to produce code that is visually similar to Haskell code.  This makes
it easier to read and write for any Haskell programmer who relies on layout to
delineate context (I assume this is ~99.9% of human Haskell users).  It also
allows for the use of Haskell programming environments that support "smart"
indentation.



% Layout Rule Spec:
%   https://www.haskell.org/onlinereport/haskell2010/haskellch10.html\#x17-17800010.3
% GHC's syntax "infelicities":
%   https://downloads.haskell.org/~ghc/latest/docs/html/users\_guide/bugs-and-infelicities.html\#infelicities-syntax
% More on parse-error:
%   http://www.dcs.gla.ac.uk/mail-www/haskell/msg01711.html


\paragraph{Expression to Expression application} -\\

In STG, function application must be to atomic values, i.e. either literals or
variables.  This often requires introducing bindings with either \texttt{let} or
\texttt{case} expressions.  Consider the \texttt{append} function that
concatenates its two list parameters.  In STG, it is written as:

\begin{verbatim}
append = FUN(l1 l2 ->
            case l1 of
              { Nil -> l2;
                Cons hd tl -> 
                  let { rec = THUNK(append tl l2);
                        result = CON(Cons hd rec) }
                  in result });
\end{verbatim}

In mini-Haskell, expressions may be applied to other expressions, so the
recursive call to \texttt{append} can be parenthesized. There is no need for the
\texttt{let} expression:

\begin{verbatim}
append l1 l2 = case l1 of
                Nil -> l2
                Cons hd tl -> Cons hd (append tl l2)
\end{verbatim}

This also serves as a good example of expression to expression application is
transformed into STG.  Any non-atomic expression in an application expression is
bound to a heap object in a generated \texttt{let} expression.  These
expressions are thus properly "atomized" for STG.  The above mini-Haskell code
is transformed in STG that is structurally identical to the STG code that
precedes it.



\paragraph{Fancy Pattern Matching} -\\

{\color{red}\dots}



\paragraph{Less significant features} -\\

\begin{itemize}
\item Constructors as functions \\
  (partial application uses generated functions, full creates CONS)

\item Primitive operations as functions \\
  (partial application uses generated functions, full creates EPrimop)

\item Lambda Expressions \\
  (desugar to let-bound FUN objects)

\item Type Annotations \\
  (passed on to type checker)

\item Haskell Block \\
  (ignored by MHS frontend, not other Haskell compilers;
  allows the same file to be tested on our compiler vs GHC)
\end{itemize}


\section{Current Status, and Expected Results by the time of the symposium}

No bridge from ghc STG to our STG yet.

At the time of writing we have a complete serial implementation, including
\begin{itemize}
\item STG (inc. ADT w/unboxed) parser
\item type inferencer
\item code generator
\item runtime system including a garbage collector
\end{itemize}

We have implemented a simple copying garbage collector \cite{Cheney:1970}
with small "nurseries" in the multithreaded case \cite{Marlow:2008}. We have not
focused on the development of a more sophisticated garbage collector, as the goal
of this project is to explore levels of strictness.

Mutual exclusion for garbage collector--heap check with every heap allocation.
In fact garbage collector can be invoked at any point in time except when
populating a heap object immediately after its allocation.



\subsection{Type system}

We perform standard Hindley-Milner type inference and enforcement, augmented
with built-in and user-defined unboxed types as described by Peyton-Jones and
Launchbury's seminal work~\cite{unboxed}, but for unboxed types code
generation is currently limited to built-in single-word (64-bit) or smaller
types (e.g., \texttt{Int\#}, \texttt{UInt\#}, \texttt{Float\#}, \texttt{Double\#})
and user-defined enumerations, e.g., \texttt{data unboxed Bool\# = False\# |
  True\#}.

\subsection{Test suite and testing}

We have developed a extensive test suite which uses the CMake \cite{cmake-book}
CTest tool. The full test suite is automatically run, and any failures reported,
after every comment to the version control system. We use a Jenkins continuous
integration server \cite{jenkins-book} to automate this build and testing process.
The (git) version control repository is hosted externally so our student
collaborators can contribute at their leisure.

There are several hundred small STG and mini-Haskell programs in the test suite.
It is partitioned into a number of directories which control what
testing is done for a specific program. Given our goal of exploring various
levels of strictness, most tests are run with all evaluation strategies (3
degrees of function strictness by two degrees of constructor strictness).
One interesting class of test programs are those which require non-strict
semantics. For these programs we test not only that the test leads to the expected
result with non-strict semantics, but also that the program returns a
"blackhole" in the case of strict semantics. Students who were Haskell/STG
beginners have contributed substantially to the test suite. We have also found
that the test suite has be of use as a learning tool for these students to
discover  what level of non-strictness was required for their programs,
a benefit that we did not initially envision. Over time we have developed
extensive STG and mini-Haskell preludes of common programming patterns, to
aid in the writing of test programs.

Another important class of test programs are those which should fail with a
known error (e.g. a parsing error) for these programs the test suite checks
the output against an expected error regular expression. This class of test
programs has proved valuable in development and debugging of both the STG
and mini-Haskell front-ends.

We also run each test with various levels of garbage collection enabled, this
has helped us to find bugs not only in the garbage collector, but also in the
code generator.

The various enumerations of strictness and garbage collection levels mean that
we run a quite large number of tests. However, this is fully automated via the
testing and continuous integration infrastructure.

debug level (runtime/compile time)

\subsection{Parallel evaluation}

Parallel runtime is being implemented following the ideas
of~\cite{SPJs-many-papers}, thread pool, stack and heap for each thread, blah,
blah.

\section{Other Related Work}
Nvidia research compiler.

\section{Summary, Future Work, Conclusions?}

\section{Acknowledgments}

Funding for this project and its participants has been provided by the DOE
NNSA LANL Laboratory Directed Research and Development program award
20150845ER, the National Science Foundation Science, Technology, Engineering,
and Mathematics Talent Expansion Program (STEP) program for undergraduate
students, and the DOE Science Undergraduate Laboratory Internship program for
undergraduate students.
%
Los Alamos National Laboratory is managed and operated by Los Alamos National
Security, LLC (LANS), under contract number DE-AC52-06NA25396 for the
Department of Energy’s National Nuclear Security Administration (NNSA).

% just get these into the bib
\cite{ghc-as-library}
\cite{Heren02}

%
% ---- Bibliography ----
%

\bibliographystyle{splncs03}

\bibliography{tfp}

\end{document}
