/*
  Based on Boost's lockfree queue implementation. Boost v. 1.65.

  Boost Software License - Version 1.0 - August 17th, 2003

  Permission is hereby granted, free of charge, to any person or organization
  obtaining a copy of the software and accompanying documentation covered by
  this license (the "Software") to use, reproduce, display, distribute,
  execute, and transmit the Software, and to prepare derivative works of the
  Software, and to permit third-parties to whom the Software is furnished to
  do so, all subject to the following:

  The copyright notices in the Software and this entire statement, including
  the above license grant, this restriction and the following disclaimer,
  must be included in all copies of the Software, in whole or in part, and
  all derivative works of the Software, unless such copies or derivative
  works are solely in the form of machine-executable object code generated by
  a source language processor.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
  SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
  FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
  DEALINGS IN THE SOFTWARE.
*/

#ifndef _LOCKFREE_QUEUE_H_
#define _LOCKFREE_QUEUE_H_

#include <iostream>
#include <cassert>
#include <limits>
#include <atomic>
#include <type_traits>

#define LOCKFREE_CACHELINE_BYTES 64
//#define LOCKFREE_FREELIST_INIT_RUNS_DTOR 1

using namespace std;

namespace lockfree{

struct copy_convertible
{
    template <typename T, typename U>
    static void copy(T & t, U & u)
    {
        u = t;
    }
};

struct copy_constructible_and_copyable
{
    template <typename T, typename U>
    static void copy(T & t, U & u)
    {
        u = U(t);
    }
};

template <typename T, typename U>
void copy_payload(T & t, U & u)
{
    if(std::is_convertible<T, U>::value){
      copy_convertible::copy(t, u);
    }
    else{
      copy_constructible_and_copyable::copy(t, u);       
    }
}

template <class T>
class tagged_ptr
{
    using compressed_ptr_t = uint64_t;

public:
    using tag_t = uint16_t;

private:
    union cast_unit
    {
        compressed_ptr_t value;
        tag_t tag[4];
    };

    static const int tag_index = 3;
    static const compressed_ptr_t ptr_mask = 0xffffffffffffUL; //(1L<<48L)-1;

    static T* extract_ptr(volatile compressed_ptr_t const & i)
    {
        return (T*)(i & ptr_mask);
    }

    static tag_t extract_tag(volatile compressed_ptr_t const & i)
    {
        cast_unit cu;
        cu.value = i;
        return cu.tag[tag_index];
    }

    static compressed_ptr_t pack_ptr(T * ptr, tag_t tag)
    {
        cast_unit ret;
        ret.value = compressed_ptr_t(ptr);
        ret.tag[tag_index] = tag;
        return ret.value;
    }

public:
    /** uninitialized constructor */
    tagged_ptr(void) noexcept//: ptr(0), tag(0)
    {}

    /** copy constructor */
    tagged_ptr(tagged_ptr const & p) = default;

    explicit tagged_ptr(T * p, tag_t t = 0):
        ptr(pack_ptr(p, t))
    {}

    /** unsafe set operation */
    /* @{ */
    tagged_ptr & operator= (tagged_ptr const & p) = default;

    void set(T * p, tag_t t)
    {
        ptr = pack_ptr(p, t);
    }
    /* @} */

    /** comparing semantics */
    /* @{ */
    bool operator== (volatile tagged_ptr const & p) const
    {
        return (ptr == p.ptr);
    }

    bool operator!= (volatile tagged_ptr const & p) const
    {
        return !operator==(p);
    }
    /* @} */

    /** pointer access */
    /* @{ */
    T * get_ptr() const
    {
        return extract_ptr(ptr);
    }

    void set_ptr(T * p)
    {
        tag_t tag = get_tag();
        ptr = pack_ptr(p, tag);
    }
    /* @} */

    /** tag access */
    /* @{ */
    tag_t get_tag() const
    {
        return extract_tag(ptr);
    }

    tag_t get_next_tag() const
    {
        tag_t next = (get_tag() + 1u) & (std::numeric_limits<tag_t>::max)();
        return next;
    }

    void set_tag(tag_t t)
    {
        T * p = get_ptr();
        ptr = pack_ptr(p, t);
    }
    /* @} */

    /** smart pointer support  */
    /* @{ */
    T & operator*() const
    {
        return *get_ptr();
    }

    T * operator->() const
    {
        return get_ptr();
    }

    operator bool(void) const
    {
        return get_ptr() != 0;
    }
    /* @} */

protected:
    compressed_ptr_t ptr;
};

template <typename T,
          typename Alloc = std::allocator<T>
         >
class freelist_stack:
    Alloc
{
    struct freelist_node
    {
        tagged_ptr<freelist_node> next;
    };

    typedef tagged_ptr<freelist_node> tagged_node_ptr;

public:
    typedef T *           index_t;
    typedef tagged_ptr<T> tagged_node_handle;

    template <typename Allocator>
    freelist_stack (Allocator const & alloc, std::size_t n = 0):
        Alloc(alloc),
        pool_(tagged_node_ptr(NULL))
    {
        for (std::size_t i = 0; i != n; ++i) {
            T * node = Alloc::allocate(1);
#ifdef LOCKFREE_FREELIST_INIT_RUNS_DTOR
            destruct<false>(node);
#else
            deallocate<false>(node);
#endif
        }
    }

    template <bool ThreadSafe>
    void reserve (std::size_t count)
    {
        for (std::size_t i = 0; i != count; ++i) {
            T * node = Alloc::allocate(1);
            deallocate<ThreadSafe>(node);
        }
    }

    template <bool ThreadSafe, bool Bounded>
    T * construct (void)
    {
        T * node = allocate<ThreadSafe, Bounded>();
        if (node)
            new(node) T();
        return node;
    }

    template <bool ThreadSafe, bool Bounded, typename ArgumentType>
    T * construct (ArgumentType const & arg)
    {
        T * node = allocate<ThreadSafe, Bounded>();
        if (node)
            new(node) T(arg);
        return node;
    }

    template <bool ThreadSafe, bool Bounded, typename ArgumentType1, typename ArgumentType2>
    T * construct (ArgumentType1 const & arg1, ArgumentType2 const & arg2)
    {
        T * node = allocate<ThreadSafe, Bounded>();
        if (node)
            new(node) T(arg1, arg2);
        return node;
    }

    template <bool ThreadSafe>
    void destruct (tagged_node_handle tagged_ptr)
    {
        T * n = tagged_ptr.get_ptr();
        n->~T();
        deallocate<ThreadSafe>(n);
    }

    template <bool ThreadSafe>
    void destruct (T * n)
    {
        n->~T();
        deallocate<ThreadSafe>(n);
    }

    ~freelist_stack(void)
    {
        tagged_node_ptr current = pool_.load();

        while (current) {
            freelist_node * current_ptr = current.get_ptr();
            if (current_ptr)
                current = current_ptr->next;
            Alloc::deallocate((T*)current_ptr, 1);
        }
    }

    bool is_lock_free(void) const
    {
        return pool_.is_lock_free();
    }

    T * get_handle(T * pointer) const
    {
        return pointer;
    }

    T * get_handle(tagged_node_handle const & handle) const
    {
        return get_pointer(handle);
    }

    T * get_pointer(tagged_node_handle const & tptr) const
    {
        return tptr.get_ptr();
    }

    T * get_pointer(T * pointer) const
    {
        return pointer;
    }

    T * null_handle(void) const
    {
        return NULL;
    }

protected: // allow use from subclasses
    template <bool ThreadSafe, bool Bounded>
    T * allocate (void)
    {
        if (ThreadSafe)
            return allocate_impl<Bounded>();
        else
            return allocate_impl_unsafe<Bounded>();
    }

private:
    template <bool Bounded>
    T * allocate_impl (void)
    {
        tagged_node_ptr old_pool = pool_.load(memory_order_consume);

        for(;;) {
            if (!old_pool.get_ptr()) {
                if (!Bounded)
                    return Alloc::allocate(1);
                else
                    return 0;
            }

            freelist_node * new_pool_ptr = old_pool->next.get_ptr();
            tagged_node_ptr new_pool (new_pool_ptr, old_pool.get_next_tag());

            if (pool_.compare_exchange_weak(old_pool, new_pool)) {
                void * ptr = old_pool.get_ptr();
                return reinterpret_cast<T*>(ptr);
            }
        }
    }

    template <bool Bounded>
    T * allocate_impl_unsafe (void)
    {
        tagged_node_ptr old_pool = pool_.load(memory_order_relaxed);

        if (!old_pool.get_ptr()) {
            if (!Bounded)
                return Alloc::allocate(1);
            else
                return 0;
        }

        freelist_node * new_pool_ptr = old_pool->next.get_ptr();
        tagged_node_ptr new_pool (new_pool_ptr, old_pool.get_next_tag());

        pool_.store(new_pool, memory_order_relaxed);
        void * ptr = old_pool.get_ptr();
        return reinterpret_cast<T*>(ptr);
    }

protected:
    template <bool ThreadSafe>
    void deallocate (T * n)
    {
        if (ThreadSafe)
            deallocate_impl(n);
        else
            deallocate_impl_unsafe(n);
    }

private:
    void deallocate_impl (T * n)
    {
        void * node = n;
        tagged_node_ptr old_pool = pool_.load(memory_order_consume);
        freelist_node * new_pool_ptr = reinterpret_cast<freelist_node*>(node);

        for(;;) {
            tagged_node_ptr new_pool (new_pool_ptr, old_pool.get_tag());
            new_pool->next.set_ptr(old_pool.get_ptr());

            if (pool_.compare_exchange_weak(old_pool, new_pool))
                return;
        }
    }

    void deallocate_impl_unsafe (T * n)
    {
        void * node = n;
        tagged_node_ptr old_pool = pool_.load(memory_order_relaxed);
        freelist_node * new_pool_ptr = reinterpret_cast<freelist_node*>(node);

        tagged_node_ptr new_pool (new_pool_ptr, old_pool.get_tag());
        new_pool->next.set_ptr(old_pool.get_ptr());

        pool_.store(new_pool, memory_order_relaxed);
    }

    atomic<tagged_node_ptr> pool_;
};

template<class T>
class queue{
public:

  struct alignas(LOCKFREE_CACHELINE_BYTES) node{
    using tagged_node_handle = tagged_ptr<class node>; 

    using handle_type = class node*;

    node(T const & v, handle_type null_handle):
        data(v)//, next(tagged_node_handle(0, 0))
    {
        /* increment tag to avoid ABA problem */
        tagged_node_handle old_next = next.load(memory_order_relaxed);
        tagged_node_handle new_next (null_handle, old_next.get_next_tag());
        next.store(new_next, memory_order_release);
    }

    node (handle_type null_handle):
        next(tagged_node_handle(null_handle, 0))
    {}

    node(void)
    {}

    std::atomic<tagged_node_handle> next;
    T data;
  };

  using tagged_node_handle = tagged_ptr<node>; 

  using handle_type = node*;

  using pool_t = freelist_stack<node, std::allocator<node>>;

  using node_allocator = std::allocator<node>;

    queue(size_t capacity = 0)
    : head_(tagged_node_handle(nullptr, 0)),
      tail_(tagged_node_handle(nullptr, 0)),
      pool(node_allocator(), capacity + 1){
      initialize();
    }

    ~queue(){
      T dummy;
      while(unsynchronized_pop(dummy))
      {}

      pool.template destruct<false>(head_.load(memory_order_relaxed));
    }

    bool empty() const{
      return pool.get_handle(head_.load()) == pool.get_handle(tail_.load());
    }

    bool push(T const & t){
      return do_push<false>(t);
    }

    bool pop(T & ret){
      return pop<T>(ret);
    }

private:
  
  void initialize(){
    node * n = pool.template construct<true, false>(pool.null_handle());
    tagged_node_handle dummy_node(pool.get_handle(n), 0);
    head_.store(dummy_node, memory_order_relaxed);
    tail_.store(dummy_node, memory_order_release);
  }

  bool unsynchronized_push(T const & t)
  {
      node * n = pool.template construct<false, false>(t, pool.null_handle());

      if (n == NULL)
          return false;

      for (;;) {
          tagged_node_handle tail = tail_.load(memory_order_relaxed);
          tagged_node_handle next = tail->next.load(memory_order_relaxed);
          node * next_ptr = next.get_ptr();

          if (next_ptr == 0) {
              tail->next.store(tagged_node_handle(n, next.get_next_tag()), memory_order_relaxed);
              tail_.store(tagged_node_handle(n, tail.get_next_tag()), memory_order_relaxed);
              return true;
          }
          else
              tail_.store(tagged_node_handle(next_ptr, tail.get_next_tag()), memory_order_relaxed);
      }
  }

  template <bool Bounded>
  bool do_push(T const & t)
  {
      node * n = pool.template construct<true, Bounded>(t, pool.null_handle());
      handle_type node_handle = pool.get_handle(n);

      if (n == NULL)
          return false;

      for (;;) {
          tagged_node_handle tail = tail_.load(memory_order_acquire);
          node * tail_node = pool.get_pointer(tail);
          tagged_node_handle next = tail_node->next.load(memory_order_acquire);
          node * next_ptr = pool.get_pointer(next);

          tagged_node_handle tail2 = tail_.load(memory_order_acquire);
          if (__builtin_expect(tail == tail2, 1) ) {
              if (next_ptr == 0) {
                  tagged_node_handle new_tail_next(node_handle, next.get_next_tag());
                  if ( tail_node->next.compare_exchange_weak(next, new_tail_next) ) {
                      tagged_node_handle new_tail(node_handle, tail.get_next_tag());
                      tail_.compare_exchange_strong(tail, new_tail);
                      return true;
                  }
              }
              else {
                  tagged_node_handle new_tail(pool.get_handle(next_ptr), tail.get_next_tag());
                  tail_.compare_exchange_strong(tail, new_tail);
              }
          }
      }
  }

  template <typename U>
  bool pop (U & ret)
  {
      for (;;) {
          tagged_node_handle head = head_.load(memory_order_acquire);
          node * head_ptr = pool.get_pointer(head);

          tagged_node_handle tail = tail_.load(memory_order_acquire);
          tagged_node_handle next = head_ptr->next.load(memory_order_acquire);
          node * next_ptr = pool.get_pointer(next);

          tagged_node_handle head2 = head_.load(memory_order_acquire);
          if (__builtin_expect(head == head2, 1)) {
              if (pool.get_handle(head) == pool.get_handle(tail)) {
                  if (next_ptr == 0)
                      return false;

                  tagged_node_handle new_tail(pool.get_handle(next), tail.get_next_tag());
                  tail_.compare_exchange_strong(tail, new_tail);

              } else {
                  if (next_ptr == 0)
                      /* this check is not part of the original algorithm as published by michael and scott
                       *
                       * however we reuse the tagged_ptr part for the freelist and clear the next part during node
                       * allocation. we can observe a null-pointer here.
                       * */
                      continue;
                  copy_payload(next_ptr->data, ret);

                  tagged_node_handle new_head(pool.get_handle(next), head.get_next_tag());
                  if (head_.compare_exchange_weak(head, new_head)) {
                      pool.template destruct<true>(head);
                      return true;
                  }
              }
          }
      }
  }

  bool unsynchronized_pop (T & ret)
  {
      return unsynchronized_pop<T>(ret);
  }

  template <typename U>
  bool unsynchronized_pop (U & ret)
  {
      for (;;) {
          tagged_node_handle head = head_.load(memory_order_relaxed);
          node * head_ptr = pool.get_pointer(head);
          tagged_node_handle tail = tail_.load(memory_order_relaxed);
          tagged_node_handle next = head_ptr->next.load(memory_order_relaxed);
          node * next_ptr = pool.get_pointer(next);

          if (pool.get_handle(head) == pool.get_handle(tail)) {
              if (next_ptr == 0)
                  return false;

              tagged_node_handle new_tail(pool.get_handle(next), tail.get_next_tag());
              tail_.store(new_tail);
          } else {
              if (next_ptr == 0)
                  /* this check is not part of the original algorithm as published by michael and scott
                   *
                   * however we reuse the tagged_ptr part for the freelist and clear the next part during node
                   * allocation. we can observe a null-pointer here.
                   * */
                  continue;
              copy_payload(next_ptr->data, ret);
              tagged_node_handle new_head(pool.get_handle(next), head.get_next_tag());
              head_.store(new_head);
              pool.template destruct<false>(head);
              return true;
          }
      }
  }

  std::atomic<tagged_node_handle> head_;
  std::atomic<tagged_node_handle> tail_;
  pool_t pool;
};

} // namespace lockfree

#endif // _LOCKFREE_QUEUE_H_
